import { useEffect, useRef, useState } from "react";
import { Image, StyleSheet, Text, View, TouchableOpacity, AppStateStatus } from "react-native";
import modelManager from "../../interceptor/ModelManager";

import { decodeJpeg } from '@tensorflow/tfjs-react-native'
import {
    Camera,
    PhotoFile,
    useCameraDevice,
    useCameraPermission,
} from 'react-native-vision-camera';
import RNFS from 'react-native-fs';
import { InferenceSession, Tensor as OnnxTensor } from "onnxruntime-react-native";
import { StudyType } from "@/types/StudyType";
import CalcStudyModule from "@/modules/calcStudy/CalcStudyModule";
import { NetInfoSubscription } from "@react-native-community/netinfo";
import { setLandmarkData } from "@/modules/fsanet/GazeModule";
import { AnnotatedPrediction, EstimateFacesConfig } from "@tensorflow-models/face-landmarks-detection/dist/mediapipe-facemesh";

// Tensorflow blazeface ëª¨ë¸ì„ ê°€ì ¸ì˜¨ë‹¤.
const blazeface = require('@tensorflow-models/blazeface');

import * as jpeg from "jpeg-js";
import { Rank, Tensor, Tensor3D, Tensor4D, TensorContainer } from "@tensorflow/tfjs-core";
import { CODE_GRP_CD } from "@/common/utils/codes/CommonCode";


import '@tensorflow/tfjs-react-native';
import OnnxModules from "../modules/OnnxModules";

const tf = modelManager.tf;


const LOOP_TIME = 100;
const LOOP_LIMIT_CNT = 10; // í•™ìŠµ ìˆ˜í–‰ì¤‘ ë£¨í”„ë‹¹ í•©ê³„ë¥¼ ë‚´ê¸° ìœ„í•œ íšŸìˆ˜
const LOOP_INTERVAL = 1; // í•™ìŠµ ë£¨í”„ ì‹œê°„


const RESIZE_HEIGHT = 320;
const RESIZE_WIDTH = 292;

const StudyScreen2 = () => {

    const [doSq, setDoSq] = useState<number>(0); // í•™ìŠµ ì‹¤í–‰ ì‹œí€€ìŠ¤
    const device = useCameraDevice('front');
    const cameraRef = useRef<Camera>(null);
    const loopMainCntRef = useRef<number>(0);
    const { hasPermission, requestPermission } = useCameraPermission();
    const loopStartSecRef = useRef<number>();           // ë£¨í”„ ì‹œì‘ ì‹œê°„(ì´ˆ)
    const loopStartTimeRef = useRef<number>(0);         // ë£¨í”„ ì‹œì‘ ì‹œê°„
    const inConnectNetworkRef = useRef<boolean>(true);  // ë„¤íŠ¸ì›Œí¬ì˜ ì—°ê²° ì—¬ë¶€ë¥¼ ì²´í¬í•©ë‹ˆë‹¤.
    const [beforePreviewBase64Image, setBeforePreviewBase64Image] = useState<string | null>(null);
    const [afterPreviewBase64Image, setAfterPreviewBase64Image] = useState<string | null>(null);
    const [finallyBase64Image, setFinallyBase64Image] = useState<string | null>(null);

    let [accAtntn] = useState<number[]>([]);


    const [imageUri, setImageUri] = useState<string | null>(null);
    const [cameraOn, setCameraOn] = useState<boolean>(true); // ìë™ ìº¡ì²˜ ì—¬ë¶€

    // ë£¨í”„ê°€ NíšŒ ìˆ˜í–‰ë˜ëŠ” ë™ì•ˆ ëˆ„ì ë˜ëŠ” í•™ìŠµ ì •ë³´
    let [accStudyDoDtlInfo, setAccStudyDoDtlInfo] = useState<StudyType.StudyDoDtlSum>({
        msrdTmArr: [], // ì¸¡ì •ëœ ì‹œê°„ - 60íšŒ ë™ì•ˆì˜ ì‹œê°„ì˜ í•©ê³„ë¥¼ ë”í•©ë‹ˆë‹¤.
        isFaceDtctArr: [], // ì–¼êµ´íƒì§€ì—¬ë¶€ - 60íšŒ ë™ì•ˆ í•œë²ˆì´ë¼ë„ ìºì¹˜ê°€ ë˜ë©´ í•´ë‹¹ ê°’ì€ 1ë¡œ ê³ ì •í•©ë‹ˆë‹¤.
        exprCdArr: [], // í‘œì •ì½”ë“œ
        valenceArr: [], // valence
        arousalArr: [], // arousal
        emtnCdArr: [], // ì •ì„œì½”ë“œ
        atntnArr: [], // ì§‘ì¤‘ë ¥ - 60íšŒê°€ ìˆ˜í–‰ë˜ëŠ” ë™ì•ˆ ì§‘ì¤‘ë ¥ ì ìˆ˜ì˜ í•©ê³„
        strssArr: [], // ìŠ¤íŠ¸ë ˆìŠ¤
        doSq: doSq, // ì‹¤í–‰ì‹œí€€ìŠ¤ - ìµœì´ˆ í•œë²ˆë§Œ ìˆ˜í–‰
        tensorResultArr: [], //
    });



    const [isFaceDtctYn, setIsFaceDtctYn] = useState<boolean>(false);               // ì–¼êµ´ íƒì§€ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥¸ ë¶ˆ(íŒŒë€ìƒ‰/ë…¸ë€ìƒ‰)ì„ ì¼œì¤ë‹ˆë‹¤.

    useEffect(() => {
        console.log("ë„ˆì˜ ì´ë¦„ì€ ", tf.getBackend())
        requestPermission();

    }, []);

    useEffect(() => {
        let stopped = false;

        const loop = async () => {
            if (stopped) return;
            await handleCapture();
            setTimeout(loop, LOOP_TIME);
        };

        if (cameraOn) loop();

        return () => {
            stopped = true;
        };
    }, [cameraOn]);



    /** 
     * VisionCameraì˜ ìŠ¤ëƒ…ìƒ·ì„ ê¸°ë°˜ìœ¼ë¡œ Tensorë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° 
     */
    const snapshotToTensor = async (snapshot: PhotoFile): Promise<Tensor3D | null> => {
        let _imageToTensor: Tensor3D | null = null;
        try {
            const base64Data = await RNFS.readFile(snapshot.path, 'base64');
            const imageBuffer = new Uint8Array(tf.util.encodeString(base64Data, 'base64').buffer);
            await RNFS.unlink(snapshot.path);           // ì„ì‹œ ì €ì¥ íŒŒì¼ íŒŒì¼ ì‚­ì œ
            const decoded = decodeJpeg(imageBuffer);
            _imageToTensor = tf.image.resizeBilinear(decoded, [RESIZE_HEIGHT, RESIZE_WIDTH]);
        } catch (error) {
            console.log(`[-] snapshotToTensor : `, error)
        }
        return _imageToTensor
    }

    /**
     * ì¼ì • ì‹œê°„ë§ˆë‹¤ Snapshotì„ ì°ì–´ì„œ ë°˜í™˜í•´ì£¼ëŠ” í•¨ìˆ˜ 
     * @returns 
     */
    const handleCapture = async (): Promise<void> => {

        if (!cameraRef.current) return;
        let _accLoopCnt = 0;                    // 60íšŒì— ë”°ë¥¸ ë£¨í”„ ê°œìˆ˜
        let _timerId: NodeJS.Timeout; // setTimer ì‹œê°„ ê´€ë¦¬
        let _strtTs: number; // í•™ìŠµ ì‹œì‘ ì‹œê°„

        loopStartTimeRef.current = Date.now();

        const startTime = performance.now(); // â± ì‹œì‘ ì‹œê°„

        try {

            console.log(`1. ë£¨í”„ ì‹œì‘ ì‹œì ì— ë©”ëª¨ë¦¬ì— ìˆëŠ” í…ì„œ ìˆ˜  ${tf.tidy(() => tf.memory().numTensors)}`);
            loopStartTimeRef.current = Date.now();

            // [STEP1] ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ë©ˆì¶¥ë‹ˆë‹¤.
            if (!inConnectNetworkRef.current) return;

            // [STEP1] ì „ì²´ì—ì„œ ëˆ„ì  ë° ê°±ì‹  í•  ë³€ìˆ˜ê°’ë“¤ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
            loopMainCntRef.current++; // í•™ìŠµì„ ìˆ˜í–‰ì‹œì‘ í•œ ì´í›„ ì „ì²´ ë£¨í”„ ëˆ„ì  íšŸìˆ˜
            _accLoopCnt++; // í•™ìŠµì˜ LIMIT ê°’ì— ë”°ë¥¸ ë£¨í”„ ëˆ„ì  íšŸìˆ˜
            _strtTs = Date.now();
            // ë£¨í”„ ì‹œì‘ì‹œ DB ì‹œê°„ì‹œê°„ ì €ì¥
            if (_accLoopCnt === 1) {
                loopStartSecRef.current = Math.round((Date.now() - startTime) / 1000); // stopwatchRef.current?.getNowSec();           // ë£¨í”„ ì‹œì‘ ì´ˆ
            }


            const SNAPSHOT_QUALITY_MODE = {
                HIGH: {
                    label: "ê³ í™”ì§ˆ",
                    value: 100,
                    estimatedSize: "300KB+",
                    description: "ê°€ì¥ ì„ ëª…í•œ í™”ì§ˆ, íŒŒì¼ í¬ê¸° í¼. ì •í™•ë„ ìš°ì„ ì¼ ë•Œ ì‚¬ìš©",
                },
                NORMAL: {
                    label: "ì¼ë°˜",
                    value: 85,
                    estimatedSize: "200KB",
                    description: "ê¸°ë³¸ ê¶Œì¥ í’ˆì§ˆ, ì†ë„ì™€ í™”ì§ˆ ê· í˜•",
                },
                SMALL: {
                    label: "ì €í™”ì§ˆ",
                    value: 50,
                    estimatedSize: "100KB",
                    description: "ì†ë„ ê°œì„ ì— ìœ ë¦¬, ì•½ê°„ì˜ í™”ì§ˆ ì†ì‹¤",
                },
                VSMALL: {
                    label: "ë§¤ìš° ì €í™”ì§ˆ",
                    value: 30,
                    estimatedSize: "50KB ì´í•˜",
                    description: "ìµœëŒ€ ì†ë„, ì •í™•ë„ ì†ì‹¤ ê°€ëŠ¥. ì €ì‚¬ì–‘ ë””ë°”ì´ìŠ¤ì— ì í•©",
                },
            } as const;

            const snapshot = await cameraRef.current.takeSnapshot({ quality: SNAPSHOT_QUALITY_MODE.VSMALL.value, });
            setImageUri(snapshot.path); // TODO: ì‚­ì œ ì˜ˆì •
            const convertStart = performance.now();
            const _imageToTensor = await snapshotToTensor(snapshot)         // Snapshot To Tensor
            const convertEnd = performance.now();
            console.log(`ğŸ“¸ base64 to Tensor ë³€í™˜ ì‹œê°„: ${(convertEnd - convertStart).toFixed(2)}ms`);
            // í…ì„œê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° ëª¨ë“  ìˆ˜í–‰
            _accLoopCnt++;                                                  // í•™ìŠµì˜ LIMIT ê°’ì— ë”°ë¥¸ ë£¨í”„ ëˆ„ì  íšŸìˆ˜
            if (_imageToTensor !== null) {
                // ë£¨í”„ ì‹œì‘ì‹œ DB ì‹œê°„ì‹œê°„ ì €ì¥
                if (_accLoopCnt === 1) {
                    _strtTs = Date.now();
                }

                if (!_imageToTensor) return;
                else {
                    // [STESP3] ì–¼êµ´ì´ ì¸¡ì •ë˜ê±°ë‚˜ ì•ˆë˜ê±°ë‚˜ í•˜ëŠ” ê²½ìš°ì— ëŒ€í•´ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ë³€ìˆ˜ ì„ ì–¸
                    let _accFaceDetectCnt: number = 0;                                                                      // ì–¼êµ´ì´ ì¸¡ì •ëœ ëˆ„ì  íšŸìˆ˜
                    let _resultHsemotion: StudyType.ResultHsemotion = { arousalArr: [], valenceArr: [], emotionCode: "" };  // HSEmotion ì½”ë“œ
                    let configArr: number[] = [];                                                                           // ìµœì¢… ì—°ì‚° ê²°ê³¼ê°’ì„ êµ¬ì„±í•œ ë°°ì—´

                    const { estimateVersionRfb320, estimatePfldModel, estimateIrisLandmarkModel } = OnnxModules
                    const { fsanetEstimate, hsemotionEstimate } = modelCalcHandler;

                    const visionRfb320Result = await estimateVersionRfb320(_imageToTensor);

                    if (visionRfb320Result.length > 0) {
                        const pfldArr = await estimatePfldModel(_imageToTensor, visionRfb320Result);
                        // const pfldArr = [[219.87143433094025, 124.16538900136948], [213.9596370458603, 134.74061065912247], [210.16076481342316, 145.29385885596275], [207.91317796707153, 156.43775495886803], [208.19643414020538, 167.74818700551987], [213.10527896881104, 178.31780016422272], [223.9328352212906, 187.88467752933502], [239.2291305065155, 195.48890852928162], [257.02195143699646, 200.3173063993454], [273.5507217645645, 202.1791274547577], [289.4326922893524, 200.53489792346954], [305.3161082267761, 196.30744844675064], [318.89204955101013, 189.40269303321838], [329.8438069820404, 180.58710831403732], [340.1512382030487, 170.82163536548615], [349.0013060569763, 160.27033418416977], [354.786860704422, 149.28040277957916], [256.0004470348358, 113.4042843580246], [272.26986265182495, 112.45204101502895], [288.01743960380554, 114.94263865053654], [300.9962019920349, 119.50891670584679], [309.98334562778473, 124.96102982759476], [327.9712972640991, 126.51233154535294], [337.9849818944931, 123.10183063149452], [348.044472694397, 121.04811911284924], [358.06571221351624, 120.64467880129814], [366.04695296287537, 123.1012515425682], [314.23951959609985, 133.42918854951859], [312.88633024692535, 141.0312992632389], [311.53150177001953, 148.52897334098816], [310.1744530200958, 156.16735690832138], [289.7999612092972, 160.99236232042313], [294.89745473861694, 163.1061463356018], [299.63417625427246, 165.03788667917252], [305.0948259830475, 165.69098043441772], [310.5486658811569, 165.7969856262207], [259.760427236557, 127.7814254462719], [271.30396938323975, 122.97273027896881], [284.3070504665375, 124.18803730607033], [293.3167097568512, 133.4372775554657], [282.28687024116516, 134.06236073374748], [269.735830783844, 133.01006495952606], [327.7824697494507, 137.4561319053173], [336.93103766441345, 130.05731788277626], [347.6162431240082, 129.7964451611042], [352.7049300670624, 136.69400984048843], [348.3410505056381, 139.86253821849823], [338.04297721385956, 139.7152991592884], [263.7366530895233, 171.6152698993683], [276.40012180805206, 168.72529697418213], [289.2413911819458, 167.70523411035538], [294.5124385356903, 170.3420494198799], [301.1139509677887, 170.48875498771667], [307.2676537036896, 175.34768497943878], [309.7126064300537, 181.3397232890129], [300.34187710285187, 183.8431839942932], [291.2339893579483, 184.29179096221924], [284.8437008857727, 183.27935200929642], [278.5584954023361, 181.47342443466187], [270.68491554260254, 177.64672273397446], [267.43452525138855, 171.97408616542816], [285.67847883701324, 173.08595514297485], [291.6082022190094, 174.7633249759674], [297.28900170326233, 176.07047003507614], [307.51601135730743, 180.38451838493347], [293.7081334590912, 177.66386741399765], [287.8274646997452, 176.34880661964417], [282.1051505804062, 174.49449092149734]]

                        if (pfldArr.length > 0) {

                            const irisJsonArr = await estimateIrisLandmarkModel(_imageToTensor, pfldArr);    // {"handler": {"inputNames": ["input_1"], "outputNames": ["output_eyes_contours_and_brows", "output_iris"]}}
                            // const irisJsonArr = { "leftIrisArr": [34.81892013549805, 34.393226623535156, -4.061603546142578, 40.069766998291016, 33.682064056396484, -4.119124412536621, 33.99586868286133, 29.492755889892578, -4.129571437835693, 29.435707092285156, 35.164794921875, -4.007841110229492, 35.63016891479492, 39.16047286987305, -4.06866979598999], "rightIrisArr": [34.81892013549805, 34.393226623535156, -4.061603546142578, 40.069766998291016, 33.682064056396484, -4.119124412536621, 33.99586868286133, 29.492755889892578, -4.129571437835693, 29.435707092285156, 35.164794921875, -4.007841110229492, 35.63016891479492, 39.16047286987305, -4.06866979598999] }

                            if (irisJsonArr) {
                                _accFaceDetectCnt += 1;                                                                             // ì–¼êµ´ì´ ì¸¡ì •ëœ ëˆ„ì  íšŸìˆ˜ë¥¼ Counting

                                // [STEP5] ì–¼êµ´ì´ ì •í™•í•œ ìœ„ì¹˜ì¸ì§€ ì²´í¬í•©ë‹ˆë‹¤. : 0.5 ì´ìƒ true ì´í•˜ëŠ” false
                                setIsFaceDtctYn(true);

                                // [STEP6] ì¸¡ì •ëœ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ 'FSA-NET' ëª¨ë¸ì„ ìˆ˜í–‰í•˜ì—¬ ê°’ì„ ë°˜í™˜ë°›ìŠµë‹ˆë‹¤.
                                const resultFsanet = await fsanetEstimate(_imageToTensor, visionRfb320Result);
                                // const resultFsanet = [26.119766235351562, -27.402212142944336, -2.7349319458007812]

                                // [STEP7] ì¸¡ì •ëœ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ 'HSEmotion' ëª¨ë¸ì„ ìˆ˜í–‰í•˜ì—¬ ê°’ì„ ë°˜í™˜ë°›ìŠµë‹ˆë‹¤.
                                _resultHsemotion = await hsemotionEstimate(_imageToTensor, visionRfb320Result);
                                // _resultHsemotion = { "arousalArr": [0.15133555233478546], "emotionCode": "SUP", "valenceArr": [-0.04291853681206703] }

                                // [STEP8] ì‹œì„  ì²˜ë¦¬ ë°ì´í„° ì¶”ì¶œ
                                const _gazeEstimateResult = tf.tidy(() => setLandmarkData(pfldArr, irisJsonArr));
                                // const _gazeEstimateResult = { "ear": 0.4030975866672722, "iris_radius": 3.075392723083496, "left_phi": -0.25268025514207865, "left_theta": -0.25268025514207865 }

                                // [STEP12] ì¸¡ì •ëœ ë°ì´í„°ë¥¼ ë°°ì—´ë¡œ êµ¬ì„±í•˜ë©° ìµœì¢… Tensorë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
                                const { left_theta, left_phi, ear, iris_radius } = _gazeEstimateResult;

                                // ì¸¡ì •í•˜ì§€ ëª»í•œ ê²½ìš° ë¹ˆ ê°’ìœ¼ë¡œ ì²˜ë¦¬
                                if (left_theta === 0 || left_phi === 0 || ear === 0 || iris_radius === 0) {
                                    configArr = Array(8).fill(NaN);
                                } else {
                                    // [faceInViewConfidence, yaw, pitch, roll, theta, phi, EAR, iris_radius]
                                    configArr = [visionRfb320Result[0][4], resultFsanet[0], resultFsanet[1], resultFsanet[2], left_theta, left_phi, ear, iris_radius];
                                }
                            }

                            // [STEP7-2] í™ì±„ ê´€ë ¨ ì¶”ì¶œ ê°’ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš° : ë°ì´í„° ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                            else {
                                configArr = Array(8).fill(NaN);
                            }

                        }
                        // [STEP6-2] ì–¼êµ´ì˜ ì¢Œí‘œê°’ ì¶”ì¶œì´ ëœ ê²½ìš° : PDLF ëª¨ë¸ì„ í†µí•´ì„œ ì–¼êµ´ ì¢Œí‘œê°’ ì¶”ì¶œ
                        else {
                            configArr = Array(8).fill(NaN);
                        }
                    }
                    // [STEP4-2] ì–¼êµ´ì´ íƒì§€ë˜ì§€ ì•Šì€ ê²½ìš° : ë°ì´í„° ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                    else {
                        // [STEP5] NaN í˜•íƒœë¡œ êµ¬ì„±ëœ ë°°ì—´ë¡œ êµ¬ì„±í•˜ë©° ìµœì¢… Tensorë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
                        configArr = Array(8).fill(NaN);
                        setIsFaceDtctYn(false);
                    }

                    let elapsedTime = Date.now() - loopStartTimeRef.current;    // ê²½ê³¼ ì‹œê°„
                    console.log(`ì¢…ë£Œ ì‹œê°„ - ì‹œì‘ ì‹œê°„: ${elapsedTime}`);

                    // [STPE5] LOOP_INTERVAL ê¸°ì¤€ë³´ë‹¤ ëœ ëœ ê²½ìš° Sleepìœ¼ë¡œ ì†ë„ë¥¼ ëŠ¦ì¶¥ë‹ˆë‹¤.
                    if (elapsedTime <= LOOP_INTERVAL) {
                        const remainTime = LOOP_INTERVAL - elapsedTime;         // ë‚¨ì€ ì‹œê°„
                        await commonHandler.sleep(remainTime);               // ëˆ„ë½ëœ ì‹œê°„ë§Œí¼ ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤.
                        elapsedTime += remainTime;
                    }

                    // [STEP6] ê°’ì„ ì „ë‹¬í•˜ì—¬ ë£¨í”„ë‹¹ ê°ê°ì˜ ê°’ì„ ëˆ„ì í•©ë‹ˆë‹¤.
                    calcHandler.calcLoopSum(_strtTs, _accLoopCnt, elapsedTime, _accFaceDetectCnt, _resultHsemotion, configArr)

                    tf.dispose(_imageToTensor);
                    console.log(`2. ë£¨í”„ ì¢…ë£Œ ì‹œì ì— ë©”ëª¨ë¦¬ì— ìˆëŠ” í…ì„œ ìˆ˜  ${tf.tidy(() => tf.memory().numTensors)}`)
                    // [STEP7] ëˆ„ì ëœ ë£¨í”„ì™€ ì œí•œëœ ê°¯ìˆ˜ê°€ ê°™ì€ ê²½ìš° ì´ˆê¸°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                    if (_accLoopCnt === LOOP_LIMIT_CNT + 1) _accLoopCnt = 0;
                    loopStartTimeRef.current = 0;

                }
                const endTime = performance.now(); // â± ì¢…ë£Œ ì‹œê°„
                const duration = (endTime - startTime).toFixed(2);
                console.log(`â± handleCapture ì‹¤í–‰ ì‹œê°„: ${duration}ms`);
            }

            // console.log("ğŸ“¸ ìº¡ì²˜ ê²°ê³¼ Tensor:", resizedTensor.shape);
        } catch (err) {
            console.error("âŒ ìº¡ì²˜ ì¤‘ ì˜¤ë¥˜:", err);

        }
    }


    /**
     * ëª¨ë¸ ì—°ì‚° ê´€ë¦¬ 
     */
    const modelCalcHandler = (() => {
        return {

            /**
             * FaceMesh ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì–¼êµ´ ì¸¡ì •ê°’ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
             *
             * [STEP1] TensorImageë¥¼ ì „ë‹¬ ë°›ê³  ëª¨ë¸ì´ ì¤€ë¹„ ëœ ê²½ìš° ìˆ˜í–‰í•©ë‹ˆë‹¤.
             * [STEP2] Face Landmark(FaceMesh)ì— ëŒ€í•œ ê°ì²´ë¥¼ êµ¬ì„±
             * [STEP3] ê°ì²´ë¥¼ ì „ë‹¬í•˜ì—¬ ëª¨ë¸ë¡œ Face Landmark(FaceMesh) ëª¨ë¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
             * @param {tf.Tensor3D} imageTensor í…ì„œ ì¹´ë©”ë¼ì—ì„œ ì „ë‹¬ ë°›ì€ ì´ë¯¸ì§€
             * @returns {Promise<AnnotatedPrediction[]>}
             */
            estimateMediaPipeFaceMesh: async (imageTensor: Tensor3D): Promise<AnnotatedPrediction[]> => {
                console.log("[+] estimateMediaPipeFaceMesh");
                const mediaPipeFaceMeshModel = modelManager.getMediaPipeFaceMesh;


                let _resultList: AnnotatedPrediction[] = [];

                try {
                    if (imageTensor && mediaPipeFaceMeshModel !== null) {
                        const _estimateConfig: EstimateFacesConfig = {
                            input: imageTensor,
                            returnTensors: false,
                            flipHorizontal: false,
                            predictIrises: true,
                        };

                        // âœ… ì‹œê°„ ì¸¡ì • ì‹œì‘
                        const startTime = performance.now();

                        await mediaPipeFaceMeshModel.estimateFaces(_estimateConfig)
                            .then((_resultPredctionArr: AnnotatedPrediction[]) => {
                                _resultList = _resultPredctionArr;
                                _resultPredctionArr = [];
                            })
                            .catch((err) => console.error(`[-] modelCalcHandler.estimateFaceMesh Error ${err}`));

                        const endTime = performance.now(); // âœ… ì‹œê°„ ì¸¡ì • ë
                        console.log(`ğŸ•’ FaceMesh ì¶”ë¡  ì‹œê°„: ${(endTime - startTime).toFixed(2)}ms`);
                    }
                } catch (error) {
                    console.log('[-] estimateFaceMesh :: ', error);
                }

                return _resultList;
            },


            /**
            * FSA-NET ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì— ëŒ€í•œ ì¸¡ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            * @param tensorImage
            * @param estimateArr
            * @returns {Promise<number[]>}
            */
            fsanetEstimate: async (tensorImage: Tensor3D, versionRfd320Result,): Promise<number[]> => {

                const fsanetModel = modelManager.getFSANetSession;
                let resultFaceDtct: number[] = [];

                // í•„ìˆ˜ê°’ ì¡´ì¬ ì²´í¬
                if (versionRfd320Result.length > 0) {

                    // [ê¸°ëŠ¥-1] FaceMeshë¥¼ í†µí•´ ì „ë‹¬ë°›ì€ ë°ì´í„°ë¥¼ í†µí•´ ê°’ ì„¸íŒ…
                    const [x1, y1, x2, y2, score] = versionRfd320Result[0]


                    const box = new Array(x1, y1, x2, y2); // [x1, y1, x2, y2]

                    const _topLeft: number[] = new Array(box[0], box[1]);
                    const _bottomRight: number[] = new Array(box[2], box[3]);

                    const w: number = _bottomRight[0] - _topLeft[0];
                    const h: number = _bottomRight[1] - _topLeft[1];

                    const _xw1: number = Math.trunc(Math.max(box[0] - (0.4 * w), 0));
                    const _yw1: number = Math.trunc(Math.max(box[1] - (0.4 * h), 0));
                    const _xw2: number = Math.trunc(Math.min(box[2] + (0.4 * w), tensorImage.shape[1] - 1));
                    const _yw2: number = Math.trunc(Math.min(box[3] + (0.4 * h), tensorImage.shape[0] - 1));


                    // í•´ë‹¹ ì˜ì—­ì•ˆì—ì„œ ì²˜ë¦¬í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©í›„ ì œê±°
                    const _expandDims: TensorContainer = tf.tidy(() => {

                        // [ê¸°ëŠ¥-2] posbox í˜•íƒœë¡œ ìë¥¸ í˜•íƒœ (TensorImage)
                        const _indexingResult: Tensor<Rank.R3> = tf.slice(
                            tensorImage,
                            [_yw1, _xw1, 0],
                            [_yw2 - _yw1, _xw2 - _xw1, 3]
                        );


                        // [ê¸°ëŠ¥-3] ì¸ë±ì‹±í•œ ê°’ì„ ë¦¬ì‚¬ì´ì§• í•˜ëŠ” ì‘ì—…
                        const _resizeFace: Tensor3D | Tensor4D = tf.image.resizeBilinear(_indexingResult, [64, 64]);

                        // [ê¸°ëŠ¥-4] ë…¸ë©€ë¼ì´ì¦ˆ ìˆ˜í–‰ ì‘ì—…
                        const _min = tf.min(_resizeFace);
                        const aa = tf.sub(_resizeFace, _min);
                        const bb = tf.sub(tf.max(_resizeFace), _min);
                        const cc = tf.div(aa, bb);
                        const _normalize_imagetensor = tf.mul(cc, 255);

                        // [ê¸°ëŠ¥-5] 3ì°¨ì› ë°ì´í„°ë¥¼ ì°¨ì› ë¶„ë¦¬í•˜ì—¬ ì›í•˜ëŠ” ìˆœì„œë¡œ ì¬ì¡°ë¦½
                        const _temp = tf.unstack(_normalize_imagetensor, 2);
                        const _finalFace = tf.stack([_temp[2], _temp[1], _temp[0]], 2);

                        // [ê¸°ëŠ¥-6] ëª¨ë¸ë§ì„ ìœ„í•œ ë§¨ì•ì— ì°¨ì›ì„ ì¶”ê°€í•˜ëŠ” ì‘ì—…(reshapeë¡œ ì°¨ì›ì¶”ê°€)
                        return tf.reshape(_finalFace, [1, 64, 64, 3])
                    });

                    // [STEP7] í…ì„œí”Œë¡œìš° ë°ì´í„°ë¥¼ ìë°”ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ì»¨ë²„íŒ…í•©ë‹ˆë‹¤.
                    const _configTensor = new Float32Array(_expandDims.dataSync());

                    // [STEP8] ì»¨ë²„íŒ…í•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.'
                    const feed = { "input": new OnnxTensor(_configTensor, [1, 64, 64, 3]) }

                    if (fsanetModel) {
                        const fetches: any = await fsanetModel.run(feed, fsanetModel.outputNames)

                        resultFaceDtct = CalcStudyModule.calcFsanetInfo(fetches)
                        await fsanetModel.run(feed, fsanetModel.outputNames)
                            .then((fetches: any) => {
                                resultFaceDtct = CalcStudyModule.calcFsanetInfo(fetches)
                                // tf.dispose(_boundingBox)
                                // tf.dispose(_faceInViewConfidence)
                                tf.dispose(_topLeft)
                                tf.dispose(_bottomRight)
                                tf.dispose(_expandDims)
                            })
                            .catch((err) => console.error(`modelCalcHandler.fsanetEstimate() í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤ : ${err}`));
                    } else console.error("[+] FSA-NET ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ ")
                }

                return resultFaceDtct;
            },

            /**
            * FaceMesh ì¸¡ì •í•œ ì‚¬ìš©ìì˜ ê°’ì— ëŒ€í•´ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            *
            * [STEP1] FaceMeshë¥¼ í†µí•´ ì „ë‹¬ë°›ì€ ì¸¡ì • ë°ì´í„° í†µí•´ ì–¼êµ´ ì¸¡ì •ê°’(boundingBox), ì§‘ì¤‘ë„(faceInViewConfidence)ë¥¼ ë°˜í™˜ë°›ìŠµë‹ˆë‹¤.
            * [STEP2] í•„ìš”í•œ ì˜ì—­(posbox)ì— ëŒ€í•´ì„œë§Œ ë‚¨ê¸°ê³  ìë¦…ë‹ˆë‹¤.
            * [STEP3] ìë¥¸ ì˜ì—­ì„ ì¼ì •í•œ í¬ê¸°(224, 224)ë¡œ ë¦¬ì‚¬ì´ì¦ˆ í•©ë‹ˆë‹¤.
            * [STEP4] ë¦¬ì‚¬ì´ì¦ˆëœ ì˜ì—­(ì–¼êµ´)ì— ëŒ€í•´ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            * [STEP5] PyTorch ë°ì´í„° í˜•íƒœë¥¼ Tensor ë°ì´í„° í˜•íƒœë¡œ ì»¨ë²„íŒ…í•©ë‹ˆë‹¤.
            *
            * @param {tf.Tensor3D} tensorImage : í…ì„œ ì¹´ë©”ë¼ì—ì„œ ì „ë‹¬ ë°›ì€ ì´ë¯¸ì§€
            * @param {AnnotatedPrediction[]} estimateArr
            */
            hsemotionEstimate: async (tensorImage: Tensor3D, versionRfd320Result): Promise<StudyType.ResultHsemotion> => {

                const hsemotionModel = modelManager.getHSEmotionSession;

                // [STEP1] ì—°ì‚° ê²°ê³¼ë¥¼ ë°˜í™˜í•  ê°ì²´ë¥¼ ì´ˆê¸° ì„ ì–¸í•©ë‹ˆë‹¤.
                let resultHsemotion: StudyType.ResultHsemotion = {
                    arousalArr: [],
                    valenceArr: [],
                    emotionCode: ""
                }

                try {
                    // [CASE1] íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ ë°›ì€ ê°’ì´ ì¡´ì¬í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
                    if (versionRfd320Result.length > 0 && hsemotionModel != null) {

                        const [x1, y1, x2, y2, score] = versionRfd320Result[0]

                        const box = new Array(x1, y1, x2, y2); // [x1, y1, x2, y2]

                        const _topLeft: number[] = new Array(box[0], box[1]);
                        const _bottomRight: number[] = new Array(box[2], box[3]);

                        const w: number = _bottomRight[0] - _topLeft[0];
                        const h: number = _bottomRight[1] - _topLeft[1];

                        const _xw1: number = Math.trunc(Math.max(box[0] - (0.4 * w), 0));
                        const _yw1: number = Math.trunc(Math.max(box[1] - (0.4 * h), 0));
                        const _xw2: number = Math.trunc(Math.min(box[2] + (0.4 * w), tensorImage.shape[1] - 1));
                        const _yw2: number = Math.trunc(Math.min(box[3] + (0.4 * h), tensorImage.shape[0] - 1));


                        //@ts-ignore
                        const resultInput: tf.TensorContainer = tf.tidy(() => {

                            // [STEP2] í•„ìš”í•œ ì˜ì—­(posbox)ì— ëŒ€í•´ì„œë§Œ ë‚¨ê¸°ê³  ìë¦…ë‹ˆë‹¤.
                            const _indexingResult: Tensor<Rank.R3> = tf.slice(
                                tensorImage,
                                [_yw1, _xw1, 0],
                                [_yw2 - _yw1, _xw2 - _xw1, 3]
                            );
                            // [STEP3] ìë¥¸ ì˜ì—­ì„ ì¼ì •í•œ í¬ê¸°(224,224)ë¡œ ë¦¬ì‚¬ì´ì¦ˆ í•©ë‹ˆë‹¤.
                            const _resizeFace: Tensor3D | Tensor4D = tf.image.resizeBilinear(_indexingResult, [224, 224]);


                            // [STEP4] ë¦¬ì‚¬ì´ì¦ˆëœ ì˜ì—­(ì–¼êµ´)ì— ëŒ€í•´ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                            const _resizeFaceDiv = tf.div(_resizeFace, 255)
                            const mean = [0.485, 0.456, 0.406];
                            const std = [0.229, 0.224, 0.225];
                            const [r, g, b] = tf.split(_resizeFaceDiv, 3, 2);
                            const rNormalized = tf.div(tf.sub(r, mean[0]), std[0]);
                            const gNormalized = tf.div(tf.sub(g, mean[1]), std[1]);
                            const bNormalized = tf.div(tf.sub(b, mean[2]), std[2]);
                            const normalizedTensor = tf.concat([rNormalized, gNormalized, bNormalized], 2);

                            // [STEP5] PyTorch ë°ì´í„° í˜•íƒœë¥¼ Tensor ë°ì´í„° í˜•íƒœë¡œ ì»¨ë²„íŒ…í•©ë‹ˆë‹¤.
                            const transposedTensor = tf.transpose(normalizedTensor, [2, 0, 1]);

                            // [STEP6] ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
                            const expandedTensor = tf.expandDims(transposedTensor, 0);

                            // [STEP7] í…ì„œí”Œë¡œìš° ë°ì´í„°ë¥¼ ìë°”ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ì»¨ë²„íŒ…í•©ë‹ˆë‹¤.
                            const data = new Float32Array(expandedTensor.dataSync());
                            return new OnnxTensor(data, [1, 3, 224, 224]);
                        });


                        // [STEP8] ì»¨ë²„íŒ…í•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                        const feed = { input: resultInput }
                        // @ts-ignore
                        await hsemotionModel.run(feed, hsemotionModel.outputNames)
                            .then((fetches: InferenceSession.OnnxValueMapType) => {
                                resultHsemotion = CalcStudyModule.calcHsemotionInfo(fetches);

                                console.log("resultHsemotion : ", resultHsemotion)

                            }).catch((err) => {
                                console.log(`[-] calcHsemotion Error ${err}`);
                            });

                        // tf.dispose(_boundingBox);
                        // tf.dispose(_topLeft)
                        // tf.dispose(_bottomRight);
                    }
                } catch (error) {
                    console.log(`[-] hsemotionEstimate error :: ${error}`)
                }


                return resultHsemotion;
            },
            /**
             * Hpose ì¸¡ì •
             * @param inputData
             * @returns
             */
            hPoseEstimate: async (data1: Float32Array): Promise<number> => {
                console.log("[+] hPoseEstimate")
                const hposeModel = modelManager.getHposeSession;
                let resultScore = 0;

                let inputTensor1: OnnxTensor | null = null;
                let inputTensor2: OnnxTensor | null = null;
                let tfHelperTensor: Tensor | null = null;

                try {
                    if (hposeModel) {
                        // [STEP1] tf.onesë¥¼ í™œìš©í•˜ì—¬ Int32Array ìƒì„±
                        tfHelperTensor = tf.tidy(() => tf.ones([1, 1]));
                        const data2 = new Int32Array(tfHelperTensor.dataSync());

                        // [STEP2] ONNX Tensor ìƒì„±
                        inputTensor1 = new OnnxTensor(data1.slice(), [1, 10, 8]);
                        inputTensor2 = new OnnxTensor(data2.slice(), [1, 1]);

                        const feed: { args_0: OnnxTensor | null; args_1: OnnxTensor | null } = {
                            args_0: inputTensor1,
                            args_1: inputTensor2,
                        };

                        // [STEP3] ëª¨ë¸ ì‹¤í–‰
                        // @ts-ignore
                        await hposeModel.run(feed, hposeModel.outputNames)
                            .then((fetches: InferenceSession.OnnxValueMapType | null) => {
                                if (fetches) {
                                    const result = tf.tidy(() => {
                                        const output = fetches!.output_1;
                                        const [, data2] = output.data;
                                        return Math.floor((data2 as number) * 100);
                                    });
                                    resultScore = result;

                                    // [STEP4] ONNX output ë¹„ìš°ê¸°
                                    for (const key in fetches) {
                                        (fetches as any)[key] = null;
                                    }
                                    // âœ… outputë„ ì§ì ‘ null ì²˜ë¦¬ (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ê¶Œì¥)
                                    (fetches as any).output_1 = null;
                                    fetches = null;
                                }

                                feed.args_0 = null;
                                feed.args_1 = null;
                            })
                            .catch((err) => {
                                console.error(`_hposeModel.run() í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ ë°œìƒ: ${err}`);
                            });
                    }
                } catch (error) {
                    console.log(`[-] hPoseEstimate error :: ${error}`);
                } finally {
                    // [STEP5] TensorFlow Tensor ì œê±°
                    tfHelperTensor?.dispose();
                    tfHelperTensor = null;

                    // [STEP6] ONNX Tensor ì°¸ì¡° ì œê±°
                    inputTensor1 = null;
                    inputTensor2 = null;
                }

                return resultScore;
            },
            fn_estimateBlazeFace: async (tensorImage: any) => {

                // í…ì„œ ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ëŠ”ì§€ ì²´í¬ 
                if (tensorImage) {

                    // [ê¸°ëŠ¥-1] blazeface ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì¸¡ì •ì„ ìˆ˜í–‰í•œë‹¤
                    const blaze = await blazeface.load();

                    // [ê¸°ëŠ¥-2] blazeface ëª¨ë¸ì„ í†µí•˜ì—¬ ì‚¬ëŒ ì–¼êµ´ì„ ì¸¡ì •
                    const predictions: any = await blaze.estimateFaces(tensorImage, false);

                    console.log("predictions :: ", predictions);

                    // console.log(`[+] ì¸¡ì • ì–¼êµ´ ìˆ˜ [${predictions.length}]`);

                    // [CASE1-1] ì–¼êµ´ì„ ì˜ˆì¸¡ í•œ ê²½ìš° true return
                    if (predictions.length > 0) {
                        return true;
                    }
                    // í…ì„œ ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš° 
                    else {
                        return false;
                    }
                } else {
                    return false;
                }
            },

        }
    })();



    const calcHandler = (() => {
        return {

            /**
             * ë£¨í”„ë¥¼ ìˆ˜í–‰í•˜ë©´ì„œ í•©ê³„ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
             *
             * @param {date} strtTs                                     : ì‹œì‘ì‹œê°„
             * @param {number} accLoopCnt                               : ë£¨í”„ì˜ ìˆ˜í–‰ íšŸìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
             * @param {number} loopTime                                 : ë£¨í”„ì˜ ìˆ˜í–‰ ì‹œê°„ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
             * @param {number} isFaceDectionCnt                         : ì–¼êµ´ì´ ì¸¡ì •ëœ íšŸìˆ˜
             * @param {StudyType.ResultHsemotion} resultHsEmotion       : HSEmotion ì²˜ë¦¬ ê²°ê³¼
             * @param {tf.Tensor3D} tensorImage                         : TensorCameraë¡œ ë¶€í„° ì²˜ë¦¬ëœ ë°ì´í„°
             * @param {number[]} configArr                              : FSA-NET, Gazeì—ì„œ ì²˜ë¦¬ëœ Tensor ë°ì´í„°
             */
            calcLoopSum: async (strtTs: number, accLoopCnt: number, loopTime: number, isFaceDectionCnt: number, resultHsEmotion: StudyType.ResultHsemotion, configArr: number[]) => {
                /**
                 * [CASE1-1] ìµœì¢… ì¹´ìš´íŠ¸ê°€ 10ë³´ë‹¤ ì‘ì€ ê²½ìš°
                 */
                if (accLoopCnt <= LOOP_LIMIT_CNT) {
                    console.log(" =================================== íŒŒë¼ë¯¸í„°ë¡œ ë“¤ì–´ì˜¨ ê°’ =====================/*  */======================================");
                    console.log("doSq :: ", doSq)
                    console.log("strtTs :: ", new Date(Number(strtTs)))
                    console.log("accLoopCnt :: ", accLoopCnt)
                    console.log("per Loof Time :: ", loopTime)
                    console.log("isFaceDetctionCnt :: ", isFaceDectionCnt)
                    console.log("resultHsEmotion :: ", resultHsEmotion)
                    console.log(" ===========================================================================================================");

                    // [STEP2] ì—°ì‚°ëœ Hsemotion ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                    const {
                        arousalArr: _resultArousal,
                        emotionCode: _resultEmotionCode,
                        valenceArr: _resultValence,
                    } = resultHsEmotion;

                    const { calcArrItemDigit, calcEmtnCd } = calcHandler;
                    /**
                     * [STEP3] ê°ê°ì˜ ì—°ì‚°ë°©ë²•ì— ë”°ë¼ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                     */
                    let _arousal = 0;
                    let _valence = 0;

                    // [STEP4] arousal, valenceëŠ” ì†Œìˆ˜ì  5ìë¦¬ê¹Œì§€ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
                    _arousal = calcArrItemDigit(_resultArousal, 5);
                    _valence = calcArrItemDigit(_resultValence, 5);

                    const _emtnCd = _arousal === 0 && _valence === 0 ? '' : calcEmtnCd(_arousal, _valence); // ì •ì„œì½”ë“œ ì—°ì‚° ì²˜ë¦¬

                    // [STEP4] [ìŠ¤íŠ¸ë ˆìŠ¤] ì—°ì‚°ì„ í†µí•´ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ ê°’ì„ ëˆ„ì í•©ë‹ˆë‹¤.
                    const _stress = _valence < 0 && _arousal > 0 ? 1 : 0;

                    /**
                     * [STEP5] ì—°ì‚°ëœ ê°’ì„ Stateë‚´ì— ëˆ„ì ì„ ì‹œì¼œ ë°°ì—´ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
                     */
                    const {
                        valenceArr,
                        arousalArr,
                        exprCdArr,
                        emtnCdArr,
                        atntnArr,
                        isFaceDtctArr,
                        strssArr,
                        tensorResultArr,
                        msrdTmArr,
                    } = accStudyDoDtlInfo;
                    valenceArr.push(_valence); // valence ê°’ ëˆ„ì 
                    arousalArr.push(_arousal); // arousal ê°’ ëˆ„ì 
                    exprCdArr.push(_resultEmotionCode); // í‘œì •ì½”ë“œ ê°’ ëˆ„ì 
                    emtnCdArr.push(_emtnCd); // ì •ì„œì½”ë“œ ê°’ ëˆ„ì 
                    atntnArr.push(0); // ì§‘ì¤‘ë ¥ ì ìˆ˜ ê°’ ëˆ„ì (* 10ë²ˆ ë£¨í”„ì—ì„œ ìµœì¢… ê³„ì‚°í•˜ì—¬ ì¶œë ¥ì´ ë©ë‹ˆë‹¤.)
                    strssArr.push(_stress); // ìŠ¤íŠ¸ë ˆìŠ¤ ê°’ ëˆ„ì 
                    msrdTmArr.push(loopTime); // ë£¨í”„ ì¸¡ì • ì‹œê°„ì„ ëˆ„ì 
                    isFaceDtctArr.push(isFaceDectionCnt); // ì–¼êµ´ íƒì§€ì—¬ë¶€ ê°’ ëˆ„ì 
                    // @ts-ignore
                    tensorResultArr.push(configArr); // FSA-NET, Gazeì—ì„œ ì¸¡ì •ë˜ëŠ” Tensor ê°’ ëˆ„ì 

                    // [STEP6] ìµœì¢… ëˆ„ì ëœ ë°ì´í„°ë¥¼ State ë‚´ì— ê°±ì‹ í•©ë‹ˆë‹¤.
                    setAccStudyDoDtlInfo({
                        doSq: doSq, // [ì‹¤í–‰ì‹œí€€ìŠ¤]     ìµœì´ˆ í•œë²ˆë§Œ ìˆ˜í–‰
                        msrdTmArr: msrdTmArr, // [ì¸¡ì •ëœ ì‹œê°„]    60íšŒ ë™ì•ˆì˜ ì‹œê°„ì˜ í•©ê³„ë¥¼ ë”í•©ë‹ˆë‹¤.
                        isFaceDtctArr: isFaceDtctArr, // [ì–¼êµ´íƒì§€ì—¬ë¶€]    60íšŒ ë™ì•ˆ í•œë²ˆì´ë¼ë„ ìºì¹˜ê°€ ë˜ë©´ í•´ë‹¹ ê°’ì€ 1ë¡œ ê³ ì •í•©ë‹ˆë‹¤.
                        exprCdArr: exprCdArr, // [í‘œì •ì½”ë“œ]
                        valenceArr: valenceArr, // [valence]
                        arousalArr: arousalArr, // [arousal]
                        emtnCdArr: emtnCdArr, // [ì •ì„œì½”ë“œ]
                        atntnArr: atntnArr, // [ì§‘ì¤‘ë ¥]        60íšŒê°€ ìˆ˜í–‰ë˜ëŠ” ë™ì•ˆ ì§‘ì¤‘ë ¥ ì ìˆ˜ì˜ í•©ê³„
                        strssArr: strssArr,
                        tensorResultArr: tensorResultArr, // FSANET Array
                    });

                    console.log(`2. ë£¨í”„ ì¢…ë£Œ ì‹œì ì— ë©”ëª¨ë¦¬ì— ìˆëŠ” í…ì„œ ìˆ˜  ${tf.tidy(() => tf.memory().numTensors)}`);
                } else {
                    /**
                     * [CASE1-2] ìµœì¢… ì¹´ìš´íŠ¸ ê°’ì´ 10ì¸ ê²½ìš° : í…Œì´ë¸” INSERT ìˆ˜í–‰
                     */
                    // [STEP2] Stateë‚´ì— ëˆ„ì ëœ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                    const {
                        valenceArr,
                        arousalArr,
                        msrdTmArr,
                        exprCdArr,
                        atntnArr,
                        emtnCdArr,
                        isFaceDtctArr,
                        strssArr,
                        tensorResultArr,
                    } = accStudyDoDtlInfo;

                    const msrdSecs = Math.floor((Date.now() - strtTs) / 1000);
                    // console.log("ì–¼êµ´ì´ ì—†ì„ë•ŒëŠ”??? : ", msrdSecs)
                    // console.log("ë£¨í”„ì˜ ì‹œì‘ ì‹œê°„ :: ", calcHandler.convertDateNowToHMS(strtTs))
                    // console.log("ë£¨í”„ì˜ ì¢…ë£Œ ì‹œê°„ :: ", calcHandler.convertDateNowToHMS(Date.now()))
                    // console.log("ë£¨í”„ì˜ ì¢…ë£Œ - ì‹œì‘ ìˆ˜í–‰ëœ ì‹œê°„ :: ", msrdSecs);

                    console.log("*************************************** ìµœì¢… ëˆ„ì ëœ ê°’ *************************************************************")
                    console.log("doSq :", doSq);
                    console.log("msrdTm :", msrdSecs);
                    console.log("isFaceDtctArr :", isFaceDtctArr);
                    console.log("exprCdArr :", exprCdArr);
                    console.log("valenceArr :", valenceArr);
                    console.log("arousalArr :", arousalArr);
                    console.log("emtnCdArr :", emtnCdArr);
                    console.log("atntnArr :", atntnArr);
                    console.log("strssArr :", strssArr);
                    console.log("****************************************************************************************************************")

                    /**
                     * [STEP3] State ë‚´ì— ê°€ì ¸ì˜¨ ê°’ì„ ê°ê°ì— ë§ëŠ” í‰ê· ì¹˜ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
                     */
                    // í‰ê·  ì ìˆ˜ ê³„ì‚°í•¨ìˆ˜ ìˆ˜í–‰
                    const _stress = CalcStudyModule.calcAverageStress(strssArr, LOOP_LIMIT_CNT); // ìŠ¤íŠ¸ë ˆìŠ¤ í‰ê· 
                    const _valence = CalcStudyModule.calcAverageFloat(valenceArr, LOOP_LIMIT_CNT); // valence í‰ê· 
                    const _arousal = CalcStudyModule.calcAverageFloat(arousalArr, LOOP_LIMIT_CNT); // arousal í‰ê· 
                    const _isFaceDtct = CalcStudyModule.calcAverageIsFaceDtct(isFaceDtctArr); // faceDtct í‰ê· 

                    // [STEP4] Best Code ì •ë³´ ì¶œë ¥ í•¨ìˆ˜ ìˆ˜í–‰
                    const _exprCd = CalcStudyModule.calcBestCode(CODE_GRP_CD.ExpressionCode, exprCdArr); // ì œì¼ ìµœê³ ì˜ í‘œí˜„ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                    const _emtnCd = CalcStudyModule.calcBestCode(CODE_GRP_CD.EmotionalCode, emtnCdArr); // ì œì¼ ìµœê³ ì˜ ê°ì •ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

                    const { hPoseEstimate } = modelCalcHandler;

                    // [STEP5] ì§‘ì¤‘ë ¥ì„ ì¶”ì •í•˜ì—¬ í…ì„œê°’ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
                    const resultConcent = calcHandler.concentrationEstimate(tensorResultArr.slice(-10));
                    /**
                     * [STEP6] ì§‘ì¤‘ë ¥ì„ ì¶”ì •í•©ë‹ˆë‹¤ : ì–¼êµ´ì„ í•˜ë‚˜ë„ ì¸ì‹í•˜ì§€ ëª»í•œê²½ìš°ì— hPoseë¥¼ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ
                     */
                    let _atntn = 0;
                    if (isFaceDtctArr.includes(1)) {
                        const data1 = new Float32Array(resultConcent.dataSync());
                        _atntn = await hPoseEstimate(data1);
                        /**
                         * [STEP7] ì§‘ì¤‘ë ¥ ì ìˆ˜ì— ëŒ€í•´ ìŠ¤ë¬´ë”©ì„ ìœ„í•´ ë¡œì§ ì¶”ê°€
                         */
                        if (accAtntn.length === 3) {
                            accAtntn.shift();
                            accAtntn.push(_atntn);
                        } else {
                            // ë°°ì—´ë¡œ êµ¬ì„±
                            accAtntn.push(_atntn);
                        }
                    }

                    // [STEP8] ì‚¬ìš©í•œ Tensor ë©”ëª¨ë¦¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    tf.dispose(resultConcent);

                    // [STEP9] ìµœì¢… ì¸¡ì •í•œ ì´ˆ

                    const result: StudyType.StudyDoDtlSQliteDto = {
                        doSq: doSq, // [ìˆ˜í–‰ ì‹œí€€ìŠ¤] ë³„ë„ì˜ ì—°ì‚° ì²˜ë¦¬ê°€ ì—†ìŒ
                        msrdCnt: accLoopCnt - 1, // [ì¸¡ì • íšŸìˆ˜] ë³„ë„ì˜ ì—°ì‚° ì²˜ë¦¬ê°€ ì—†ìŒ (*61íšŒ ì¸ê²½ìš° ìˆ˜í–‰ë˜ê¸°ì— ê°’ì„ 1ë¹¼ì¤ë‹ˆë‹¤.)
                        faceDtctYn: _isFaceDtct, // [ì–¼êµ´íƒì§€ì—¬ë¶€] ë³„ë„ì˜ ì—°ì‚°ì²˜ë¦¬ê°€ ì—†ìŒ
                        strss: _stress, // [ìŠ¤íŠ¸ë ˆìŠ¤] ë³„ë„ì˜ ê³„ì‚°ë²•ì„ í†µí•´ì„œ í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤.
                        valence: _valence, // [valence] í•©ê³„ì˜ í‰ê· ê°’ì„ ë„£ìŠµë‹ˆë‹¤.
                        arousal: _arousal, // [arousal] í•©ê³„ì˜ í‰ê· ê°’ì„ ë„£ìŠµë‹ˆë‹¤.
                        atntn: _atntn, // [ì§‘ì¤‘ë ¥] í•©ê³„ì˜ í‰ê· ê°’ì„ ë„£ìŠµë‹ˆë‹¤.
                        exprCd: _exprCd, // [í‘œí˜„ì½”ë“œ] ê³„ì‚°ëœ ìµœì¢… ê°’ì„ ë„£ìŠµë‹ˆë‹¤.
                        emtnCd: _emtnCd, // [ì •ì„œì½”ë“œ] ê³„ì‚°ëœ ìµœì¢… ê°’ì„ ë„£ìŠµë‹ˆë‹¤.
                        strtTs: new Date(Number(strtTs)).toString(), // [ì‹œì‘ íƒ€ì„ìŠ¤íƒ¬í”„] ë£¨í”„ ì‹œì‘ì‹œê°„
                        endTs: new Date(Number(Date.now())).toString(), // [ì¢…ë£Œ íƒ€ì„ìŠ¤íƒ¬í”„] ë£¨í”„ ì¢…ë£Œì‹œê°„
                        msrdSecs: msrdSecs, // [ë£¨í”„ìˆ˜í–‰ì‹œê°„] ì‹œì‘ ì‹œê°„ì—ì„œ ì¢…ë£Œ ì‹œì ì˜ 'ì´ˆ'ë¥¼ ë„£ìŠµë‹ˆë‹¤.
                        regTs: new Date(Number(Date.now())).toString(), // [ë“±ë¡ íƒ€ì„ìŠ¤íƒ¬í”„] INSERT ì‹œì  ì‹œê°„
                    };

                    console.log("ìµœì¢… ê²°ê³¼ :: ", result);

                    // [STEP10] [SQlite] êµ¬ì„±í•œ ë°ì´í„°ë¥¼ ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤(SQLite)ë‚´ì— ì €ì¥í•©ë‹ˆë‹¤.
                    // await TbStdyDoDtlModules.insertRowData(result);

                    /**
                     * [STEP11] Stateì˜ ëˆ„ì ëœ í•™ìŠµ ìƒì„¸ ì •ë³´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                     */
                    resetHandler.cleanUpAccStdInfo();
                    // console.log(`3. ë£¨í”„ ì¢…ë£Œ ì‹œì ì— ë©”ëª¨ë¦¬ì— ìˆëŠ” í…ì„œ ìˆ˜  ${tf.tidy(() => tf.memory().numTensors)}`)
                    // console.log("â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ ìµœì¢… ì—°ì‚° ìˆ˜í–‰ì‹œê°„ : ", msrdSecs, "ì´ˆ â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸")
                }
            },

            /**
             * arousal, valence ê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
             * @param paramArr
             * @param digit
             */
            calcArrItemDigit: (paramArr: number[] | Float32Array, digit: number): number => {
                return paramArr.length > 0 ? parseFloat(paramArr[0].toFixed(digit)) : 0;
            },


            /**
             * ê³„ì‚°ì„ í†µí•´ ì •ì„œì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
             * @param arousal
             * @param valence
             * @returns {string} ì •ì„œì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
             */
            calcEmtnCd: (arousal: number, valence: number): string => {
                let _result = '';

                // [CASE1] ì •ì„œì½”ë“œ : Enjoyment(ì¦ê±°ì›€)
                if (valence >= 0 && arousal >= 0) _result = 'ENJ';

                // [CASE2] ì •ì„œì½”ë“œ : Anger(í™”)
                if (valence < 0 && arousal >= 0) _result = 'AGR';

                // [CASE3] ì •ì„œì½”ë“œ : Boredum(ì§€ë£¨í•¨)
                if (valence < 0 && arousal < 0) _result = 'BDM';

                // [CASE4] ì •ì„œì½”ë“œ : Relax(ì´ì™„)
                if (valence >= 0 && arousal < 0) _result = 'RLX';
                return _result;
            },

            /**
             * í•©ê³„ì˜ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì§‘ì¤‘ë ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤
             * @param tensorResultArr
             */
            concentrationEstimate: (tensorResultArr: number[]) => {
                const result = tf.tidy(() => {
                    const tensorResult = tf.tensor(tensorResultArr);
                    // const tensorStack = tf.stack(tensorResult);
                    return tf.expandDims(tensorResult, 0);
                });
                return result;
            },
        }

    })();

    /**
     * ì¼ë°˜ì ì¸ í•¸ë“¤ëŸ¬
     */
    const commonHandler = (() => {
        return {

            float32ArrayToBase64Image: (floatArray: Float32Array, width: number, height: number): string => {
                // STEP1. Float32Array â†’ Uint8Array (0~255ë¡œ ì •ê·œí™”)
                const uint8Data = new Uint8Array(floatArray.length);
                for (let i = 0; i < floatArray.length; i++) {
                    uint8Data[i] = Math.min(255, Math.max(0, Math.round(floatArray[i] * 255)));
                }

                // STEP2. Tensor3D ìƒì„±: [height, width, 3]
                const imageTensor = tf.tensor3d(uint8Data, [height, width, 3], 'int32');

                // STEP3. Alpha ì±„ë„ ì¶”ê°€ (RGBAë¡œ í™•ì¥)
                const alphaChannel = tf.fill([height, width, 1], 255, 'int32');
                const rgbaTensor = tf.concat([imageTensor, alphaChannel], -1); // shape: [H, W, 4]

                // STEP4. jpeg-js ì¸ì½”ë”©ì„ ìœ„í•œ ë²„í¼ ìƒì„±
                const buffer = Buffer.from(rgbaTensor.dataSync());
                const rawImageData = { data: buffer, width, height };
                const jpegImageData = jpeg.encode(rawImageData, 100);

                // STEP5. Base64 ë³€í™˜
                const base64String = tf.util.decodeString(jpegImageData.data, 'base64');

                return `data:image/jpeg;base64,${base64String}`;
            },
            cvtTensorImgToBase64: (tensorImage: Tensor3D): string => {

                // [STEP1] ì „ë‹¬ ë°›ì€ TensorImageë¥¼ ì¢Œìš° ë°˜ì „í•©ë‹ˆë‹¤.
                const flippedTensor = tf.reverse(tensorImage, [1]);

                // [STEP2] TensorImageì—ì„œ ë†’ì´ì™€ ë„ˆë¹„ ê°’ì„ ë°˜í™˜ ë°›ìŠµë‹ˆë‹¤.
                const height: number = flippedTensor.shape[0];
                const width: number = flippedTensor.shape[1];

                // [STEP3] í•˜ë‚˜ì˜ ì°¨ì›ì„ ë§Œë“­ë‹ˆë‹¤.
                const tensor3D = tf.fill([height, width, 1], 255);

                // [STEP4] tensorImageëŠ” 3ì°¨ì›ì´ë©° ì´ì „ì— ë§Œë“  ì°¨ì›ì„ í•©ì³ì„œ 4ì°¨ì›ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
                const data = new Buffer(
                    tf.slice(
                        tf.concat([flippedTensor, tensor3D], -1), [0], [height, width, 4]).dataSync()
                )

                // [STEP5] êµ¬ì„±í•œ ë²„í¼ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
                const rawImageData = { data, width, height };

                // [ê¸°ëŠ¥-1] ì •ì œëœ Tensor ê°’ì„ í†µí•´ jpegë¡œ ì¸ì½”ë”©í•œë‹¤.
                const jpegImageData = jpeg.encode(rawImageData, 100);

                // [ê¸°ëŠ¥-2] jpeg ë°ì´í„°ë¥¼ 'base64'ë¡œ ì „í™˜í•œë‹¤.
                const imgBase64 = tf.util.decodeString(jpegImageData.data, "base64");

                return `data:image/jpeg;base64,${imgBase64}`
            },

            /**
             * ì§€ì •í•œ ì‹œê°„ë§Œí¼ ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤.
             * @param ms
             * @returns
             */
            sleep: (ms: number): Promise<void> => {
                console.log(` ===== > í•´ë‹¹ ${ms}ì´ˆ ë§Œí¼ ì ì‹œ ì‰½ë‹ˆë‹¤.. <===========`);
                return new Promise((resolve) => setTimeout(resolve, ms));
            },

            /**
             * ì•± ìƒí…Œ ë³€í™”ë¥¼ ê°ì§€í•˜ëŠ” ë¦¬ìŠ¤ë„ˆ 
             * @param {AppStateStatus} nextAppState Appì—ì„œ ë³€ê²½ëœ ìƒíƒœ ê°’ì´ ì „ë‹¬ë°›ìŒ (active, inactive, background)
             * @returns {void}
             */
            appStateChangeListener: (nextAppState: AppStateStatus): void => {
                // fetch()
                //     .then(state => {
                //         const { type, isConnected } = state;
                //         console.log("Connection type", type);
                //         console.log("Is connected?", isConnected);
                //         console.log("ì•± ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤ >>> [", nextAppState, "] ì•±ì˜ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ [", isConnected, "]");

                //         switch (nextAppState) {
                //             // [CASE1-1] ì•± ìƒíƒœê°€ "background", "inactive" ìƒíƒœì¸ ê²½ìš°: stopwatchë¥¼ ë©ˆì¶¥ë‹ˆë‹¤.
                //             case "background":
                //                 if (isActiveStopwatch) stopwatchHandler.pause();                // ìŠ¤íƒ‘ì›Œì¹˜ê°€ ì‹¤í–‰ì¤‘ì¸ ê²½ìš°ë§Œ ì´ë¥¼ ë©ˆì¶¥ë‹ˆë‹¤.
                //                 break;

                //             case "inactive":
                //                 if (isActiveStopwatch) stopwatchHandler.pause();                // ìŠ¤íƒ‘ì›Œì¹˜ë¥¼ ë©ˆì¶¥ë‹ˆë‹¤.
                //                 if (Platform.OS === "ios") preAppState.current = "inactive";    // iOSì˜ ì‘ì—…ì°½ì„ ë‚´ë¦° ê²½ìš° ì´ë¥¼ ìˆ˜í–‰
                //                 break;

                //             // [CASE1-2] ì•± ìƒíƒœê°€ "active" ìƒíƒœì¸ ê²½ìš°: stopwatchë¥¼ ì¬ê°œ í•©ë‹ˆë‹¤.
                //             case "active":
                //                 switch (Platform.OS) {
                //                     case "android":
                //                         stopwatchHandler.start();       // ë¬´ì¡°ê»€ ì‹¤í–‰ì´ ëœë‹¤.
                //                         break;

                //                     case "ios":
                //                         // ì´ì „ì— inactiveê°€ ì‹¤í–‰ë˜ê³  ìŠ¤íƒ‘ì›Œì¹˜ê°€ ìˆ˜í–‰ëœ ê²½ìš° : ìŠ¤íƒ‘ì›Œì¹˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
                //                         if (preAppState.current === "inactive" && isActiveStopwatch) {
                //                             stopwatchHandler.start();
                //                             preAppState.current = "active";         // ìƒíƒœë¥¼ ë‹¤ì‹œ í™œì„±í™”ë¡œ ë³€ê²½í•œë‹¤.
                //                         }
                //                         break;
                //                     default:
                //                         break;
                //                 }
                //                 break;
                //             default:
                //                 break;
                //         }
                //     });
            },

            /**
             * ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì²´í¬í•˜ëŠ” í•¨ìˆ˜
             */
            checkMemoryUsage: async () => {
                // const totalMemory = await DeviceInfo.getTotalMemory(); // ê¸°ê¸°ì˜ ì „ì²´ ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸ ë‹¨ìœ„)
                // const usedMemory = await DeviceInfo.getUsedMemory();   // ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸ ë‹¨ìœ„)

                // const freeMemory = totalMemory - usedMemory; // ì‚¬ìš©ëœ ë©”ëª¨ë¦¬
                // const usedMemoryPercentage = (usedMemory / totalMemory) * 100; // ì‚¬ìš©ëœ ë©”ëª¨ë¦¬ ë¹„ìœ¨

                // console.log("totalMemory-->", totalMemory / (1024 * 1024));
                // console.log("usedMemory-->", usedMemory / (1024 * 1024));
                // console.log("usedMemoryPercentage-->", usedMemoryPercentage);
            },

            /**
             * ë„¤íŠ¸ì›Œí¬ ë³€í™”ì— ëŒ€í•´ ì²´í¬í•˜ëŠ” ë¦¬ìŠ¤ë„ˆ
             * @returns
             */
            networkChangeCheckListener: (): NetInfoSubscription | any => {
                // console.log("[+] ì—°ê²° ìƒíƒœ í™•ì¸");
                // return NetInfo.addEventListener(state => {
                //     inConnectNetworkRef.current = state.isConnected!;   // ì—°ê²° ìƒíƒœë¥¼ ë³€ìˆ˜ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
                //     // ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠê²¼ì„ë•Œ, í•™ìŠµì„ ì¤‘ë‹¨ì‹œí‚¤ê³  íŒì—…ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
                //     if (!inConnectNetworkRef.current) {
                //         stopwatchHandler.pause();
                //         console.log("ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠê²¼ìŠµë‹ˆë‹¤.");
                //         Alert.alert("ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠê²¼ìŠµë‹ˆë‹¤.", " ë””ë°”ì´ìŠ¤ ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.");
                //     };
                // });
            },

            /**
             * ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠê²¼ì„ë•Œ, ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
             * @returns
             */
            disconnectNetworkAlert: (): void => {
                // console.log("ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠê²¼ìŠµë‹ˆë‹¤.");
                // Alert.alert("ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠê²¼ìŠµë‹ˆë‹¤.", " ë””ë°”ì´ìŠ¤ ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.");
                // return
            },
        }
    })();

    /**
         * ==============================================================================================================================
         * ì´ˆê¸°í™”ë¥¼ ê´€ë¦¬í•˜ëŠ” Handler
         * ==============================================================================================================================
         */
    const resetHandler = (() => {
        return {
            /**
             * Stateì— ëˆ„ì ëœ í•™ìŠµ ìƒì„¸ ì •ë³´ë¥¼ ì´ˆê¸°í™” ì‹œí‚µë‹ˆë‹¤.
             */
            cleanUpAccStdInfo: () => {
                console.log('[+] ëˆ„ì ëœ State ê°’ì„ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.');
                accStudyDoDtlInfo.valenceArr = [];
                accStudyDoDtlInfo.arousalArr = [];
                accStudyDoDtlInfo.emtnCdArr = [];
                accStudyDoDtlInfo.atntnArr = [];
                accStudyDoDtlInfo.strssArr = [];
                accStudyDoDtlInfo.exprCdArr = [];
                accStudyDoDtlInfo.isFaceDtctArr = [];
                accStudyDoDtlInfo.tensorResultArr = [];
                accStudyDoDtlInfo.msrdTmArr = [];
                accStudyDoDtlInfo.msrdCnt = 0;
            },

            /**
             * ì‚¬ìš© ì™„ë£Œí•œ ONNX ëª¨ë¸, ì¹´ë©”ë¼, ë³€ìˆ˜ë“¤ì„ ì´ˆê¸°í™” ì‹œí‚µë‹ˆë‹¤.
             */
            cleanUpStudyInfo: () => {
                const { getFSANetSession, clearFSANetModel, getHposeSession, clearHposeModel, getHSEmotionSession, clearHSEmotionModel } = modelManager

                if (getFSANetSession) clearFSANetModel()
                if (getHposeSession) clearHposeModel()
                if (getHSEmotionSession) clearHSEmotionModel();
                resetHandler.cleanUpAccStdInfo(); // ëˆ„ì ëœ ë°°ì—´ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                setIsFaceDtctYn(false);
            },

            faceMeshCleanupPrediction: (paramArr: AnnotatedPrediction[]) => {
                for (const item of paramArr) {
                    if (item.kind === 'MediaPipePredictionTensors') {
                        item.mesh.dispose();
                        item.scaledMesh.dispose();
                        item.boundingBox.topLeft.dispose();
                        item.boundingBox.bottomRight.dispose();
                        // (item.mesh as any) = null;
                        // (item.scaledMesh as any) = null;
                        // (item.boundingBox.topLeft as any) = null;
                        // (item.boundingBox.bottomRight as any) = null
                    } else if (item.kind === 'MediaPipePredictionValues') {
                        if ('dispose' in item.boundingBox.topLeft && typeof item.boundingBox.topLeft.dispose === 'function') {
                            item.boundingBox.topLeft.dispose();
                        }
                        if ('dispose' in item.boundingBox.bottomRight && typeof item.boundingBox.bottomRight.dispose === 'function') {
                            item.boundingBox.bottomRight.dispose();
                        }
                        // item.mesh = [];
                        // item.scaledMesh = [];
                        // ë¹ˆ ë°°ì—´ ì´ˆê¸°í™”
                        // for (const key in item.annotations) {
                        //     if (Array.isArray(item.annotations[key])) {
                        //         item.annotations[key] = [];
                        //     }
                        // }
                    }
                }

                // console.log('[+] faceMeshCleanupPrediction ìˆ˜í–‰ì™„ë£Œ');
            },
        };
    })();


    /**
    * TensorCameraì— ì¶œë ¥ëœ TensorImage ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ì¶œë ¥ì„ ìœ„í•œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    * @param {tf.Tensor3D} tensorImage 
    * @return {string} ì´ë¯¸ì§€ ê²½ë¡œ ë°˜í™˜
    */
    const cvtTensorImgToBase64 = (tensorImage: Tensor3D): string => {

        // [STEP1] ì „ë‹¬ ë°›ì€ TensorImageë¥¼ ì¢Œìš° ë°˜ì „í•©ë‹ˆë‹¤.
        const flippedTensor = tf.reverse(tensorImage, [1]);

        // [STEP2] TensorImageì—ì„œ ë†’ì´ì™€ ë„ˆë¹„ ê°’ì„ ë°˜í™˜ ë°›ìŠµë‹ˆë‹¤.
        const height: number = flippedTensor.shape[0];
        const width: number = flippedTensor.shape[1];

        // [STEP3] í•˜ë‚˜ì˜ ì°¨ì›ì„ ë§Œë“­ë‹ˆë‹¤.
        const tensor3D = tf.fill([height, width, 1], 255);

        // [STEP4] tensorImageëŠ” 3ì°¨ì›ì´ë©° ì´ì „ì— ë§Œë“  ì°¨ì›ì„ í•©ì³ì„œ 4ì°¨ì›ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
        const data = new Buffer(
            tf.slice(
                tf.concat([flippedTensor, tensor3D], -1), [0], [height, width, 4]).dataSync()
        )

        // [STEP5] êµ¬ì„±í•œ ë²„í¼ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        const rawImageData = { data, width, height };

        // [ê¸°ëŠ¥-1] ì •ì œëœ Tensor ê°’ì„ í†µí•´ jpegë¡œ ì¸ì½”ë”©í•œë‹¤.
        const jpegImageData = jpeg.encode(rawImageData, 100);

        // [ê¸°ëŠ¥-2] jpeg ë°ì´í„°ë¥¼ 'base64'ë¡œ ì „í™˜í•œë‹¤.
        const imgBase64 = tf.util.decodeString(jpegImageData.data, "base64");

        return `data:image/jpeg;base64,${imgBase64}`
    }




    if (device == null || !hasPermission) {
        return <Text>ì¹´ë©”ë¼ ë¡œë”© ì¤‘...</Text>;
    }

    return (
        <View style={styles.container}>
            <Camera
                ref={cameraRef}
                device={device}
                style={StyleSheet.absoluteFill}
                isActive={true}
                enableFpsGraph={true}
                photo={true}
            />

            <TouchableOpacity style={styles.captureButton} onPress={() => setCameraOn(!cameraOn)}>
                <Text style={styles.captureText}>{cameraOn ? 'â¸ ìë™ ì¤‘ì§€' : 'â–¶ï¸ ìë™ ì‹œì‘'}</Text>
            </TouchableOpacity>

            {/* {imageUri && (
                <Image source={{ uri: `file://${imageUri}` }} style={styles.previewImage} />
            )} */}

            {beforePreviewBase64Image && (
                <>
                    <Text style={{ fontSize: 30 }}>BEFORE</Text>
                    <Image
                        source={{ uri: beforePreviewBase64Image }}
                        style={styles.previewImage}
                    />
                </>
            )}

            {afterPreviewBase64Image && (
                <>
                    <Text style={{ fontSize: 30 }}>AFTER</Text>
                    <Image
                        source={{ uri: afterPreviewBase64Image }}
                        style={styles.previewBase64Image}
                    />
                </>
            )}
        </View>
    );
};

export default StudyScreen2;

const styles = StyleSheet.create({
    container: {
        flex: 1,
        backgroundColor: 'black',
        justifyContent: 'flex-end',
        alignItems: 'center',
    },
    captureButton: {
        backgroundColor: 'white',
        padding: 16,
        borderRadius: 50,
        marginBottom: 40,
    },
    captureText: {
        fontSize: 16,
        fontWeight: 'bold',
    },
    previewImage: {
        position: 'absolute',
        top: 60,
        right: 20,
        width: 100,
        height: 160,
        borderRadius: 8,
        borderWidth: 2,
        borderColor: 'white',
    },
    previewBase64Image: {
        position: 'absolute',
        top: 60,
        left: 20,
        width: 91.25,     // 100 * (292 / 320)
        height: 100,      // ê¸°ì¤€ ë†’ì´
        borderRadius: 8,
        borderWidth: 2,
        borderColor: 'white',
    },
});