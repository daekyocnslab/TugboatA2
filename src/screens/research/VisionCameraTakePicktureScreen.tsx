import { FaceDetetorType } from "../../types/FaceDetetorType";
import { useEffect, useRef, useState } from "react";
import { Image, StyleSheet, Text, View, TouchableOpacity } from "react-native";
import modelManager from "../../interceptor/ModelManager";

import { decodeJpeg } from '@tensorflow/tfjs-react-native'
import {
    Camera,
    useCameraDevice,
    useCameraPermission,
} from 'react-native-vision-camera';
import * as tf from '@tensorflow/tfjs';
import RNFS from 'react-native-fs';

const LOOP_TIME = 100;

const VisionCameraTakePicktureScreen = () => {

    const { hasPermission, requestPermission } = useCameraPermission();
    const device = useCameraDevice('front');
    const cameraRef = useRef<Camera>(null);

    const [imageUri, setImageUri] = useState<string | null>(null);
    const [cameraOn, setCameraOn] = useState<boolean>(true); // ìë™ ìº¡ì²˜ ì—¬ë¶€

    const [initModel, setInitModel] = useState<FaceDetetorType.InitModelState>({
        isLoading: false,
        isTensorReady: false,
        faceMeshModel: null,
        hsemotionModel: null,
        fsanetModel: null,
        hPoseModel: null,
    });

    useEffect(() => {
        requestPermission();
    }, []);

    useEffect(() => {
        let stopped = false;

        const loop = async () => {
            if (stopped) return;
            await handleCapture();
            setTimeout(loop, LOOP_TIME);
        };

        if (cameraOn) loop();

        return () => {
            stopped = true;
        };
    }, [cameraOn]);


    const handleCapture = async () => {
        if (!cameraRef.current) return;

        const startTime = performance.now(); // â± ì‹œì‘ ì‹œê°„

        try {
            const { getFSANetSession, getHposeSession, getHSEmotion, getFaceDetectionModel, getIrisModel } = modelManager
            const snapshot = await cameraRef.current.takeSnapshot({ quality: 85, });
            setImageUri(snapshot.path);

            const base64Data = await RNFS.readFile(snapshot.path, 'base64');
            const imageBuffer = tf.util.encodeString(base64Data, 'base64').buffer;
            const rawImageData = new Uint8Array(imageBuffer);
            const result = decodeJpeg(rawImageData);
            // const resizedTensor = tf.image.resizeBilinear(result, [292, 340]);

            // const normalized = resizedTensor.div(255.0).expandDims(0); // shape: [1, 128, 128, 3]

            const faceModel = getFaceDetectionModel;
            // console.log("getFaceDetectionModel :: ", getFaceDetectionModel)
            if (!faceModel) throw new Error("Face detection model not loaded.");

            // const inputTypedArray = normalized.dataSync(); // Float32Array


            await modelCalcHandler.estimateVersionRfb320(result)

            // const outputTypedArrays = await faceModel.run([inputTypedArray]); // âœ… ì´ í˜•íƒœê°€ ë§ìŒ
            // console.log("ğŸ” ì˜ˆì¸¡ ê²°ê³¼:", outputTypedArrays);

            // // ğŸ” ë©”ëª¨ë¦¬ ì •ë¦¬
            // tf.dispose([resizedTensor, normalized]);

            // âœ… íŒŒì¼ ì‚­ì œ
            await RNFS.unlink(snapshot.path);

            // console.log("ğŸ“¸ ìº¡ì²˜ ê²°ê³¼ Tensor:", resizedTensor.shape);
        } catch (err) {
            console.error("âŒ ìº¡ì²˜ ì¤‘ ì˜¤ë¥˜:", err);
        }
        const endTime = performance.now(); // â± ì¢…ë£Œ ì‹œê°„
        const duration = (endTime - startTime).toFixed(2);
        console.log(`â± handleCapture ì‹¤í–‰ ì‹œê°„: ${duration}ms`);
    };

    const modelCalcHandler = (() => {
        return {
            /**
             * ì–¼êµ´ íƒì§€ ìˆ˜í–‰ 
             * @param _imageToTensor 
             * @returns Promise<number[][]>
             */
            estimateVersionRfb320: async (_imageToTensor: tf.Tensor3D): Promise<any> => {


                if (modelManager.getFaceDetectionModel) {
                    try {
                        let _configTensor: Float32Array;
                        let filteredBoxes: number[][] = [];
                        let boxesArray: number[][]
                        let confidences: number[][] | string[][];
                        let boxScores: number[][];
                        // âœ… OK 
                        _configTensor = tf.tidy(() => {
                            const resizedTensor = tf.image.resizeBilinear(_imageToTensor, [240, 320]);
                            // 2. ì •ê·œí™” ë° RGB ë°°ì—´ ìœ ì§€
                            const normalizedTensor = tf.div(resizedTensor, tf.scalar(255.0));
                            // 3. ì°¨ì› ë³€í™˜ [H, W, C] -> [1, C, H, W]
                            const transposedTensor = tf.transpose(normalizedTensor, [2, 0, 1]);
                            const dataSyncRst = transposedTensor.dataSync();
                            return new Float32Array(dataSyncRst)
                        });


                        console.log(">>>", _configTensor);
                        const faceModel = modelManager.getFaceDetectionModel;



                        await faceModel.run([_configTensor]) // âœ… ì´ í˜•íƒœê°€ ë§ìŒ
                            .then((fetches) => {

                                if (fetches) {
                                    // ì›ë³¸ ì´ë¯¸ì§€ì˜ í¬ê¸°
                                    const origWidth = 320; // ì´ë¯¸ì§€ ë„ˆë¹„
                                    const origHeight = 240; // ì´ë¯¸ì§€ ë†’ì´

                                    // ì‹ ë¢°ë„ ì„ê³„ê°’ê³¼ IoU ì„ê³„ê°’
                                    const probThreshold = 0.5; // ì‹ ë¢°ë„ ì„ê³„ê°’
                                    const iouThreshold = 0.3; // IoU ì„ê³„ê°’
                                    const topK = -1; // ìƒìœ„ kê°œ ê²°ê³¼ë§Œ ìœ ì§€ (-1ì´ë©´ ëª¨ë“  ê²°ê³¼ ìœ ì§€)


                                    // let ouputBox = fetches[0]
                                    // let ouputScores = fetches[1];

                                    // // ë°•ìŠ¤ ê°œìˆ˜ ê³„ì‚°
                                    // const numBoxes = Math.floor(ouputBox!.data.length / 4);
                                    // confidences = Array.from({ length: numBoxes }, (_, i) => {
                                    //     const startIndex = i * (ouputScores!.data.length / numBoxes);
                                    //     const endIndex = startIndex + ouputScores!.data.length / numBoxes;
                                    //     // @ts-ignore
                                    //     return Array.from(ouputScores!.data.slice(startIndex, endIndex));
                                    // });

                                    // // @ts-ignore
                                    // boxesArray = Array.from({ length: numBoxes }, (_, i) => Array.from(ouputBox.data.slice(i * 4, (i + 1) * 4)));

                                    // // scoresì™€ boxesë¥¼ ê²°í•©í•˜ì—¬ [x_min, y_min, x_max, y_max, score] í˜•íƒœë¡œ ë³€í™˜
                                    // boxScores = boxesArray.map((box, idx) => {
                                    //     if (!confidences[idx] || !box) {
                                    //         console.error(`Invalid data at index ${idx}`, { confidences, boxesArray });
                                    //         return null;
                                    //     }
                                    //     return [...box, confidences[idx][1]]; // í´ë˜ìŠ¤ 1ì˜ ì‹ ë¢°ë„ ì‚¬ìš©
                                    // })
                                    //     //@ts-ignore
                                    //     .filter((box): box is number[] => box !== null);

                                    // // NMSë¥¼ í†µí•´ ê²¹ì¹˜ëŠ” ë°•ìŠ¤ ì œê±°
                                    // filteredBoxes = hardNMS(boxScores.filter(box => box[4] > probThreshold), iouThreshold, topK)
                                    //     .map(box => {
                                    //         return [
                                    //             Math.round(box[0] * origWidth), // x_min
                                    //             Math.round(box[1] * origHeight), // y_min
                                    //             Math.round(box[2] * origWidth), // x_max
                                    //             Math.round(box[3] * origHeight), // y_max
                                    //             box[4] // score
                                    //         ];
                                    //     });

                                    // for (const key in fetches) {
                                    //     (fetches as any)[key] = null;
                                    // }
                                    // // âœ… outputë„ ì§ì ‘ null ì²˜ë¦¬ (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ê¶Œì¥)
                                    // ouputBox = null;
                                    // ouputScores = null;
                                    // (fetches.boxes as any) = null;
                                    // (fetches.scores as any) = null;
                                    // // ONNX Tensor ì´ˆê¸°í™” 
                                    // fetches = null;                                     // ì‚¬ìš©í•œ ê²°ê³¼ê°’ ì œê±° 
                                }
                                // feed.input = null;
                                // feed = null as any; // ğŸ‘ˆ feed ìì²´ë„ ëª…ì‹œì ìœ¼ë¡œ ì •ë¦¬ ê°€ëŠ¥
                            })
                            .catch((err) => {
                                console.error(`versionRfb320Model.run() í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤ : ${err}`);
                            });
                    } catch (error) {
                        console.log(`[-] estimateVersionRfb320 error :: ${error}`)
                    }
                }
                // return filteredBoxes;
            }
        }
    })();




    if (device == null || !hasPermission) {
        return <Text>ì¹´ë©”ë¼ ë¡œë”© ì¤‘...</Text>;
    }

    return (
        <View style={styles.container}>
            <Camera
                ref={cameraRef}
                device={device}
                style={StyleSheet.absoluteFill}
                isActive={true}
                enableFpsGraph={true}
                photo={true}
            />

            <TouchableOpacity style={styles.captureButton} onPress={() => setCameraOn(!cameraOn)}>
                <Text style={styles.captureText}>{cameraOn ? 'â¸ ìë™ ì¤‘ì§€' : 'â–¶ï¸ ìë™ ì‹œì‘'}</Text>
            </TouchableOpacity>

            {imageUri && (
                <Image source={{ uri: `file://${imageUri}?t=${Date.now()}` }} style={styles.previewImage} />
            )}
        </View>
    );
};

export default VisionCameraTakePicktureScreen;

const styles = StyleSheet.create({
    container: {
        flex: 1,
        backgroundColor: 'black',
        justifyContent: 'flex-end',
        alignItems: 'center',
    },
    captureButton: {
        backgroundColor: 'white',
        padding: 16,
        borderRadius: 50,
        marginBottom: 40,
    },
    captureText: {
        fontSize: 16,
        fontWeight: 'bold',
    },
    previewImage: {
        position: 'absolute',
        top: 60,
        right: 20,
        width: 100,
        height: 160,
        borderRadius: 8,
        borderWidth: 2,
        borderColor: 'white',
    },
});