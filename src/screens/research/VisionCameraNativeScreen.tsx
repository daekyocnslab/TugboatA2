import { useEffect } from "react";
import { StyleSheet, Text, View } from "react-native";
import { Camera, Frame, runAtTargetFps, useCameraDevice, useCameraPermission, useFrameProcessor, VisionCameraProxy } from "react-native-vision-camera";
import modelManager from "../../interceptor/ModelManager";
import { useRunOnJS } from "react-native-worklets-core";

import * as tf from '@tensorflow/tfjs';
import '@tensorflow/tfjs-core'
import { decodeJpeg } from '@tensorflow/tfjs-react-native'

interface YUVPluginResult {
    y: number;
    u: number;
    v: number;
    r: number;
    g: number;
    b: number;
    floatData: any;
    width: number;
    height: number;
    base64: string;
}

const TARGET_FPS = 2

/**
 * Nativeì—ì„œ ë°ì´í„° ì²˜ë¦¬ ì´í›„ ì „ë‹¬í•˜ëŠ” ê²½ìš° 
 */
const VisionCameraNativeScreen = () => {

    const device = useCameraDevice('front')
    const { hasPermission, requestPermission } = useCameraPermission()

    useEffect(() => {
        requestPermission();
        loadModel();
    }, []);


    const loadModel = async () => {

        await tf.setBackend('cpu')
            .then(() => {
                console.log("[+] Load Tensorflow.js....");
                console.log("TensorFlow backend:", tf.getBackend());

                console.log("rn-webgl í™œì„±í™” ì²´í¬ : ", tf.engine().registry); // ë°±ì—”ë“œ ë“±ë¡ëœ ëª©ë¡ í™•ì¸
            })
            .catch((error) => { console.error(`[-] Tensorflow Ready Error: ${error}`); });


        const fsanetModel = modelManager.getFSANetSession;
        console.log("âœ… fsanetModel loaded: ", fsanetModel);

        const hposeModel = modelManager.getHposeSession;
        console.log("âœ… hposeModel loaded: ", hposeModel);


        console.log("âœ… FaceDetectionModel:: ", modelManager.getFaceDetectionModel);
        console.log("âœ… IrisModel:: ", modelManager.getIrisModel);
        console.log("âœ… FaceLandmarkModel:: ", modelManager.getFaceLandmarkModel);
    }
    const base64ToUint8Array = async (result: YUVPluginResult) => {
        try {

            const { width, height, floatData, base64 } = result;
            console.log(`width: ${width}  height : ${height}`)

            console.log("âœ… TensorFlow ready:", tf.getBackend());

            if (!floatData || !width || !height) {
                console.warn("âš ï¸ floatData or dimensions missing:", result);
                return null;
            }

            // const imageBuffer = tf.util.encodeString(base64, 'base64').buffer;
            // const rawImageData = new Uint8Array(imageBuffer);
            // const result22 = await decodeJpeg(rawImageData);
            // console.log("result 22 ::", result22);

            // const zerosTensor = tf.zeros([2, 3]);
            // console.log(zerosTensor.shape);  // [height, width]
            // console.log(zerosTensor.dtype);  // 'float32'

            // const tensor = tf.tensor(floatData); // floatListëŠ” number[]

            // const tensor = tf.tensor2d(floatData, [height, width]);
            // console.log(tensor.shape);  // [height, width]
            // console.log(tensor.dtype);  // 'float32'
            // const floatArray = new Float32Array(floatData);


            // console.log("floatArray.length:", floatArray.length);
            // console.log("expected:", height * width);

            // const inputTensor = tf.tensor(floatArray, [1, height, width, 1], 'float32');
            // console.log("âœ… inputTensor shape:", inputTensor.shape);

            // const data = inputTensor.dataSync();  // ê°’ ì§ì ‘ í™•ì¸
        } catch (error) {
            console.error("ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤ ::", error);

        }



        // try {
        //     const binary = decode(base64); // base64 ë¬¸ìì—´ â†’ ë°”ì´ë„ˆë¦¬ ë¬¸ìì—´
        //     const len = binary.length;
        //     const bytes = new Uint8Array(len);
        //     for (let i = 0; i < len; i++) {
        //         bytes[i] = binary.charCodeAt(i); // âœ… í•µì‹¬: ë¬¸ìì—´ì„ ë°”ì´íŠ¸ë¡œ í•´ì„
        //     }

        //     // console.log("ğŸ§ª Uint8Array sample:", bytes.slice(0, 10)); // ì• 10ê°œë§Œ ì¶œë ¥

        //     // ì •ê·œí™”
        //     const floatArray = Float32Array.from(bytes.map((v) => v / 255));

        //     // console.log("ğŸ§ª Uint8Array sample:", floatArray); // ì• 10ê°œë§Œ ì¶œë ¥

        //     console.log("âœ… TensorFlow ready:", tf.getBackend());

        //     // âœ… í…ì„œ ìƒì„± (Grayscale)
        //     const tensor = tf.tensor(floatArray, [floatArray.length, 1], 'float32');
        //     // console.log("ğŸ“¦ bytes:", bytes);
        //     // const tensor = tf.tensor(Float32Array.from(bytes.map(v => v / 255)), [height, width, 1], 'float32');


        //     console.log("tensor :: ", tensor)
        //     return bytes;
        // } catch (error) {
        //     console.log("âŒ Base64 decoding error: ", error);
        //     return new Uint8Array(); // fallback
        // }

        // try {



        //     console.log(`2. ë£¨í”„ ì¢…ë£Œ ì‹œì ì— ë©”ëª¨ë¦¬ì— ìˆëŠ” í…ì„œ ìˆ˜  ${tf.tidy(() => tf.memory().numTensors)}`)

        //     const dummyData = new Float32Array([1, 2, 3, 4]);
        //     console.log("dummyData : ", dummyData)
        //     console.log("dummyData.slice() : ", dummyData.slice())


        //     // const zero = tf.zeros([1, 64, 64, 1]);
        //     // console.log(zero);


        //     const tensor1 = tf.tensor(dummyData, [2, 2]);
        //     console.log("âœ… í…ŒìŠ¤íŠ¸ í…ì„œ ìƒì„± ì™„ë£Œ:", tensor1.dataSync);


        //     const floatArray = new Float32Array(floatData); // JSì—ì„œ íƒ€ì… ì•ˆì •ì„± í™•ë³´
        //     const tensor = tf.tensor(floatArray, [1, height, width, 1], 'float32'); // [batch, h, w, 1] í˜•íƒœ

        //     console.log("ğŸ“¦ Generated Tensor:", tensor);
        //     return tensor;
        // } catch (err) {
        //     console.error("âŒ Tensor ìƒì„± ì˜¤ë¥˜:", err);
        //     return null;
        // }


        // // console.log(`floatData : `, floatData)

    };

    // âœ… useRunOnJSë¡œ ë˜í•‘ (Worklet â†’ JS ì•ˆì „ í˜¸ì¶œìš©)
    const onRGBParsedJS = useRunOnJS(base64ToUint8Array, [base64ToUint8Array]);

    /**
     * í”„ë ˆì„ í”„ë¡œì„¸ì„œ
     * width : 640,
     * height: 480
     */
    const frameProcessor = useFrameProcessor((frame: Frame) => {
        'worklet';
        const plugin = VisionCameraProxy.initFrameProcessorPlugin('my_yuv_plugin', { foo: 1234 });
        if (plugin) {
            const result = plugin.call(frame) as unknown as YUVPluginResult;
            // âœ… Worklet â†’ JS ì•ˆì „ í˜¸ì¶œ
            onRGBParsedJS(result);
        }
    }, []);



    if (device == null || !hasPermission) {
        return <Text>ì¹´ë©”ë¼ ë¡œë”© ì¤‘...</Text>;
    }
    return (
        <View style={styles.container}>
            <Camera
                device={device}
                style={StyleSheet.absoluteFill}
                isActive={true}
                frameProcessor={frameProcessor}
                enableFpsGraph={true}
            />
        </View>
    );
}
export default VisionCameraNativeScreen


const styles = StyleSheet.create({
    container: {
        flex: 1,
        backgroundColor: 'black',
    },
});