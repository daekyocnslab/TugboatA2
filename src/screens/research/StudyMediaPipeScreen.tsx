import { FaceDetetorType } from "../../types/FaceDetetorType";
import { useEffect, useRef, useState } from "react";
import { Image, StyleSheet, Text, View, TouchableOpacity, AppStateStatus } from "react-native";
import modelManager from "../../interceptor/ModelManager";

import { decodeJpeg } from '@tensorflow/tfjs-react-native'
import {
    Camera,
    useCameraDevice,
    useCameraPermission,
} from 'react-native-vision-camera';
import RNFS from 'react-native-fs';
import { InferenceSession, Tensor as OnnxTensor } from "onnxruntime-react-native";
import { StudyType } from "@/types/StudyType";
import CalcStudyModule from "@/modules/calcStudy/CalcStudyModule";
import { NetInfoSubscription } from "@react-native-community/netinfo";
import { setLandmarkData } from "@/modules/fsanet/GazeModule";
import { AnnotatedPrediction, EstimateFacesConfig } from "@tensorflow-models/face-landmarks-detection/dist/mediapipe-facemesh";



import * as jpeg from "jpeg-js";
import { Rank, Tensor, Tensor3D, Tensor4D, TensorContainer } from "@tensorflow/tfjs-core";
import { CODE_GRP_CD } from "@/common/utils/codes/CommonCode";


import '@tensorflow/tfjs-react-native';

const tf = modelManager.tf;


const LOOP_TIME = 100;
const LOOP_LIMIT_CNT = 10; // í•™ìŠµ ìˆ˜í–‰ì¤‘ ë£¨í”„ë‹¹ í•©ê³„ë¥¼ ë‚´ê¸° ìœ„í•œ íšŸìˆ˜
const LOOP_INTERVAL = 1; // í•™ìŠµ ë£¨í”„ ì‹œê°„

const StudyMediaPipeScreen = () => {

    const [doSq, setDoSq] = useState<number>(0); // í•™ìŠµ ì‹¤í–‰ ì‹œí€€ìŠ¤
    const device = useCameraDevice('front');
    const cameraRef = useRef<Camera>(null);
    const { hasPermission, requestPermission } = useCameraPermission();
    const loopStartSecRef = useRef<number>();           // ë£¨í”„ ì‹œì‘ ì‹œê°„(ì´ˆ)
    const loopStartTimeRef = useRef<number>(0);         // ë£¨í”„ ì‹œì‘ ì‹œê°„
    const inConnectNetworkRef = useRef<boolean>(true);  // ë„¤íŠ¸ì›Œí¬ì˜ ì—°ê²° ì—¬ë¶€ë¥¼ ì²´í¬í•©ë‹ˆë‹¤.
    const [previewBase64Image, setPreviewBase64Image] = useState<string | null>(null);
    let [accAtntn] = useState<number[]>([]);


    const [imageUri, setImageUri] = useState<string | null>(null);
    const [cameraOn, setCameraOn] = useState<boolean>(true); // ìë™ ìº¡ì²˜ ì—¬ë¶€

    // ë£¨í”„ê°€ NíšŒ ìˆ˜í–‰ë˜ëŠ” ë™ì•ˆ ëˆ„ì ë˜ëŠ” í•™ìŠµ ì •ë³´
    let [accStudyDoDtlInfo, setAccStudyDoDtlInfo] = useState<StudyType.StudyDoDtlSum>({
        msrdTmArr: [], // ì¸¡ì •ëœ ì‹œê°„ - 60íšŒ ë™ì•ˆì˜ ì‹œê°„ì˜ í•©ê³„ë¥¼ ë”í•©ë‹ˆë‹¤.
        isFaceDtctArr: [], // ì–¼êµ´íƒì§€ì—¬ë¶€ - 60íšŒ ë™ì•ˆ í•œë²ˆì´ë¼ë„ ìºì¹˜ê°€ ë˜ë©´ í•´ë‹¹ ê°’ì€ 1ë¡œ ê³ ì •í•©ë‹ˆë‹¤.
        exprCdArr: [], // í‘œì •ì½”ë“œ
        valenceArr: [], // valence
        arousalArr: [], // arousal
        emtnCdArr: [], // ì •ì„œì½”ë“œ
        atntnArr: [], // ì§‘ì¤‘ë ¥ - 60íšŒê°€ ìˆ˜í–‰ë˜ëŠ” ë™ì•ˆ ì§‘ì¤‘ë ¥ ì ìˆ˜ì˜ í•©ê³„
        strssArr: [], // ìŠ¤íŠ¸ë ˆìŠ¤
        doSq: doSq, // ì‹¤í–‰ì‹œí€€ìŠ¤ - ìµœì´ˆ í•œë²ˆë§Œ ìˆ˜í–‰
        tensorResultArr: [], //
    });



    const [isFaceDtctYn, setIsFaceDtctYn] = useState<boolean>(false);               // ì–¼êµ´ íƒì§€ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥¸ ë¶ˆ(íŒŒë€ìƒ‰/ë…¸ë€ìƒ‰)ì„ ì¼œì¤ë‹ˆë‹¤.

    useEffect(() => {
        console.log("ë„ˆì˜ ì´ë¦„ì€ ", tf.getBackend())
        requestPermission();

    }, []);

    useEffect(() => {
        let stopped = false;

        const loop = async () => {
            if (stopped) return;
            await handleCapture();
            setTimeout(loop, LOOP_TIME);
        };

        if (cameraOn) loop();

        return () => {
            stopped = true;
        };
    }, [cameraOn]);


    /**
     * ì¼ì • ì‹œê°„ë§ˆë‹¤ Snapshotì„ ì°ì–´ì„œ ë°˜í™˜í•´ì£¼ëŠ” í•¨ìˆ˜ 
     * @returns 
     */
    const handleCapture = async (): Promise<void> => {

        if (!cameraRef.current) return;
        let _accTotalLoopCnt = 0;               // ë£¨í”„ì˜ ì´ ê°œìˆ˜
        let _accLoopCnt = 0;                    // 60íšŒì— ë”°ë¥¸ ë£¨í”„ ê°œìˆ˜
        let _timerId: NodeJS.Timeout; // setTimer ì‹œê°„ ê´€ë¦¬
        let _strtTs: number; // í•™ìŠµ ì‹œì‘ ì‹œê°„

        loopStartTimeRef.current = Date.now();

        const startTime = performance.now(); // â± ì‹œì‘ ì‹œê°„

        try {




            console.log(`1. ë£¨í”„ ì‹œì‘ ì‹œì ì— ë©”ëª¨ë¦¬ì— ìˆëŠ” í…ì„œ ìˆ˜  ${tf.tidy(() => tf.memory().numTensors)}`);
            loopStartTimeRef.current = Date.now();

            // [STEP1] ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ë©ˆì¶¥ë‹ˆë‹¤.
            if (!inConnectNetworkRef.current) return;

            // [STEP1] ì „ì²´ì—ì„œ ëˆ„ì  ë° ê°±ì‹  í•  ë³€ìˆ˜ê°’ë“¤ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
            _accTotalLoopCnt++; // í•™ìŠµì„ ìˆ˜í–‰ì‹œì‘ í•œ ì´í›„ ì „ì²´ ë£¨í”„ ëˆ„ì  íšŸìˆ˜
            _accLoopCnt++; // í•™ìŠµì˜ LIMIT ê°’ì— ë”°ë¥¸ ë£¨í”„ ëˆ„ì  íšŸìˆ˜
            _strtTs = Date.now();
            // ë£¨í”„ ì‹œì‘ì‹œ DB ì‹œê°„ì‹œê°„ ì €ì¥
            if (_accLoopCnt === 1) {
                loopStartSecRef.current = Math.round((Date.now() - startTime) / 1000); // stopwatchRef.current?.getNowSec();           // ë£¨í”„ ì‹œì‘ ì´ˆ
            }


            const SNAPSHOT_QUALITY_MODE = {
                HIGH: {
                    label: "ê³ í™”ì§ˆ",
                    value: 100,
                    estimatedSize: "300KB+",
                    description: "ê°€ì¥ ì„ ëª…í•œ í™”ì§ˆ, íŒŒì¼ í¬ê¸° í¼. ì •í™•ë„ ìš°ì„ ì¼ ë•Œ ì‚¬ìš©",
                },
                NORMAL: {
                    label: "ì¼ë°˜",
                    value: 85,
                    estimatedSize: "200KB",
                    description: "ê¸°ë³¸ ê¶Œì¥ í’ˆì§ˆ, ì†ë„ì™€ í™”ì§ˆ ê· í˜•",
                },
                SMALL: {
                    label: "ì €í™”ì§ˆ",
                    value: 50,
                    estimatedSize: "100KB",
                    description: "ì†ë„ ê°œì„ ì— ìœ ë¦¬, ì•½ê°„ì˜ í™”ì§ˆ ì†ì‹¤",
                },
                VSMALL: {
                    label: "ë§¤ìš° ì €í™”ì§ˆ",
                    value: 30,
                    estimatedSize: "50KB ì´í•˜",
                    description: "ìµœëŒ€ ì†ë„, ì •í™•ë„ ì†ì‹¤ ê°€ëŠ¥. ì €ì‚¬ì–‘ ë””ë°”ì´ìŠ¤ì— ì í•©",
                },
            } as const;



            /**
             * quality : 30 -> 4.5, 3.4, 4.8, 4.6
             * quality : 85 -> 4.0
             */
            const snapshot = await cameraRef.current.takeSnapshot({ quality: SNAPSHOT_QUALITY_MODE.VSMALL.value, });
            setImageUri(snapshot.path); // TODO: ì‚­ì œ ì˜ˆì •
            const resizeHeight = 320
            const resizeWidth = 292;

            const convertStart = performance.now();
            const base64Data = await RNFS.readFile(snapshot.path, 'base64');

            await RNFS.unlink(snapshot.path);           // ì„ì‹œ ì €ì¥ íŒŒì¼ íŒŒì¼ ì‚­ì œ
            const imageBuffer = new Uint8Array(tf.util.encodeString(base64Data, 'base64').buffer);

            const _imageToTensor = tf.tidy(() => {
                const decoded = decodeJpeg(imageBuffer);
                return tf.image.resizeBilinear(decoded, [resizeHeight, resizeWidth]);
            });

            const convertEnd = performance.now();
            console.log(`ğŸ“¸ base64 to Tensor ë³€í™˜ ì‹œê°„: ${(convertEnd - convertStart).toFixed(2)}ms`);


            if (!_imageToTensor) return;
            else {
                // [STESP3] ì–¼êµ´ì´ ì¸¡ì •ë˜ê±°ë‚˜ ì•ˆë˜ê±°ë‚˜ í•˜ëŠ” ê²½ìš°ì— ëŒ€í•´ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ë³€ìˆ˜ ì„ ì–¸
                let _accFaceDetectCnt: number = 0;                                                                      // ì–¼êµ´ì´ ì¸¡ì •ëœ ëˆ„ì  íšŸìˆ˜
                let _resultHsemotion: StudyType.ResultHsemotion = { arousalArr: [], valenceArr: [], emotionCode: "" };  // HSEmotion ì½”ë“œ
                let configArr: number[] = [];                                                                           // ìµœì¢… ì—°ì‚° ê²°ê³¼ê°’ì„ êµ¬ì„±í•œ ë°°ì—´

                const { estimateMediaPipeFaceMesh, fsanetEstimate, hsemotionEstimate } = modelCalcHandler;

                let _estimateArr: AnnotatedPrediction[] = await estimateMediaPipeFaceMesh(_imageToTensor);
                // console.log(_estimateArr)

                // const _estimateArr = [{ "annotations": { "leftCheek": [Array], "leftEyeIris": [Array], "leftEyeLower0": [Array], "leftEyeLower1": [Array], "leftEyeLower2": [Array], "leftEyeLower3": [Array], "leftEyeUpper0": [Array], "leftEyeUpper1": [Array], "leftEyeUpper2": [Array], "leftEyebrowLower": [Array], "leftEyebrowUpper": [Array], "lipsLowerInner": [Array], "lipsLowerOuter": [Array], "lipsUpperInner": [Array], "lipsUpperOuter": [Array], "midwayBetweenEyes": [Array], "noseBottom": [Array], "noseLeftCorner": [Array], "noseRightCorner": [Array], "noseTip": [Array], "rightCheek": [Array], "rightEyeIris": [Array], "rightEyeLower0": [Array], "rightEyeLower1": [Array], "rightEyeLower2": [Array], "rightEyeLower3": [Array], "rightEyeUpper0": [Array], "rightEyeUpper1": [Array], "rightEyeUpper2": [Array], "rightEyebrowLower": [Array], "rightEyebrowUpper": [Array], "silhouette": [Array] }, "boundingBox": { "bottomRight": [Array], "topLeft": [Array] }, "faceInViewConfidence": 1, "kind": "MediaPipePredictionValues", "mesh": [[Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array]], "scaledMesh": [[Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array], [Array]] }]

                // [STEP4-1] ì¸¡ì •ëœ ì‚¬ëŒì´ ìˆëŠ” ê²½ìš° : ë°ì´í„° ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                if (_estimateArr.length > 0 && parseFloat(_estimateArr[0].faceInViewConfidence.toFixed(1)) >= 0.5) {
                    _accFaceDetectCnt += 1; // ì–¼êµ´ì´ ì¸¡ì •ëœ ëˆ„ì  íšŸìˆ˜ë¥¼ Counting

                    // [STEP5] ì–¼êµ´ì´ ì •í™•í•œ ìœ„ì¹˜ì¸ì§€ ì²´í¬í•©ë‹ˆë‹¤. : 0.5 ì´ìƒ true ì´í•˜ëŠ” false
                    setIsFaceDtctYn(true);

                    // [STEP6] ì¸¡ì •ëœ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ 'FSA-NET' ëª¨ë¸ì„ ìˆ˜í–‰í•˜ì—¬ ê°’ì„ ë°˜í™˜ë°›ìŠµë‹ˆë‹¤.
                    const resultFsanet = await fsanetEstimate(_imageToTensor, _estimateArr,);

                    // [STEP7] ì¸¡ì •ëœ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ 'HSEmotion' ëª¨ë¸ì„ ìˆ˜í–‰í•˜ì—¬ ê°’ì„ ë°˜í™˜ë°›ìŠµë‹ˆë‹¤.
                    _resultHsemotion = await hsemotionEstimate(_imageToTensor, _estimateArr);

                    // [STEP8] ì‹œì„  ì²˜ë¦¬ ë°ì´í„° ì¶”ì¶œ
                    const _gazeEstimateResult = tf.tidy(() => setLandmarkData(_estimateArr));

                    // [STEP9] ì¸¡ì •ëœ ë°ì´í„°ë¥¼ ë°°ì—´ë¡œ êµ¬ì„±í•˜ë©° ìµœì¢… Tensorë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
                    const { left_theta, left_phi, ear, iris_radius } = _gazeEstimateResult;

                    console.log('[+][+][+][+][+][+][+][+] ì–¼êµ´ì´ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤!!!![+][+][+][+][+][+][+][+][+][+]');
                    // [faceInViewConfidence, yaw, pitch, roll, theta, phi, EAR, iris_radius]
                    configArr = [
                        _estimateArr[0].faceInViewConfidence,
                        resultFsanet[0],
                        resultFsanet[1],
                        resultFsanet[2],
                        left_theta,
                        left_phi,
                        ear,
                        iris_radius,
                    ];



                    console.log("ìµœì¢… ê²°ê³¼ :: ", configArr)

                }
                // [STEP4-2] ì¸¡ì •ëœ ì‚¬ëŒì´ ì—†ëŠ” ê²½ìš° : ë°ì´í„° ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                else {
                    // [STEP5] NaN í˜•íƒœë¡œ êµ¬ì„±ëœ ë°°ì—´ë¡œ êµ¬ì„±í•˜ë©° ìµœì¢… Tensorë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
                    configArr = Array(8).fill(NaN);
                    setIsFaceDtctYn(false);
                }

                // cleanup - _estimateArr
                resetHandler.faceMeshCleanupPrediction(_estimateArr);

                let elapsedTime = Date.now() - loopStartTimeRef.current; // ê²½ê³¼ ì‹œê°„
                console.log(`ì¢…ë£Œ ì‹œê°„ - ì‹œì‘ ì‹œê°„: ${elapsedTime}`);

                // [STPE5] LOOP_INTERVAL ê¸°ì¤€ë³´ë‹¤ ëœ ëœ ê²½ìš° Sleepìœ¼ë¡œ ì†ë„ë¥¼ ëŠ¦ì¶¥ë‹ˆë‹¤.
                if (elapsedTime <= LOOP_INTERVAL) {
                    const remainTime = LOOP_INTERVAL - elapsedTime; // ë‚¨ì€ ì‹œê°„
                    await commonHandler.sleep(remainTime); // ëˆ„ë½ëœ ì‹œê°„ë§Œí¼ ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤.
                    elapsedTime += remainTime;
                }

                // // [STEP6] ê°’ì„ ì „ë‹¬í•˜ì—¬ ë£¨í”„ë‹¹ ê°ê°ì˜ ê°’ì„ ëˆ„ì í•©ë‹ˆë‹¤.
                calcHandler.calcLoopSum(
                    _strtTs,
                    _accLoopCnt,
                    elapsedTime,
                    _accFaceDetectCnt,
                    _resultHsemotion,
                    _imageToTensor,
                    configArr,
                );
                tf.dispose(_imageToTensor);

                // [STEP7] ëˆ„ì ëœ ë£¨í”„ì™€ ì œí•œëœ ê°¯ìˆ˜ê°€ ê°™ì€ ê²½ìš° ì´ˆê¸°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                if (_accLoopCnt === LOOP_LIMIT_CNT + 1) _accLoopCnt = 0;
                // console.log("===============================================================")
            }

            const endTime = performance.now(); // â± ì¢…ë£Œ ì‹œê°„
            const duration = (endTime - startTime).toFixed(2);
            console.log(`â± handleCapture ì‹¤í–‰ ì‹œê°„: ${duration}ms`);


            // console.log("ğŸ“¸ ìº¡ì²˜ ê²°ê³¼ Tensor:", resizedTensor.shape);
        } catch (err) {
            console.error("âŒ ìº¡ì²˜ ì¤‘ ì˜¤ë¥˜:", err);

        }
    }


    /**
     * ëª¨ë¸ ì—°ì‚° ê´€ë¦¬ 
     */
    const modelCalcHandler = (() => {
        return {

            /**
             * FaceMesh ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì–¼êµ´ ì¸¡ì •ê°’ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
             *
             * [STEP1] TensorImageë¥¼ ì „ë‹¬ ë°›ê³  ëª¨ë¸ì´ ì¤€ë¹„ ëœ ê²½ìš° ìˆ˜í–‰í•©ë‹ˆë‹¤.
             * [STEP2] Face Landmark(FaceMesh)ì— ëŒ€í•œ ê°ì²´ë¥¼ êµ¬ì„±
             * [STEP3] ê°ì²´ë¥¼ ì „ë‹¬í•˜ì—¬ ëª¨ë¸ë¡œ Face Landmark(FaceMesh) ëª¨ë¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
             * @param {tf.Tensor3D} imageTensor í…ì„œ ì¹´ë©”ë¼ì—ì„œ ì „ë‹¬ ë°›ì€ ì´ë¯¸ì§€
             * @returns {Promise<AnnotatedPrediction[]>}
             */
            estimateMediaPipeFaceMesh: async (imageTensor: Tensor3D): Promise<AnnotatedPrediction[]> => {
                console.log("[+] estimateMediaPipeFaceMesh");

                const mediaPipeFaceMeshModel = modelManager.getMediaPipeFaceMesh;
                let _resultList: AnnotatedPrediction[] = [];

                try {
                    if (imageTensor && mediaPipeFaceMeshModel !== null) {
                        const _estimateConfig: EstimateFacesConfig = {
                            input: imageTensor,
                            returnTensors: false,
                            flipHorizontal: false,
                            predictIrises: true,
                        };

                        // âœ… ì‹œê°„ ì¸¡ì • ì‹œì‘
                        const startTime = performance.now();

                        await mediaPipeFaceMeshModel.estimateFaces(_estimateConfig)
                            .then((_resultPredctionArr: AnnotatedPrediction[]) => {
                                _resultList = _resultPredctionArr;
                                _resultPredctionArr = [];
                            })
                            .catch((err) => console.error(`[-] modelCalcHandler.estimateFaceMesh Error ${err}`));

                        const endTime = performance.now(); // âœ… ì‹œê°„ ì¸¡ì • ë
                        console.log(`ğŸ•’ FaceMesh ì¶”ë¡  ì‹œê°„: ${(endTime - startTime).toFixed(2)}ms`);
                    }
                } catch (error) {
                    console.log('[-] estimateFaceMesh :: ', error);
                }

                return _resultList;
            },
            /**
             * ì–¼êµ´ íƒì§€ ìˆ˜í–‰ 
             * @param _imageToTensor 
             * @returns Promise<number[][]>
             */
            estimateFaceDetect: async (_imageToTensor: Tensor3D): Promise<number[][]> => {
                const faceModel = modelManager.getFaceDetectionModel;
                if (!faceModel) throw new Error("Face detection model not loaded.");

                let _configTensor: Float32Array;
                let filteredBoxes: number[][] = [];
                let boxesArray: number[][]
                let confidences: number[][] | string[][];
                let boxScores: number[][];

                if (faceModel) {
                    try {

                        _configTensor = tf.tidy(() => {
                            const resizedTensor = tf.image.resizeBilinear(_imageToTensor, [240, 320]);
                            // 2. ì •ê·œí™” ë° RGB ë°°ì—´ ìœ ì§€
                            const normalizedTensor = tf.div(resizedTensor, tf.scalar(255.0));
                            // 3. ì°¨ì› ë³€í™˜ [H, W, C] -> [1, C, H, W]
                            const transposedTensor = tf.transpose(normalizedTensor, [2, 0, 1]);
                            const dataSyncRst = transposedTensor.dataSync();
                            return new Float32Array(dataSyncRst)
                        });

                        // inputOnnxTensor = new Tensor(_configTensor.slice(), [1, 3, 240, 320]);
                        // let feed: { input: Tensor | null } = { "input": inputOnnxTensor };

                        console.log(">>>", _configTensor);

                        await faceModel.run([_configTensor]) // âœ… ì´ í˜•íƒœê°€ ë§ìŒ
                            .then((fetches) => {

                                if (fetches) {
                                    // ì›ë³¸ ì´ë¯¸ì§€ì˜ í¬ê¸°
                                    const origWidth = 320; // ì´ë¯¸ì§€ ë„ˆë¹„
                                    const origHeight = 240; // ì´ë¯¸ì§€ ë†’ì´

                                    // ì‹ ë¢°ë„ ì„ê³„ê°’ê³¼ IoU ì„ê³„ê°’
                                    const probThreshold = 0.5; // ì‹ ë¢°ë„ ì„ê³„ê°’
                                    const iouThreshold = 0.3; // IoU ì„ê³„ê°’
                                    const topK = -1; // ìƒìœ„ kê°œ ê²°ê³¼ë§Œ ìœ ì§€ (-1ì´ë©´ ëª¨ë“  ê²°ê³¼ ìœ ì§€)


                                    // let ouputBox: Tensor | null = fetches.boxes;
                                    // let ouputScores: Tensor | null = fetches.scores;

                                    // // ë°•ìŠ¤ ê°œìˆ˜ ê³„ì‚°
                                    // const numBoxes = Math.floor(ouputBox!.data.length / 4);
                                    // confidences = Array.from({ length: numBoxes }, (_, i) => {
                                    //     const startIndex = i * (ouputScores!.data.length / numBoxes);
                                    //     const endIndex = startIndex + ouputScores!.data.length / numBoxes;
                                    //     // @ts-ignore
                                    //     return Array.from(ouputScores!.data.slice(startIndex, endIndex));
                                    // });

                                    // // @ts-ignore
                                    // boxesArray = Array.from({ length: numBoxes }, (_, i) => Array.from(ouputBox.data.slice(i * 4, (i + 1) * 4)));

                                    // // scoresì™€ boxesë¥¼ ê²°í•©í•˜ì—¬ [x_min, y_min, x_max, y_max, score] í˜•íƒœë¡œ ë³€í™˜
                                    // boxScores = boxesArray.map((box, idx) => {
                                    //     if (!confidences[idx] || !box) {
                                    //         console.error(`Invalid data at index ${idx}`, { confidences, boxesArray });
                                    //         return null;
                                    //     }
                                    //     return [...box, confidences[idx][1]]; // í´ë˜ìŠ¤ 1ì˜ ì‹ ë¢°ë„ ì‚¬ìš©
                                    // })
                                    //     //@ts-ignore
                                    //     .filter((box): box is number[] => box !== null);

                                    // // NMSë¥¼ í†µí•´ ê²¹ì¹˜ëŠ” ë°•ìŠ¤ ì œê±°
                                    // filteredBoxes = calcHandler.hardNMS(boxScores.filter(box => box[4] > probThreshold), iouThreshold, topK)
                                    //     .map(box => {
                                    //         return [
                                    //             Math.round(box[0] * origWidth), // x_min
                                    //             Math.round(box[1] * origHeight), // y_min
                                    //             Math.round(box[2] * origWidth), // x_max
                                    //             Math.round(box[3] * origHeight), // y_max
                                    //             box[4] // score
                                    //         ];
                                    //     });
                                }
                            })
                            .catch((err) => {
                                console.error(`versionRfb320Model.run() í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤ : ${err}`);
                            });
                    } catch (error) {
                        console.log(`[-] estimateVersionRfb320 error :: ${error}`)
                    }
                }
                return filteredBoxes;
            },
            /**
             * í™ì±„ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
             * @param imageToTensor 
             * @param pfldArr 
             * @returns 
             */
            estimateIrisLandmark: async (imageToTensor: Tensor3D, pfldArr: number[][]): Promise<{ leftIrisArr: number[]; rightIrisArr: any[]; }> => {
                const resultObj = { leftIrisArr: [], rightIrisArr: [] }
                // const faceModel = modelManager.getFaceDetectionModel;

                // if (pfldArr.length > 0) {

                //     let outputIrisLeftArr = [];
                //     let outputIrisRightArr = [];

                //     // 1. ì™¼ìª½ ë° ì˜¤ë¥¸ìª½ ëˆˆ ì˜ì—­ ì¶”ì¶œ
                //     const { leftEye, rightEye } = calcHandler.extractEyeRegions(pfldArr, imageToTensor);

                //     let leftTensor = new Tensor(leftEye, [1, 64, 64, 3]);
                //     let rightTensor = new Tensor(rightEye, [1, 64, 64, 3])

                //     let feed1: { input_1: Tensor | null } = { "input_1": leftTensor };
                //     let feed2: { input_1: Tensor | null } = { "input_1": rightTensor };

                //     if (initModel.irisLandmarkModel) {
                //         try {
                //             //@ts-ignore
                //             await initModel.irisLandmarkModel.run(feed1, initModel.irisLandmarkModel.outputNames)
                //                 .then((fetches: InferenceSession.OnnxValueMapType | null) => {
                //                     feed1.input_1 = null;                                  // ì°¸ì¡° ì œê±° (input ê°’ ì œê±°)
                //                     (feed1 as any) = null;
                //                     (leftTensor as any) = null;

                //                     if (fetches) {
                //                         let outputIris = fetches.output_iris.data;
                //                         (fetches.output_iris as any) = null;                        // í° ë°ì´í„° ì´ˆê¸°í™”
                //                         (fetches.output_eyes_contours_and_brows as any) = null;     // í° ë°ì´í„° ì´ˆê¸°í™”
                //                         // @ts-ignore
                //                         outputIrisLeftArr = outputIris.slice();

                //                         for (const key in fetches) {
                //                             (fetches as any)[key] = null;
                //                         }
                //                         outputIris = [];                                    // ì‚¬ìš©ëœ ë°°ì—´ ì´ˆê¸°í™”
                //                     }
                //                     fetches = null;

                //                 })
                //                 .catch((err) => {
                //                     console.error(`initModel.irisLandmarkModel.run() Model1 í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤ : ${err}`);
                //                 });

                //             //@ts-ignore
                //             await initModel.irisLandmarkModel.run(feed2, initModel.irisLandmarkModel.outputNames)
                //                 .then((fetches: InferenceSession.OnnxValueMapType | null) => {
                //                     feed2.input_1 = null;                                  // ì°¸ì¡° ì œê±° (input ê°’ ì œê±°)
                //                     (feed2 as any) = null;
                //                     (rightTensor as any) = null;

                //                     if (fetches) {
                //                         let outputIris = fetches.output_iris.data;
                //                         (fetches.output_iris as any) = null;                        // í° ë°ì´í„° ì´ˆê¸°í™”
                //                         (fetches.output_eyes_contours_and_brows as any) = null;     // í° ë°ì´í„° ì´ˆê¸°í™”

                //                         // @ts-ignore
                //                         outputIrisRightArr = outputIris.slice();

                //                         outputIris = [];                                // ì‚¬ìš©ëœ ë°°ì—´ ì´ˆê¸°í™”
                //                     }
                //                 })
                //                 .catch((err) => {
                //                     console.error(`initModel.irisLandmarkModel.run() Model2 í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤ : ${err}`);
                //                 });
                //         } catch (error) {
                //             console.log(`[-] estimateIrisLandmark error :: ${error}`);
                //         } finally {
                //             resultObj.leftIrisArr = outputIrisLeftArr;
                //             resultObj.rightIrisArr = outputIrisRightArr;
                //             // ë©”ëª¨ë¦¬ ì •ë¦¬
                //             outputIrisLeftArr = [];
                //             outputIrisRightArr = [];
                //             outputIrisLeftArr.length = 0;
                //             outputIrisRightArr.length = 0;
                //         }
                //     }
                // }
                return resultObj;
            },
            /**
             * PDLF ëª¨ë¸ì„ í†µí•´ì„œ ì–¼êµ´ ì¢Œí‘œê°’ ì¶”ì¶œ
             * @param imageToTensor 
             * @param result 
             * @returns Promise<number[][]>
             */
            estimatePfld: async (imageToTensor: Tensor3D, versionRfd320Result: number[][]): Promise<number[][]> => {

                let resultPfld: number[][] = [];



                // try {

                //     if (initModel.pfldModel) {
                //         const [x, y, width, height] = versionRfd320Result[0]


                //         // í•´ë‹¹ ì˜ì—­ì•ˆì—ì„œ ì²˜ë¦¬í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©í›„ ì œê±°
                //         const _configTensor = tf.tidy(() => {
                //             const box = new Array(x, y, width, height); // [x1, y1, x2, y2]

                //             const _topLeft: number[] = new Array(box[0], box[1]);
                //             const _bottomRight: number[] = new Array(box[2], box[3]);

                //             const w: number = _bottomRight[0] - _topLeft[0];
                //             const h: number = _bottomRight[1] - _topLeft[1];

                //             const _xw1: number = Math.trunc(Math.max(box[0] - (0.4 * w), 0));
                //             const _yw1: number = Math.trunc(Math.max(box[1] - (0.4 * h), 0));
                //             const _xw2: number = Math.trunc(Math.min(box[2] + (0.4 * w), imageToTensor.shape[1] - 1));
                //             const _yw2: number = Math.trunc(Math.min(box[3] + (0.4 * h), imageToTensor.shape[0] - 1));


                //             // [ê¸°ëŠ¥-2] posbox í˜•íƒœë¡œ ìë¥¸ í˜•íƒœ (TensorImage)
                //             const _indexingResult: tf.Tensor<tf.Rank.R3> = tf.slice(
                //                 imageToTensor,
                //                 [_yw1, _xw1, 0],
                //                 [_yw2 - _yw1, _xw2 - _xw1, 3]
                //             );

                //             // 2. í¬ê¸° ì¡°ì •
                //             const _resizeFace = tf.image.resizeBilinear(_indexingResult, [112, 112]);

                //             // 3. ì •ê·œí™”: [0, 1] ë²”ìœ„ë¡œ ë³€í™˜
                //             const normalizedTensor = tf.div(_resizeFace, tf.scalar(255.0));

                //             // [ê¸°ëŠ¥-6] ëª¨ë¸ë§ì„ ìœ„í•œ ë§¨ì•ì— ì°¨ì›ì„ ì¶”ê°€í•˜ëŠ” ì‘ì—…(reshapeë¡œ ì°¨ì›ì¶”ê°€)
                //             const _expandDims = tf.reshape(normalizedTensor, [1, 112, 112, 3])

                //             // [STEP7] í…ì„œí”Œë¡œìš° ë°ì´í„°ë¥¼ ìë°”ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ì»¨ë²„íŒ…í•©ë‹ˆë‹¤.
                //             return new Float32Array(_expandDims.dataSync());

                //         });

                //         let feed: { input: Tensor | null } = { "input": new Tensor(_configTensor.slice(), [1, 3, 112, 112]) };

                //         //@ts-ignore
                //         await initModel.pfldModel.run(feed, initModel.pfldModel.outputNames)
                //             .then((fetches: InferenceSession.OnnxValueMapType | null) => {

                //                 if (fetches) {
                //                     const result = Array.from(
                //                         { length: fetches.output.data.length / 2 }, (_, i) => [fetches!.output.data[i * 2], fetches!.output.data[i * 2 + 1]]);

                //                     // @ts-ignore
                //                     const restoredLandmarks = result.map(([nx, ny]: [number, number]) => {
                //                         const originalX = nx * width + x; // x ë³µì›
                //                         const originalY = ny * height + y; // y ë³µì›
                //                         return [originalX, originalY];
                //                     });
                //                     resultPfld = restoredLandmarks
                //                 }
                //                 for (const key in fetches) {
                //                     (fetches as any)[key] = null;
                //                 }
                //                 // input/out ONNX Tensor ë¹„ìš°ê¸°
                //                 feed.input = null;                                  // ì°¸ì¡° ì œê±° (input ê°’ ì œê±°)
                //                 fetches = null;

                //             })
                //             .catch((err) => {
                //                 console.error(`pfldModel.run() í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤ : ${err}`);
                //             });
                //     }
                // }
                // catch (error) {
                //     console.log(`[-] estimatePfld error :: ${error}`)
                // }

                return resultPfld
            },
            /**
                 * FSA-NET ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì— ëŒ€í•œ ì¸¡ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                 * @param tensorImage
                 * @param estimateArr
                 * @param {NodeJS.Timeout} timer
                 * @returns {Promise<number[]>}
                 */
            fsanetEstimate: async (
                tensorImage: Tensor3D,
                estimateArr: AnnotatedPrediction[],
            ): Promise<number[]> => {
                console.log("[+] fsanetEstimate")
                let resultFaceDtct: number[] = [];

                const fsanetModel = modelManager.getFSANetSession;
                // í•„ìˆ˜ê°’ ì¡´ì¬ ì²´í¬
                if (estimateArr.length > 0) {
                    // [ê¸°ëŠ¥-1] FaceMeshë¥¼ í†µí•´ ì „ë‹¬ë°›ì€ ë°ì´í„°ë¥¼ í†µí•´ ê°’ ì„¸íŒ…
                    // boundingBox : ì–¼êµ´ ì¸¡ì •ê°’ , faceInViewConfidence : ì§‘ì¤‘ë„
                    const { boundingBox: _boundingBox, faceInViewConfidence: _faceInViewConfidence } = estimateArr[0];
                    const { topLeft: _topLeft, bottomRight: _bottomRight } = _boundingBox;

                    const w: number = _bottomRight[0] - _topLeft[0];
                    const h: number = _bottomRight[1] - _topLeft[1];

                    const _xw1: number = Math.trunc(Math.max(parseInt(_topLeft[0]) - 0.4 * w, 0));
                    const _yw1: number = Math.trunc(Math.max(parseInt(_topLeft[1]) - 0.4 * h, 0));
                    const _xw2: number = Math.trunc(Math.min(parseInt(_bottomRight[0]) + 0.4 * w, tensorImage.shape[1] - 1));
                    const _yw2: number = Math.trunc(Math.min(parseInt(_bottomRight[1]) + 0.4 * h, tensorImage.shape[0] - 1));

                    tf.dispose([_topLeft, _bottomRight]);
                    // í•´ë‹¹ ì˜ì—­ì•ˆì—ì„œ ì²˜ë¦¬í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©í›„ ì œê±°
                    const _expandDims: TensorContainer = tf.tidy(() => {
                        // [ê¸°ëŠ¥-2] posbox í˜•íƒœë¡œ ìë¥¸ í˜•íƒœ (TensorImage)
                        const _indexingResult: Tensor<Rank.R3> = tf.slice(
                            tensorImage,
                            [_yw1, _xw1, 0],
                            [_yw2 - _yw1, _xw2 - _xw1, 3],
                        );
                        // [ê¸°ëŠ¥-3] ì¸ë±ì‹±í•œ ê°’ì„ ë¦¬ì‚¬ì´ì§• í•˜ëŠ” ì‘ì—…
                        const _resizeFace: Tensor3D | Tensor4D = tf.image.resizeBilinear(_indexingResult, [64, 64]);

                        // [ê¸°ëŠ¥-4] ë…¸ë©€ë¼ì´ì¦ˆ ìˆ˜í–‰ ì‘ì—…
                        const _min = tf.min(_resizeFace);
                        const aa = tf.sub(_resizeFace, _min);
                        const bb = tf.sub(tf.max(_resizeFace), _min);
                        const cc = tf.div(aa, bb);
                        const _normalize_imagetensor = tf.mul(cc, 255);

                        // [ê¸°ëŠ¥-5] 3ì°¨ì› ë°ì´í„°ë¥¼ ì°¨ì› ë¶„ë¦¬í•˜ì—¬ ì›í•˜ëŠ” ìˆœì„œë¡œ ì¬ì¡°ë¦½
                        const _temp = tf.unstack(_normalize_imagetensor, 2);
                        const _finalFace = tf.stack([_temp[2], _temp[1], _temp[0]], 2);

                        tf.dispose([_indexingResult, _resizeFace, _min, aa, bb, cc, _normalize_imagetensor, _temp]);

                        // [ê¸°ëŠ¥-6] ëª¨ë¸ë§ì„ ìœ„í•œ ë§¨ì•ì— ì°¨ì›ì„ ì¶”ê°€í•˜ëŠ” ì‘ì—…(reshapeë¡œ ì°¨ì›ì¶”ê°€)
                        return tf.reshape(_finalFace, [1, 64, 64, 3]);
                    });

                    // [STEP7] í…ì„œí”Œë¡œìš° ë°ì´í„°ë¥¼ ìë°”ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ì»¨ë²„íŒ…í•©ë‹ˆë‹¤.
                    const _configTensor = new Float32Array(_expandDims.dataSync());

                    tf.dispose(_expandDims);

                    // [STEP8] ì»¨ë²„íŒ…í•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.'
                    let feed: { input: OnnxTensor | null } = { input: new OnnxTensor(_configTensor.slice(), [1, 64, 64, 3]), };


                    if (fsanetModel) {
                        //@ts-ignore
                        await fsanetModel.run(feed, fsanetModel.outputNames)
                            .then((fetches: InferenceSession.OnnxValueMapType | null) => {
                                //@ts-ignore
                                resultFaceDtct = tf.tidy(() => CalcStudyModule.calcFsanetInfo(fetches));
                                for (const key in fetches) {
                                    (fetches as any)[key] = null;
                                }
                                // input/out ONNX Tensor ë¹„ìš°ê¸°
                                feed.input = null; // ì°¸ì¡° ì œê±° (input ê°’ ì œê±°)
                                fetches = null; // ì°¸ì¡° ì œê±° (result ê°’ ì œê±°)
                            })
                            .catch((err) => console.error(`modelCalcHandler.fsanetEstimate() í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤ : ${err}`));
                    } else console.error('[+] FSA-NET ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ ');
                }

                return resultFaceDtct;
            },

            /**
                  * FaceMesh ì¸¡ì •í•œ ì‚¬ìš©ìì˜ ê°’ì— ëŒ€í•´ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                  *
                  * [STEP1] FaceMeshë¥¼ í†µí•´ ì „ë‹¬ë°›ì€ ì¸¡ì • ë°ì´í„° í†µí•´ ì–¼êµ´ ì¸¡ì •ê°’(boundingBox), ì§‘ì¤‘ë„(faceInViewConfidence)ë¥¼ ë°˜í™˜ë°›ìŠµë‹ˆë‹¤.
                  * [STEP2] í•„ìš”í•œ ì˜ì—­(posbox)ì— ëŒ€í•´ì„œë§Œ ë‚¨ê¸°ê³  ìë¦…ë‹ˆë‹¤.
                  * [STEP3] ìë¥¸ ì˜ì—­ì„ ì¼ì •í•œ í¬ê¸°(224, 224)ë¡œ ë¦¬ì‚¬ì´ì¦ˆ í•©ë‹ˆë‹¤.
                  * [STEP4] ë¦¬ì‚¬ì´ì¦ˆëœ ì˜ì—­(ì–¼êµ´)ì— ëŒ€í•´ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                  * [STEP5] PyTorch ë°ì´í„° í˜•íƒœë¥¼ Tensor ë°ì´í„° í˜•íƒœë¡œ ì»¨ë²„íŒ…í•©ë‹ˆë‹¤.
                  *
                  * @param {tf.Tensor3D} tensorImage : í…ì„œ ì¹´ë©”ë¼ì—ì„œ ì „ë‹¬ ë°›ì€ ì´ë¯¸ì§€
                  * @param {AnnotatedPrediction[]} estimateArr
                  * @param {NodeJS.Timeout} timer
                  */
            hsemotionEstimate: async (
                tensorImage: Tensor3D,
                estimateArr: AnnotatedPrediction[],
            ): Promise<StudyType.ResultHsemotion> => {

                console.log("[+] hsemotionEstimate")
                // [STEP1] ì—°ì‚° ê²°ê³¼ë¥¼ ë°˜í™˜í•  ê°ì²´ë¥¼ ì´ˆê¸° ì„ ì–¸í•©ë‹ˆë‹¤.
                let resultHsemotion: StudyType.ResultHsemotion = {
                    arousalArr: [],
                    valenceArr: [],
                    emotionCode: '',
                };

                const hsemptionModel = modelManager.getHSEmotionSession;

                try {
                    // [CASE1] íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ ë°›ì€ ê°’ì´ ì¡´ì¬í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
                    if (estimateArr.length > 0 && hsemptionModel != null) {
                        // [STEP1] FaceMeshë¥¼ í†µí•´ ì „ë‹¬ë°›ì€ ì¸¡ì • ë°ì´í„° í†µí•´ ì–¼êµ´ ì¸¡ì •ê°’(boundingBox), ì§‘ì¤‘ë„(faceInViewConfidence)ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
                        const { boundingBox: _boundingBox, faceInViewConfidence: _faceInViewConfidence } = estimateArr[0];

                        const { topLeft: _topLeft, bottomRight: _bottomRight } = _boundingBox;

                        const _xw1: number = Math.trunc(Math.max(parseInt(_topLeft[0]), 0));
                        const _yw1: number = Math.trunc(Math.max(parseInt(_topLeft[1]), 0));
                        const _xw2: number = Math.trunc(Math.min(parseInt(_bottomRight[0]), tensorImage.shape[1]));
                        const _yw2: number = Math.trunc(Math.min(parseInt(_bottomRight[1]), tensorImage.shape[0]));

                        //@ts-ignore
                        const _configTensor: TensorContainer = tf.tidy(() => {
                            // [STEP2] í•„ìš”í•œ ì˜ì—­(posbox)ì— ëŒ€í•´ì„œë§Œ ë‚¨ê¸°ê³  ìë¦…ë‹ˆë‹¤.
                            const _indexingResult: Tensor<Rank.R3> = tf.slice(
                                tensorImage,
                                [_yw1, _xw1, 0],
                                [_yw2 - _yw1, _xw2 - _xw1, 3],
                            );
                            // [STEP3] ìë¥¸ ì˜ì—­ì„ ì¼ì •í•œ í¬ê¸°(224,224)ë¡œ ë¦¬ì‚¬ì´ì¦ˆ í•©ë‹ˆë‹¤.
                            const _resizeFace: Tensor3D | Tensor4D = tf.image.resizeBilinear(_indexingResult, [224, 224]);

                            // [STEP4] ë¦¬ì‚¬ì´ì¦ˆëœ ì˜ì—­(ì–¼êµ´)ì— ëŒ€í•´ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                            const _resizeFaceDiv = tf.div(_resizeFace, 255);
                            const mean = [0.485, 0.456, 0.406];
                            const std = [0.229, 0.224, 0.225];
                            const [r, g, b] = tf.split(_resizeFaceDiv, 3, 2);
                            const rNormalized = tf.div(tf.sub(r, mean[0]), std[0]);
                            const gNormalized = tf.div(tf.sub(g, mean[1]), std[1]);
                            const bNormalized = tf.div(tf.sub(b, mean[2]), std[2]);
                            const normalizedTensor = tf.concat([rNormalized, gNormalized, bNormalized], 2);

                            // [STEP5] PyTorch ë°ì´í„° í˜•íƒœë¥¼ Tensor ë°ì´í„° í˜•íƒœë¡œ ì»¨ë²„íŒ…í•©ë‹ˆë‹¤.
                            const transposedTensor = tf.transpose(normalizedTensor, [2, 0, 1]);

                            // [STEP6] ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
                            const expanded = tf.expandDims(transposedTensor, 0); // ì´ê±´ ë°–ì—ì„œ ì°¸ì¡°í•˜ë¯€ë¡œ í•´ì œ ì•ˆë¨
                            // ëª¨ë“  ì¤‘ê°„ í…ì„œ í•´ì œ
                            tf.dispose([
                                _indexingResult,
                                _resizeFace,
                                _resizeFaceDiv,
                                r,
                                g,
                                b,
                                rNormalized,
                                gNormalized,
                                bNormalized,
                                normalizedTensor,
                                transposedTensor,
                            ]);
                            return expanded;
                        });

                        const tensorData = new Float32Array(_configTensor.dataSync());
                        tf.dispose(_configTensor); // ëª…ì‹œì ìœ¼ë¡œ í•´ì œ

                        // [STEP8] ì»¨ë²„íŒ…í•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                        const feed: { input: OnnxTensor | null } = {
                            input: new OnnxTensor(tensorData.slice(), [1, 3, 224, 224]),
                        };

                        // @ts-ignore
                        await hsemptionModel.run(feed, hsemptionModel.outputNames)
                            .then((fetches: InferenceSession.OnnxValueMapType | null) => {
                                resultHsemotion = tf.tidy(() => CalcStudyModule.calcHsemotionInfo(fetches!));
                                // resultHsemotion = { "arousalArr": [0.08552277088165283], "emotionCode": "SUP", "valenceArr": [-0.027108697220683098] }

                                // âœ… fetches ë‚´ë¶€ í•´ì œ
                                for (const key in fetches) {
                                    (fetches as any)[key] = null;
                                }
                                // input/out ONNX Tensor ë¹„ìš°ê¸°
                                feed.input = null; // ì°¸ì¡° ì œê±° (input ê°’ ì œê±°)
                                fetches = null;
                            })
                            .catch((err) => {
                                console.log(`[-] calcHsemotion Error ${err}`);
                            });

                        // tf.dispose(_boundingBox);
                        // tf.dispose(_topLeft)
                        // tf.dispose(_bottomRight);
                    }
                } catch (error) {
                    console.log(`[-] hsemotionEstimate error :: ${error}`);
                }

                return resultHsemotion;
            },
            /**
             * Hpose ì¸¡ì •
             * @param inputData
             * @returns
             */
            hPoseEstimate: async (data1: Float32Array): Promise<number> => {
                console.log("[+] hPoseEstimate")
                const hposeModel = modelManager.getHposeSession;
                let resultScore = 0;

                let inputTensor1: OnnxTensor | null = null;
                let inputTensor2: OnnxTensor | null = null;
                let tfHelperTensor: Tensor | null = null;

                try {
                    if (hposeModel) {
                        // [STEP1] tf.onesë¥¼ í™œìš©í•˜ì—¬ Int32Array ìƒì„±
                        tfHelperTensor = tf.tidy(() => tf.ones([1, 1]));
                        const data2 = new Int32Array(tfHelperTensor.dataSync());

                        // [STEP2] ONNX Tensor ìƒì„±
                        inputTensor1 = new OnnxTensor(data1.slice(), [1, 10, 8]);
                        inputTensor2 = new OnnxTensor(data2.slice(), [1, 1]);

                        const feed: { args_0: OnnxTensor | null; args_1: OnnxTensor | null } = {
                            args_0: inputTensor1,
                            args_1: inputTensor2,
                        };

                        // [STEP3] ëª¨ë¸ ì‹¤í–‰
                        // @ts-ignore
                        await hposeModel.run(feed, hposeModel.outputNames)
                            .then((fetches: InferenceSession.OnnxValueMapType | null) => {
                                if (fetches) {
                                    const result = tf.tidy(() => {
                                        const output = fetches!.output_1;
                                        const [, data2] = output.data;
                                        return Math.floor((data2 as number) * 100);
                                    });
                                    resultScore = result;

                                    // [STEP4] ONNX output ë¹„ìš°ê¸°
                                    for (const key in fetches) {
                                        (fetches as any)[key] = null;
                                    }
                                    // âœ… outputë„ ì§ì ‘ null ì²˜ë¦¬ (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ê¶Œì¥)
                                    (fetches as any).output_1 = null;
                                    fetches = null;
                                }

                                feed.args_0 = null;
                                feed.args_1 = null;
                            })
                            .catch((err) => {
                                console.error(`_hposeModel.run() í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ ë°œìƒ: ${err}`);
                            });
                    }
                } catch (error) {
                    console.log(`[-] hPoseEstimate error :: ${error}`);
                } finally {
                    // [STEP5] TensorFlow Tensor ì œê±°
                    tfHelperTensor?.dispose();
                    tfHelperTensor = null;

                    // [STEP6] ONNX Tensor ì°¸ì¡° ì œê±°
                    inputTensor1 = null;
                    inputTensor2 = null;
                }

                return resultScore;
            },

        }
    })();

    const calcHandler = (() => {
        return {

            /**
             * ë£¨í”„ë¥¼ ìˆ˜í–‰í•˜ë©´ì„œ í•©ê³„ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
             *
             * @param {date} strtTs                                     : ì‹œì‘ì‹œê°„
             * @param {number} accLoopCnt                               : ë£¨í”„ì˜ ìˆ˜í–‰ íšŸìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
             * @param {number} loopTime                                 : ë£¨í”„ì˜ ìˆ˜í–‰ ì‹œê°„ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
             * @param {number} isFaceDectionCnt                         : ì–¼êµ´ì´ ì¸¡ì •ëœ íšŸìˆ˜
             * @param {StudyType.ResultHsemotion} resultHsEmotion       : HSEmotion ì²˜ë¦¬ ê²°ê³¼
             * @param {tf.Tensor3D} tensorImage                         : TensorCameraë¡œ ë¶€í„° ì²˜ë¦¬ëœ ë°ì´í„°
             * @param {number[]} configArr                              : FSA-NET, Gazeì—ì„œ ì²˜ë¦¬ëœ Tensor ë°ì´í„°
             */
            calcLoopSum: async (
                strtTs: number,
                accLoopCnt: number,
                loopTime: number,
                isFaceDectionCnt: number,
                resultHsEmotion: StudyType.ResultHsemotion,
                tensorImage: Tensor3D,
                configArr: number[],
            ) => {
                /**
                 * [CASE1-1] ìµœì¢… ì¹´ìš´íŠ¸ê°€ 10ë³´ë‹¤ ì‘ì€ ê²½ìš°
                 */
                if (accLoopCnt <= LOOP_LIMIT_CNT) {
                    // console.log(" =================================== íŒŒë¼ë¯¸í„°ë¡œ ë“¤ì–´ì˜¨ ê°’ ===========================================================");
                    // // console.log("tensorResult :: ", tensorResult)
                    console.log("doSq :: ", doSq)
                    console.log("strtTs :: ", new Date(Number(strtTs)))
                    console.log("accLoopCnt :: ", accLoopCnt)
                    console.log("per Loof Time :: ", loopTime)
                    console.log("isFaceDetctionCnt :: ", isFaceDectionCnt)
                    console.log("resultHsEmotion :: ", resultHsEmotion)
                    console.log(" ===========================================================================================================");

                    // [STEP2] ì—°ì‚°ëœ Hsemotion ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                    const {
                        arousalArr: _resultArousal,
                        emotionCode: _resultEmotionCode,
                        valenceArr: _resultValence,
                    } = resultHsEmotion;

                    const { calcArrItemDigit, calcEmtnCd } = calcHandler;
                    /**
                     * [STEP3] ê°ê°ì˜ ì—°ì‚°ë°©ë²•ì— ë”°ë¼ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                     */
                    let _arousal = 0;
                    let _valence = 0;

                    // [STEP4] arousal, valenceëŠ” ì†Œìˆ˜ì  5ìë¦¬ê¹Œì§€ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
                    _arousal = calcArrItemDigit(_resultArousal, 5);
                    _valence = calcArrItemDigit(_resultValence, 5);

                    const _emtnCd = _arousal === 0 && _valence === 0 ? '' : calcEmtnCd(_arousal, _valence); // ì •ì„œì½”ë“œ ì—°ì‚° ì²˜ë¦¬

                    // [STEP4] [ìŠ¤íŠ¸ë ˆìŠ¤] ì—°ì‚°ì„ í†µí•´ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ ê°’ì„ ëˆ„ì í•©ë‹ˆë‹¤.
                    const _stress = _valence < 0 && _arousal > 0 ? 1 : 0;

                    /**
                     * [STEP5] ì—°ì‚°ëœ ê°’ì„ Stateë‚´ì— ëˆ„ì ì„ ì‹œì¼œ ë°°ì—´ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
                     */
                    const {
                        valenceArr,
                        arousalArr,
                        exprCdArr,
                        emtnCdArr,
                        atntnArr,
                        isFaceDtctArr,
                        strssArr,
                        tensorResultArr,
                        msrdTmArr,
                    } = accStudyDoDtlInfo;
                    valenceArr.push(_valence); // valence ê°’ ëˆ„ì 
                    arousalArr.push(_arousal); // arousal ê°’ ëˆ„ì 
                    exprCdArr.push(_resultEmotionCode); // í‘œì •ì½”ë“œ ê°’ ëˆ„ì 
                    emtnCdArr.push(_emtnCd); // ì •ì„œì½”ë“œ ê°’ ëˆ„ì 
                    atntnArr.push(0); // ì§‘ì¤‘ë ¥ ì ìˆ˜ ê°’ ëˆ„ì (* 10ë²ˆ ë£¨í”„ì—ì„œ ìµœì¢… ê³„ì‚°í•˜ì—¬ ì¶œë ¥ì´ ë©ë‹ˆë‹¤.)
                    strssArr.push(_stress); // ìŠ¤íŠ¸ë ˆìŠ¤ ê°’ ëˆ„ì 
                    msrdTmArr.push(loopTime); // ë£¨í”„ ì¸¡ì • ì‹œê°„ì„ ëˆ„ì 
                    isFaceDtctArr.push(isFaceDectionCnt); // ì–¼êµ´ íƒì§€ì—¬ë¶€ ê°’ ëˆ„ì 
                    // @ts-ignore
                    tensorResultArr.push(configArr); // FSA-NET, Gazeì—ì„œ ì¸¡ì •ë˜ëŠ” Tensor ê°’ ëˆ„ì 

                    // [STEP6] ìµœì¢… ëˆ„ì ëœ ë°ì´í„°ë¥¼ State ë‚´ì— ê°±ì‹ í•©ë‹ˆë‹¤.
                    setAccStudyDoDtlInfo({
                        doSq: doSq, // [ì‹¤í–‰ì‹œí€€ìŠ¤]     ìµœì´ˆ í•œë²ˆë§Œ ìˆ˜í–‰
                        msrdTmArr: msrdTmArr, // [ì¸¡ì •ëœ ì‹œê°„]    60íšŒ ë™ì•ˆì˜ ì‹œê°„ì˜ í•©ê³„ë¥¼ ë”í•©ë‹ˆë‹¤.
                        isFaceDtctArr: isFaceDtctArr, // [ì–¼êµ´íƒì§€ì—¬ë¶€]    60íšŒ ë™ì•ˆ í•œë²ˆì´ë¼ë„ ìºì¹˜ê°€ ë˜ë©´ í•´ë‹¹ ê°’ì€ 1ë¡œ ê³ ì •í•©ë‹ˆë‹¤.
                        exprCdArr: exprCdArr, // [í‘œì •ì½”ë“œ]
                        valenceArr: valenceArr, // [valence]
                        arousalArr: arousalArr, // [arousal]
                        emtnCdArr: emtnCdArr, // [ì •ì„œì½”ë“œ]
                        atntnArr: atntnArr, // [ì§‘ì¤‘ë ¥]        60íšŒê°€ ìˆ˜í–‰ë˜ëŠ” ë™ì•ˆ ì§‘ì¤‘ë ¥ ì ìˆ˜ì˜ í•©ê³„
                        strssArr: strssArr,
                        tensorResultArr: tensorResultArr, // FSANET Array
                    });

                    console.log(`2. ë£¨í”„ ì¢…ë£Œ ì‹œì ì— ë©”ëª¨ë¦¬ì— ìˆëŠ” í…ì„œ ìˆ˜  ${tf.tidy(() => tf.memory().numTensors)}`);
                } else {
                    /**
                     * [CASE1-2] ìµœì¢… ì¹´ìš´íŠ¸ ê°’ì´ 10ì¸ ê²½ìš° : í…Œì´ë¸” INSERT ìˆ˜í–‰
                     */
                    // [STEP2] Stateë‚´ì— ëˆ„ì ëœ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                    const {
                        valenceArr,
                        arousalArr,
                        msrdTmArr,
                        exprCdArr,
                        atntnArr,
                        emtnCdArr,
                        isFaceDtctArr,
                        strssArr,
                        tensorResultArr,
                    } = accStudyDoDtlInfo;

                    const msrdSecs = Math.floor((Date.now() - strtTs) / 1000);
                    // console.log("ì–¼êµ´ì´ ì—†ì„ë•ŒëŠ”??? : ", msrdSecs)
                    // console.log("ë£¨í”„ì˜ ì‹œì‘ ì‹œê°„ :: ", calcHandler.convertDateNowToHMS(strtTs))
                    // console.log("ë£¨í”„ì˜ ì¢…ë£Œ ì‹œê°„ :: ", calcHandler.convertDateNowToHMS(Date.now()))
                    // console.log("ë£¨í”„ì˜ ì¢…ë£Œ - ì‹œì‘ ìˆ˜í–‰ëœ ì‹œê°„ :: ", msrdSecs);

                    // console.log("*************************************** ìµœì¢… ëˆ„ì ëœ ê°’ *************************************************************")
                    // console.log("doSq :", doSq);
                    // console.log("msrdTm :", msrdSecs);
                    // console.log("isFaceDtctArr :", isFaceDtctArr);
                    // console.log("exprCdArr :", exprCdArr);
                    // console.log("valenceArr :", valenceArr);
                    // console.log("arousalArr :", arousalArr);
                    // console.log("emtnCdArr :", emtnCdArr);
                    // console.log("atntnArr :", atntnArr);
                    // console.log("strssArr :", strssArr);
                    // console.log("****************************************************************************************************************")

                    /**
                     * [STEP3] State ë‚´ì— ê°€ì ¸ì˜¨ ê°’ì„ ê°ê°ì— ë§ëŠ” í‰ê· ì¹˜ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
                     */
                    // í‰ê·  ì ìˆ˜ ê³„ì‚°í•¨ìˆ˜ ìˆ˜í–‰
                    const _stress = CalcStudyModule.calcAverageStress(strssArr, LOOP_LIMIT_CNT); // ìŠ¤íŠ¸ë ˆìŠ¤ í‰ê· 
                    const _valence = CalcStudyModule.calcAverageFloat(valenceArr, LOOP_LIMIT_CNT); // valence í‰ê· 
                    const _arousal = CalcStudyModule.calcAverageFloat(arousalArr, LOOP_LIMIT_CNT); // arousal í‰ê· 
                    const _isFaceDtct = CalcStudyModule.calcAverageIsFaceDtct(isFaceDtctArr); // faceDtct í‰ê· 

                    // [STEP4] Best Code ì •ë³´ ì¶œë ¥ í•¨ìˆ˜ ìˆ˜í–‰
                    const _exprCd = CalcStudyModule.calcBestCode(CODE_GRP_CD.ExpressionCode, exprCdArr); // ì œì¼ ìµœê³ ì˜ í‘œí˜„ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                    const _emtnCd = CalcStudyModule.calcBestCode(CODE_GRP_CD.EmotionalCode, emtnCdArr); // ì œì¼ ìµœê³ ì˜ ê°ì •ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

                    const { hPoseEstimate } = modelCalcHandler;



                    // [STEP5] ì§‘ì¤‘ë ¥ì„ ì¶”ì •í•˜ì—¬ í…ì„œê°’ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
                    const resultConcent = calcHandler.concentrationEstimate(tensorResultArr.slice(-10));

                    /**
                     * [STEP6] ì§‘ì¤‘ë ¥ì„ ì¶”ì •í•©ë‹ˆë‹¤ : ì–¼êµ´ì„ í•˜ë‚˜ë„ ì¸ì‹í•˜ì§€ ëª»í•œê²½ìš°ì— hPoseë¥¼ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ
                     */
                    let _atntn = 0;
                    if (isFaceDtctArr.includes(1)) {
                        const data1 = new Float32Array(resultConcent.dataSync());
                        _atntn = await hPoseEstimate(data1);
                        /**
                         * [STEP7] ì§‘ì¤‘ë ¥ ì ìˆ˜ì— ëŒ€í•´ ìŠ¤ë¬´ë”©ì„ ìœ„í•´ ë¡œì§ ì¶”ê°€
                         */
                        if (accAtntn.length === 3) {
                            accAtntn.shift();
                            accAtntn.push(_atntn);
                        } else {
                            // ë°°ì—´ë¡œ êµ¬ì„±
                            accAtntn.push(_atntn);
                        }
                    }

                    // [STEP8] ì‚¬ìš©í•œ Tensor ë©”ëª¨ë¦¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    tf.dispose(resultConcent);
                    tf.dispose(tensorImage);

                    // [STEP9] ìµœì¢… ì¸¡ì •í•œ ì´ˆ

                    const result: StudyType.StudyDoDtlSQliteDto = {
                        doSq: doSq, // [ìˆ˜í–‰ ì‹œí€€ìŠ¤] ë³„ë„ì˜ ì—°ì‚° ì²˜ë¦¬ê°€ ì—†ìŒ
                        msrdCnt: accLoopCnt - 1, // [ì¸¡ì • íšŸìˆ˜] ë³„ë„ì˜ ì—°ì‚° ì²˜ë¦¬ê°€ ì—†ìŒ (*61íšŒ ì¸ê²½ìš° ìˆ˜í–‰ë˜ê¸°ì— ê°’ì„ 1ë¹¼ì¤ë‹ˆë‹¤.)
                        faceDtctYn: _isFaceDtct, // [ì–¼êµ´íƒì§€ì—¬ë¶€] ë³„ë„ì˜ ì—°ì‚°ì²˜ë¦¬ê°€ ì—†ìŒ
                        strss: _stress, // [ìŠ¤íŠ¸ë ˆìŠ¤] ë³„ë„ì˜ ê³„ì‚°ë²•ì„ í†µí•´ì„œ í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤.
                        valence: _valence, // [valence] í•©ê³„ì˜ í‰ê· ê°’ì„ ë„£ìŠµë‹ˆë‹¤.
                        arousal: _arousal, // [arousal] í•©ê³„ì˜ í‰ê· ê°’ì„ ë„£ìŠµë‹ˆë‹¤.
                        atntn: _atntn, // [ì§‘ì¤‘ë ¥] í•©ê³„ì˜ í‰ê· ê°’ì„ ë„£ìŠµë‹ˆë‹¤.
                        exprCd: _exprCd, // [í‘œí˜„ì½”ë“œ] ê³„ì‚°ëœ ìµœì¢… ê°’ì„ ë„£ìŠµë‹ˆë‹¤.
                        emtnCd: _emtnCd, // [ì •ì„œì½”ë“œ] ê³„ì‚°ëœ ìµœì¢… ê°’ì„ ë„£ìŠµë‹ˆë‹¤.
                        strtTs: new Date(Number(strtTs)).toString(), // [ì‹œì‘ íƒ€ì„ìŠ¤íƒ¬í”„] ë£¨í”„ ì‹œì‘ì‹œê°„
                        endTs: new Date(Number(Date.now())).toString(), // [ì¢…ë£Œ íƒ€ì„ìŠ¤íƒ¬í”„] ë£¨í”„ ì¢…ë£Œì‹œê°„
                        msrdSecs: msrdSecs, // [ë£¨í”„ìˆ˜í–‰ì‹œê°„] ì‹œì‘ ì‹œê°„ì—ì„œ ì¢…ë£Œ ì‹œì ì˜ 'ì´ˆ'ë¥¼ ë„£ìŠµë‹ˆë‹¤.
                        regTs: new Date(Number(Date.now())).toString(), // [ë“±ë¡ íƒ€ì„ìŠ¤íƒ¬í”„] INSERT ì‹œì  ì‹œê°„
                    };

                    console.log("ìµœì¢… ê²°ê³¼ :: ", result);

                    // [STEP10] [SQlite] êµ¬ì„±í•œ ë°ì´í„°ë¥¼ ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤(SQLite)ë‚´ì— ì €ì¥í•©ë‹ˆë‹¤.
                    // await TbStdyDoDtlModules.insertRowData(result);

                    /**
                     * [STEP11] Stateì˜ ëˆ„ì ëœ í•™ìŠµ ìƒì„¸ ì •ë³´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                     */
                    resetHandler.cleanUpAccStdInfo();
                    // console.log(`3. ë£¨í”„ ì¢…ë£Œ ì‹œì ì— ë©”ëª¨ë¦¬ì— ìˆëŠ” í…ì„œ ìˆ˜  ${tf.tidy(() => tf.memory().numTensors)}`)
                    // console.log("â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ ìµœì¢… ì—°ì‚° ìˆ˜í–‰ì‹œê°„ : ", msrdSecs, "ì´ˆ â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸")
                }
            },

            /**
             * arousal, valence ê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
             * @param paramArr
             * @param digit
             */
            calcArrItemDigit: (paramArr: number[] | Float32Array, digit: number): number => {
                return paramArr.length > 0 ? parseFloat(paramArr[0].toFixed(digit)) : 0;
            },


            /**
             * ê³„ì‚°ì„ í†µí•´ ì •ì„œì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
             * @param arousal
             * @param valence
             * @returns {string} ì •ì„œì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
             */
            calcEmtnCd: (arousal: number, valence: number): string => {
                let _result = '';

                // [CASE1] ì •ì„œì½”ë“œ : Enjoyment(ì¦ê±°ì›€)
                if (valence >= 0 && arousal >= 0) _result = 'ENJ';

                // [CASE2] ì •ì„œì½”ë“œ : Anger(í™”)
                if (valence < 0 && arousal >= 0) _result = 'AGR';

                // [CASE3] ì •ì„œì½”ë“œ : Boredum(ì§€ë£¨í•¨)
                if (valence < 0 && arousal < 0) _result = 'BDM';

                // [CASE4] ì •ì„œì½”ë“œ : Relax(ì´ì™„)
                if (valence >= 0 && arousal < 0) _result = 'RLX';
                return _result;
            },

            /**
             * í•©ê³„ì˜ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì§‘ì¤‘ë ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤
             * @param tensorResultArr
             */
            concentrationEstimate: (tensorResultArr: number[]) => {
                const result = tf.tidy(() => {
                    const tensorResult = tf.tensor(tensorResultArr);
                    // const tensorStack = tf.stack(tensorResult);
                    return tf.expandDims(tensorResult, 0);
                });
                return result;
            },

            /**
             * NMS(Non-Maximum Suppression) êµ¬í˜„
             * @param boxScores ë°•ìŠ¤ ì¢Œí‘œ ë° ì ìˆ˜ ë°°ì—´ [[x_min, y_min, x_max, y_max, score], ...]
             * @param iouThreshold IoU ì„ê³„ê°’
             * @param topK ìƒìœ„ kê°œ ê²°ê³¼ë§Œ ìœ ì§€ (-1ì´ë©´ ëª¨ë“  ê²°ê³¼ ìœ ì§€)
             * @param candidateSize ê³ ë ¤í•  í›„ë³´ ê°œìˆ˜
             * @returns ìœ ì§€ëœ ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸
             */
            hardNMS: (boxScores: number[][], iouThreshold: number, topK: number = -1, candidateSize: number = 200): number[][] => {
                // ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
                let sortedBoxes = [...boxScores].sort((a, b) => b[4] - a[4]);

                // ìƒìœ„ í›„ë³´ë§Œ ê³ ë ¤
                let candidates = sortedBoxes.slice(0, candidateSize);
                const picked: number[][] = [];

                while (candidates.length > 0) {
                    const current = candidates.shift()!;
                    picked.push(current);
                    if (topK > 0 && picked.length >= topK) break;

                    const remainingBoxes = candidates.filter(box => {
                        const iou = calcHandler.iouOf([current], [box.slice(0, 4)]);
                        return iou[0] <= iouThreshold;
                    });

                    // candidates ê°±ì‹ 
                    candidates.splice(0, candidates.length, ...remainingBoxes);

                    // âœ¨ remainingBoxes ì •ë¦¬
                    for (let i = 0; i < remainingBoxes.length; i++) {
                        remainingBoxes[i] = [];
                    }
                }
                // âœ¨ ëª…ì‹œì ìœ¼ë¡œ ë©”ëª¨ë¦¬ í•´ì œ
                for (let i = 0; i < sortedBoxes.length; i++) {
                    sortedBoxes[i] = [];
                }
                for (let i = 0; i < candidates.length; i++) {
                    candidates[i] = [];
                }
                sortedBoxes = [];
                candidates = [];

                return picked;
            },

            /**
            * IoU(Intersection over Union) ê³„ì‚° í•¨ìˆ˜
            * @param boxes0 ê¸°ì¤€ ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸ [[x_min, y_min, x_max, y_max], ...]
            * @param boxes1 ë¹„êµ ëŒ€ìƒ ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸ [[x_min, y_min, x_max, y_max], ...]
            * @param eps 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì‘ì€ ê°’
            * @returns IoU ê°’ ë¦¬ìŠ¤íŠ¸
            */
            iouOf: (boxes0: number[][], boxes1: number[][], eps: number = 1e-5): number[] => {
                const baseBox = boxes0[0];
                const baseX1 = baseBox[0], baseY1 = baseBox[1], baseX2 = baseBox[2], baseY2 = baseBox[3];
                const area0 = (baseX2 - baseX1) * (baseY2 - baseY1);

                const results: number[] = [];

                for (let i = 0; i < boxes1.length; i++) {
                    const b = boxes1[i];
                    const x1 = Math.max(baseX1, b[0]);
                    const y1 = Math.max(baseY1, b[1]);
                    const x2 = Math.min(baseX2, b[2]);
                    const y2 = Math.min(baseY2, b[3]);

                    const overlapW = Math.max(x2 - x1, 0);
                    const overlapH = Math.max(y2 - y1, 0);
                    const overlapArea = overlapW * overlapH;

                    const area1 = (b[2] - b[0]) * (b[3] - b[1]);

                    const iou = overlapArea / (area0 + area1 - overlapArea + eps);
                    results.push(iou);
                }

                return results;
            },
            extractEyeRegions: (
                landmarks: number[][], // [x, y] í˜•ì‹ì˜ 68ê°œì˜ ëœë“œë§ˆí¬ ì¢Œí‘œ
                origImage: Tensor3D, // ì›ë³¸ ì´ë¯¸ì§€ (TensorFlow í…ì„œ)
                padding: number = 50, // ëˆˆ ì˜ì—­ì— ì¶”ê°€í•  íŒ¨ë”© ê°’
                targetSize: [number, number] = [64, 64] // ëˆˆ ì˜ì—­ì„ ê³ ì • í¬ê¸°ë¡œ ì¡°ì •
            ): { leftEye: Float32Array; rightEye: Float32Array } => {

                // 1. ì™¼ìª½ ë° ì˜¤ë¥¸ìª½ ëˆˆ ëœë“œë§ˆí¬ ì¢Œí‘œ
                let leftEyeLandmarks = landmarks.slice(36, 42); // ì™¼ìª½ ëˆˆ (36~41)
                let rightEyeLandmarks = landmarks.slice(42, 48); // ì˜¤ë¥¸ìª½ ëˆˆ (42~47)

                // 2. ì™¼ìª½ ëˆˆ ì˜ì—­ ê³„ì‚°
                const [leftEyeX1, leftEyeY1] = leftEyeLandmarks.reduce(
                    ([minX, minY], [x, y]) => [Math.min(minX, x - padding), Math.min(minY, y - padding)],
                    [Infinity, Infinity]
                );
                const [leftEyeX2, leftEyeY2] = leftEyeLandmarks.reduce(
                    ([maxX, maxY], [x, y]) => [Math.max(maxX, x + padding), Math.max(maxY, y + padding)],
                    [-Infinity, -Infinity]
                );

                // 3. ì˜¤ë¥¸ìª½ ëˆˆ ì˜ì—­ ê³„ì‚°
                const [rightEyeX1, rightEyeY1] = rightEyeLandmarks.reduce(
                    ([minX, minY], [x, y]) => [Math.min(minX, x - padding), Math.min(minY, y - padding)],
                    [Infinity, Infinity]
                );
                const [rightEyeX2, rightEyeY2] = rightEyeLandmarks.reduce(
                    ([maxX, maxY], [x, y]) => [Math.max(maxX, x + padding), Math.max(maxY, y + padding)],
                    [-Infinity, -Infinity]
                );

                leftEyeLandmarks = [];
                rightEyeLandmarks = [];

                // 4. ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ ì™¼ìª½ ëˆˆ ì˜ì—­ ì¶”ì¶œ
                const region1 = tf.slice(origImage,
                    [
                        Math.floor(Math.max(leftEyeY1, 0)), // ì‹œì‘ y ì¢Œí‘œ
                        Math.floor(Math.max(leftEyeX1, 0)), // ì‹œì‘ x ì¢Œí‘œ
                        0, // ì±„ë„
                    ],
                    [
                        Math.floor(Math.min(leftEyeY2 - leftEyeY1, origImage.shape[0] - leftEyeY1)), // ë†’ì´
                        Math.floor(Math.min(leftEyeX2 - leftEyeX1, origImage.shape[1] - leftEyeX1)), // ë„ˆë¹„
                        3, // ì±„ë„ (RGB)
                    ]
                );
                const leftEyeRegion = tf.image.resizeBilinear(region1, targetSize);

                // 2. ì™¼ìª½ ëˆˆ ë°ì´í„° ì •ê·œí™” ë° í˜•íƒœ ë³€í™˜
                const leftEyeTensorData = tf.tidy(() => {
                    const _scalar = tf.scalar(255);
                    const _normalized = tf.div(leftEyeRegion, tf.scalar(255)); // 0~255 -> 0~1ë¡œ ì •ê·œí™”
                    const _reshapeInput = tf.reshape(_normalized, [1, 64, 64, 3]); // [H, W, C] -> [1, H, W, C]
                    const _resultData = _reshapeInput.dataSync();
                    tf.dispose([leftEyeRegion, _scalar, _normalized, _reshapeInput])
                    return new Float32Array(_resultData);
                });

                tf.dispose([region1, leftEyeRegion]);

                // 5. ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ ì˜¤ë¥¸ìª½ ëˆˆ ì˜ì—­ ì¶”ì¶œ
                const region2 = tf.slice(origImage,
                    [
                        Math.floor(Math.max(rightEyeY1, 0)), // ì‹œì‘ y ì¢Œí‘œ
                        Math.floor(Math.max(rightEyeX1, 0)), // ì‹œì‘ x ì¢Œí‘œ
                        0, // ì±„ë„
                    ],
                    [
                        Math.floor(Math.min(rightEyeY2 - rightEyeY1, origImage.shape[0] - rightEyeY1)), // ë†’ì´
                        Math.floor(Math.min(rightEyeX2 - rightEyeX1, origImage.shape[1] - rightEyeX1)), // ë„ˆë¹„
                        3, // ì±„ë„ (RGB)
                    ]
                )
                const rightEyeRegion = tf.image.resizeBilinear(region2, targetSize);


                // 3. ì˜¤ë¥¸ìª½ ëˆˆ ë°ì´í„° ì •ê·œí™” ë° í˜•íƒœ ë³€í™˜
                const rightEyeTensorData = tf.tidy(() => {
                    const _scalar = tf.scalar(255);
                    const _normalized = tf.div(rightEyeRegion, _scalar); // 0~255 -> 0~1ë¡œ ì •ê·œí™”
                    const _reshapeInput = tf.reshape(_normalized, [1, 64, 64, 3]); // [H, W, C] -> [1, H, W, C]
                    const _resultData = _reshapeInput.dataSync();
                    tf.dispose([_scalar, rightEyeRegion, _normalized, _reshapeInput])
                    return new Float32Array(_resultData);
                });
                tf.dispose([region2, rightEyeRegion]);

                // 6. ê²°ê³¼ ë°˜í™˜
                return { leftEye: leftEyeTensorData, rightEye: rightEyeTensorData };
            },
        }

    })();

    /**
     * ì¼ë°˜ì ì¸ í•¸ë“¤ëŸ¬
     */
    const commonHandler = (() => {
        return {

            /**
             * ì§€ì •í•œ ì‹œê°„ë§Œí¼ ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤.
             * @param ms
             * @returns
             */
            sleep: (ms: number): Promise<void> => {
                console.log(` ===== > í•´ë‹¹ ${ms}ì´ˆ ë§Œí¼ ì ì‹œ ì‰½ë‹ˆë‹¤.. <===========`);
                return new Promise((resolve) => setTimeout(resolve, ms));
            },

            /**
             * ì•± ìƒí…Œ ë³€í™”ë¥¼ ê°ì§€í•˜ëŠ” ë¦¬ìŠ¤ë„ˆ 
             * @param {AppStateStatus} nextAppState Appì—ì„œ ë³€ê²½ëœ ìƒíƒœ ê°’ì´ ì „ë‹¬ë°›ìŒ (active, inactive, background)
             * @returns {void}
             */
            appStateChangeListener: (nextAppState: AppStateStatus): void => {
                // fetch()
                //     .then(state => {
                //         const { type, isConnected } = state;
                //         console.log("Connection type", type);
                //         console.log("Is connected?", isConnected);
                //         console.log("ì•± ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤ >>> [", nextAppState, "] ì•±ì˜ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ [", isConnected, "]");

                //         switch (nextAppState) {
                //             // [CASE1-1] ì•± ìƒíƒœê°€ "background", "inactive" ìƒíƒœì¸ ê²½ìš°: stopwatchë¥¼ ë©ˆì¶¥ë‹ˆë‹¤.
                //             case "background":
                //                 if (isActiveStopwatch) stopwatchHandler.pause();                // ìŠ¤íƒ‘ì›Œì¹˜ê°€ ì‹¤í–‰ì¤‘ì¸ ê²½ìš°ë§Œ ì´ë¥¼ ë©ˆì¶¥ë‹ˆë‹¤.
                //                 break;

                //             case "inactive":
                //                 if (isActiveStopwatch) stopwatchHandler.pause();                // ìŠ¤íƒ‘ì›Œì¹˜ë¥¼ ë©ˆì¶¥ë‹ˆë‹¤.
                //                 if (Platform.OS === "ios") preAppState.current = "inactive";    // iOSì˜ ì‘ì—…ì°½ì„ ë‚´ë¦° ê²½ìš° ì´ë¥¼ ìˆ˜í–‰
                //                 break;

                //             // [CASE1-2] ì•± ìƒíƒœê°€ "active" ìƒíƒœì¸ ê²½ìš°: stopwatchë¥¼ ì¬ê°œ í•©ë‹ˆë‹¤.
                //             case "active":
                //                 switch (Platform.OS) {
                //                     case "android":
                //                         stopwatchHandler.start();       // ë¬´ì¡°ê»€ ì‹¤í–‰ì´ ëœë‹¤.
                //                         break;

                //                     case "ios":
                //                         // ì´ì „ì— inactiveê°€ ì‹¤í–‰ë˜ê³  ìŠ¤íƒ‘ì›Œì¹˜ê°€ ìˆ˜í–‰ëœ ê²½ìš° : ìŠ¤íƒ‘ì›Œì¹˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
                //                         if (preAppState.current === "inactive" && isActiveStopwatch) {
                //                             stopwatchHandler.start();
                //                             preAppState.current = "active";         // ìƒíƒœë¥¼ ë‹¤ì‹œ í™œì„±í™”ë¡œ ë³€ê²½í•œë‹¤.
                //                         }
                //                         break;
                //                     default:
                //                         break;
                //                 }
                //                 break;
                //             default:
                //                 break;
                //         }
                //     });
            },

            /**
             * ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì²´í¬í•˜ëŠ” í•¨ìˆ˜
             */
            checkMemoryUsage: async () => {
                // const totalMemory = await DeviceInfo.getTotalMemory(); // ê¸°ê¸°ì˜ ì „ì²´ ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸ ë‹¨ìœ„)
                // const usedMemory = await DeviceInfo.getUsedMemory();   // ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸ ë‹¨ìœ„)

                // const freeMemory = totalMemory - usedMemory; // ì‚¬ìš©ëœ ë©”ëª¨ë¦¬
                // const usedMemoryPercentage = (usedMemory / totalMemory) * 100; // ì‚¬ìš©ëœ ë©”ëª¨ë¦¬ ë¹„ìœ¨

                // console.log("totalMemory-->", totalMemory / (1024 * 1024));
                // console.log("usedMemory-->", usedMemory / (1024 * 1024));
                // console.log("usedMemoryPercentage-->", usedMemoryPercentage);
            },

            /**
             * ë„¤íŠ¸ì›Œí¬ ë³€í™”ì— ëŒ€í•´ ì²´í¬í•˜ëŠ” ë¦¬ìŠ¤ë„ˆ
             * @returns
             */
            networkChangeCheckListener: (): NetInfoSubscription | any => {
                // console.log("[+] ì—°ê²° ìƒíƒœ í™•ì¸");
                // return NetInfo.addEventListener(state => {
                //     inConnectNetworkRef.current = state.isConnected!;   // ì—°ê²° ìƒíƒœë¥¼ ë³€ìˆ˜ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
                //     // ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠê²¼ì„ë•Œ, í•™ìŠµì„ ì¤‘ë‹¨ì‹œí‚¤ê³  íŒì—…ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
                //     if (!inConnectNetworkRef.current) {
                //         stopwatchHandler.pause();
                //         console.log("ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠê²¼ìŠµë‹ˆë‹¤.");
                //         Alert.alert("ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠê²¼ìŠµë‹ˆë‹¤.", " ë””ë°”ì´ìŠ¤ ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.");
                //     };
                // });
            },

            /**
             * ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠê²¼ì„ë•Œ, ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
             * @returns
             */
            disconnectNetworkAlert: (): void => {
                // console.log("ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠê²¼ìŠµë‹ˆë‹¤.");
                // Alert.alert("ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠê²¼ìŠµë‹ˆë‹¤.", " ë””ë°”ì´ìŠ¤ ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.");
                // return
            },

        }
    })();

    /**
         * ==============================================================================================================================
         * ì´ˆê¸°í™”ë¥¼ ê´€ë¦¬í•˜ëŠ” Handler
         * ==============================================================================================================================
         */
    const resetHandler = (() => {
        return {
            /**
             * Stateì— ëˆ„ì ëœ í•™ìŠµ ìƒì„¸ ì •ë³´ë¥¼ ì´ˆê¸°í™” ì‹œí‚µë‹ˆë‹¤.
             */
            cleanUpAccStdInfo: () => {
                console.log('[+] ëˆ„ì ëœ State ê°’ì„ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.');
                accStudyDoDtlInfo.valenceArr = [];
                accStudyDoDtlInfo.arousalArr = [];
                accStudyDoDtlInfo.emtnCdArr = [];
                accStudyDoDtlInfo.atntnArr = [];
                accStudyDoDtlInfo.strssArr = [];
                accStudyDoDtlInfo.exprCdArr = [];
                accStudyDoDtlInfo.isFaceDtctArr = [];
                accStudyDoDtlInfo.tensorResultArr = [];
                accStudyDoDtlInfo.msrdTmArr = [];
                accStudyDoDtlInfo.msrdCnt = 0;
            },

            /**
             * ì‚¬ìš© ì™„ë£Œí•œ ONNX ëª¨ë¸, ì¹´ë©”ë¼, ë³€ìˆ˜ë“¤ì„ ì´ˆê¸°í™” ì‹œí‚µë‹ˆë‹¤.
             */
            cleanUpStudyInfo: () => {
                const { getFSANetSession, clearFSANetModel, getHposeSession, clearHposeModel, getHSEmotionSession, clearHSEmotionModel } = modelManager

                if (getFSANetSession) clearFSANetModel()
                if (getHposeSession) clearHposeModel()
                if (getHSEmotionSession) clearHSEmotionModel();
                resetHandler.cleanUpAccStdInfo(); // ëˆ„ì ëœ ë°°ì—´ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                setIsFaceDtctYn(false);
            },

            faceMeshCleanupPrediction: (paramArr: AnnotatedPrediction[]) => {
                for (const item of paramArr) {
                    if (item.kind === 'MediaPipePredictionTensors') {
                        item.mesh.dispose();
                        item.scaledMesh.dispose();
                        item.boundingBox.topLeft.dispose();
                        item.boundingBox.bottomRight.dispose();
                        // (item.mesh as any) = null;
                        // (item.scaledMesh as any) = null;
                        // (item.boundingBox.topLeft as any) = null;
                        // (item.boundingBox.bottomRight as any) = null
                    } else if (item.kind === 'MediaPipePredictionValues') {
                        if ('dispose' in item.boundingBox.topLeft && typeof item.boundingBox.topLeft.dispose === 'function') {
                            item.boundingBox.topLeft.dispose();
                        }
                        if ('dispose' in item.boundingBox.bottomRight && typeof item.boundingBox.bottomRight.dispose === 'function') {
                            item.boundingBox.bottomRight.dispose();
                        }
                        // item.mesh = [];
                        // item.scaledMesh = [];
                        // ë¹ˆ ë°°ì—´ ì´ˆê¸°í™”
                        // for (const key in item.annotations) {
                        //     if (Array.isArray(item.annotations[key])) {
                        //         item.annotations[key] = [];
                        //     }
                        // }
                    }
                }

                // console.log('[+] faceMeshCleanupPrediction ìˆ˜í–‰ì™„ë£Œ');
            },
        };
    })();


    /**
    * TensorCameraì— ì¶œë ¥ëœ TensorImage ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ì¶œë ¥ì„ ìœ„í•œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    * @param {tf.Tensor3D} tensorImage 
    * @return {string} ì´ë¯¸ì§€ ê²½ë¡œ ë°˜í™˜
    */
    const cvtTensorImgToBase64 = (tensorImage: Tensor3D): string => {

        // [STEP1] ì „ë‹¬ ë°›ì€ TensorImageë¥¼ ì¢Œìš° ë°˜ì „í•©ë‹ˆë‹¤.
        const flippedTensor = tf.reverse(tensorImage, [1]);

        // [STEP2] TensorImageì—ì„œ ë†’ì´ì™€ ë„ˆë¹„ ê°’ì„ ë°˜í™˜ ë°›ìŠµë‹ˆë‹¤.
        const height: number = flippedTensor.shape[0];
        const width: number = flippedTensor.shape[1];

        // [STEP3] í•˜ë‚˜ì˜ ì°¨ì›ì„ ë§Œë“­ë‹ˆë‹¤.
        const tensor3D = tf.fill([height, width, 1], 255);

        // [STEP4] tensorImageëŠ” 3ì°¨ì›ì´ë©° ì´ì „ì— ë§Œë“  ì°¨ì›ì„ í•©ì³ì„œ 4ì°¨ì›ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
        const data = new Buffer(
            tf.slice(
                tf.concat([flippedTensor, tensor3D], -1), [0], [height, width, 4]).dataSync()
        )

        // [STEP5] êµ¬ì„±í•œ ë²„í¼ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        const rawImageData = { data, width, height };

        // [ê¸°ëŠ¥-1] ì •ì œëœ Tensor ê°’ì„ í†µí•´ jpegë¡œ ì¸ì½”ë”©í•œë‹¤.
        const jpegImageData = jpeg.encode(rawImageData, 100);

        // [ê¸°ëŠ¥-2] jpeg ë°ì´í„°ë¥¼ 'base64'ë¡œ ì „í™˜í•œë‹¤.
        const imgBase64 = tf.util.decodeString(jpegImageData.data, "base64");

        return `data:image/jpeg;base64,${imgBase64}`
    }




    if (device == null || !hasPermission) {
        return <Text>ì¹´ë©”ë¼ ë¡œë”© ì¤‘...</Text>;
    }

    return (
        <View style={styles.container}>
            <Camera
                ref={cameraRef}
                device={device}
                style={StyleSheet.absoluteFill}
                isActive={true}
                enableFpsGraph={true}
                photo={true}
            />

            <TouchableOpacity style={styles.captureButton} onPress={() => setCameraOn(!cameraOn)}>
                <Text style={styles.captureText}>{cameraOn ? 'â¸ ìë™ ì¤‘ì§€' : 'â–¶ï¸ ìë™ ì‹œì‘'}</Text>
            </TouchableOpacity>

            {imageUri && (
                <Image source={{ uri: `file://${imageUri}?t=${Date.now()}` }} style={styles.previewImage} />
            )}

            {previewBase64Image && (
                <Image
                    source={{ uri: previewBase64Image }}
                    style={styles.previewBase64Image}
                />
            )}
        </View>
    );
};

export default StudyMediaPipeScreen;

const styles = StyleSheet.create({
    container: {
        flex: 1,
        backgroundColor: 'black',
        justifyContent: 'flex-end',
        alignItems: 'center',
    },
    captureButton: {
        backgroundColor: 'white',
        padding: 16,
        borderRadius: 50,
        marginBottom: 40,
    },
    captureText: {
        fontSize: 16,
        fontWeight: 'bold',
    },
    previewImage: {
        position: 'absolute',
        top: 60,
        right: 20,
        width: 100,
        height: 160,
        borderRadius: 8,
        borderWidth: 2,
        borderColor: 'white',
    },
    previewBase64Image: {
        position: 'absolute',
        top: 60,
        left: 20,
        width: 91.25,     // 100 * (292 / 320)
        height: 100,      // ê¸°ì¤€ ë†’ì´
        borderRadius: 8,
        borderWidth: 2,
        borderColor: 'white',
    },
});