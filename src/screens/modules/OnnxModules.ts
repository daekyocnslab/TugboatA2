import modelManager from '@/interceptor/ModelManager';
import CalcStudyModule from '@/modules/calcStudy/CalcStudyModule';
import { StudyType } from '@/types/StudyType';
import { Rank, Tensor, Tensor3D, Tensor4D, TensorContainer } from '@tensorflow/tfjs-core';
import { InferenceSession, Tensor as OnnxTensor } from 'onnxruntime-react-native';
const tf = modelManager.tf;

/**
 * ONNX ëª¨ë¸ë“¤ì„ ë¶ˆëŸ¬ì˜¤ê³  ì—°ì‚°ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
 */
const OnnxModules = {
	estimateVersionRfb320: async (_imageToTensor: Tensor3D): Promise<number[][]> => {
		const visionRfd320OnnxModel = modelManager.getVisionRfb320OnnxModel;
		let filteredBoxes: number[][] = [];
		let boxesArray: number[][];
		let confidences: number[][] | string[][];
		let boxScores: number[][];
		try {
			if (visionRfd320OnnxModel) {
				const _configTensor = tf.tidy(() => {
					const resizedTensor = tf.image.resizeBilinear(_imageToTensor, [240, 320]);
					const normalizedTensor = tf.div(resizedTensor, tf.scalar(255.0)); // tidy ë‚´ë¶€ë¼ OK
					return tf.transpose(normalizedTensor, [2, 0, 1]);
				});

				const tensorData = new Float32Array(_configTensor.dataSync());
				tf.dispose(_configTensor); // ëª…ì‹œì ìœ¼ë¡œ í•´ì œ
				let feed: { input: OnnxTensor | null } = { input: new OnnxTensor(tensorData.slice(), [1, 3, 240, 320]) };

				await visionRfd320OnnxModel
					// @ts-ignore
					.run(feed, visionRfd320OnnxModel.outputNames)
					.then((fetches: InferenceSession.OnnxValueMapType | null) => {
						if (fetches) {
							// ì›ë³¸ ì´ë¯¸ì§€ì˜ í¬ê¸°
							const origWidth = 320; // ì´ë¯¸ì§€ ë„ˆë¹„
							const origHeight = 240; // ì´ë¯¸ì§€ ë†’ì´

							// ì‹ ë¢°ë„ ì„ê³„ê°’ê³¼ IoU ì„ê³„ê°’
							const probThreshold = 0.5; // ì‹ ë¢°ë„ ì„ê³„ê°’
							const iouThreshold = 0.3; // IoU ì„ê³„ê°’
							const topK = -1; // ìƒìœ„ kê°œ ê²°ê³¼ë§Œ ìœ ì§€ (-1ì´ë©´ ëª¨ë“  ê²°ê³¼ ìœ ì§€)

							let ouputBox: OnnxTensor | null = fetches.boxes;
							let ouputScores: OnnxTensor | null = fetches.scores;

							// ë°•ìŠ¤ ê°œìˆ˜ ê³„ì‚°
							const numBoxes = Math.floor(ouputBox!.data.length / 4);
							confidences = Array.from({ length: numBoxes }, (_, i) => {
								const startIndex = i * (ouputScores!.data.length / numBoxes);
								const endIndex = startIndex + ouputScores!.data.length / numBoxes;
								// @ts-ignore
								return Array.from(ouputScores!.data.slice(startIndex, endIndex));
							});

							// @ts-ignore
							boxesArray = Array.from({ length: numBoxes }, (_, i) => Array.from(ouputBox.data.slice(i * 4, (i + 1) * 4)));

							// scoresì™€ boxesë¥¼ ê²°í•©í•˜ì—¬ [x_min, y_min, x_max, y_max, score] í˜•íƒœë¡œ ë³€í™˜
							boxScores = boxesArray
								.map((box, idx) => {
									if (!confidences[idx] || !box) {
										console.error(`Invalid data at index ${idx}`, { confidences, boxesArray });
										return null;
									}
									return [...box, confidences[idx][1]]; // í´ë˜ìŠ¤ 1ì˜ ì‹ ë¢°ë„ ì‚¬ìš©
								})
								//@ts-ignore
								.filter((box): box is number[] => box !== null);

							// NMSë¥¼ í†µí•´ ê²¹ì¹˜ëŠ” ë°•ìŠ¤ ì œê±°
							filteredBoxes = calcHandler
								.hardNMS(
									boxScores.filter((box) => box[4] > probThreshold),
									iouThreshold,
									topK,
								)
								.map((box) => {
									return [
										Math.round(box[0] * origWidth), // x_min
										Math.round(box[1] * origHeight), // y_min
										Math.round(box[2] * origWidth), // x_max
										Math.round(box[3] * origHeight), // y_max
										box[4], // score
									];
								});

							for (const key in fetches) {
								(fetches as any)[key] = null;
							}
							// âœ… outputë„ ì§ì ‘ null ì²˜ë¦¬ (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ê¶Œì¥)
							ouputBox = null;
							ouputScores = null;
							// (fetches.boxes as any) = null;
							// (fetches.scores as any) = null;
							// ONNX Tensor ì´ˆê¸°í™”
						}
						feed.input = null;
						feed = null as any; // ğŸ‘ˆ feed ìì²´ë„ ëª…ì‹œì ìœ¼ë¡œ ì •ë¦¬ ê°€ëŠ¥
						fetches = null;
					})
					.catch((err) => {
						console.error(`versionRfb320Model.run() í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤ : ${err}`);
					});
			}
		} catch (error) {
			console.log(`[-] estimateVersionRfb320 error :: ${error}`);
		} finally {
			// [STEP6] ONNX Tensor ì°¸ì¡° ì œê±°
			boxesArray = null as any; // 2ì°¨ì› ë°°ì—´ ì´ˆê¸°í™”
			confidences = null as any; // 2ì°¨ì› ë°°ì—´ ì´ˆê¸°í™”
			boxScores = null as any; // 2ì°¨ì› ë°°ì—´ ì´ˆê¸°í™”
		}
		return filteredBoxes;
	},

	/**
	 * PDLF ëª¨ë¸ì„ í†µí•´ì„œ ì–¼êµ´ ì¢Œí‘œê°’ ì¶”ì¶œ
	 * @param imageToTensor
	 * @param result
	 * @returns Promise<number[][]>
	 */
	estimatePfldModel: async (imageToTensor: Tensor3D, versionRfd320Result: number[][]): Promise<number[][]> => {
		let resultPfld: number[][] = [];

		try {
			const pfldOnnxModel = modelManager.getPfldOnnxModel;
			if (pfldOnnxModel) {
				const [x, y, width, height] = versionRfd320Result[0];

				// í•´ë‹¹ ì˜ì—­ì•ˆì—ì„œ ì²˜ë¦¬í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©í›„ ì œê±°
				const _configTensor = tf.tidy(() => {
					const box = [x, y, width, height];
					const _topLeft = [box[0], box[1]];
					const _bottomRight = [box[2], box[3]];

					const w = _bottomRight[0] - _topLeft[0];
					const h = _bottomRight[1] - _topLeft[1];

					const _xw1 = Math.trunc(Math.max(box[0] - 0.4 * w, 0));
					const _yw1 = Math.trunc(Math.max(box[1] - 0.4 * h, 0));
					const _xw2 = Math.trunc(Math.min(box[2] + 0.4 * w, imageToTensor.shape[1] - 1));
					const _yw2 = Math.trunc(Math.min(box[3] + 0.4 * h, imageToTensor.shape[0] - 1));

					const _indexingResult = tf.slice(imageToTensor, [_yw1, _xw1, 0], [_yw2 - _yw1, _xw2 - _xw1, 3]);
					const _resizeFace = tf.image.resizeBilinear(_indexingResult, [112, 112]);
					const normalizedTensor = tf.div(_resizeFace, tf.scalar(255.0));
					const _expandDims = tf.reshape(normalizedTensor, [1, 112, 112, 3]);

					// dataSync()ëŠ” ë§ˆì§€ë§‰ ê²°ê³¼ë¥¼ ì¶”ì¶œí•˜ê³ , tidyëŠ” ëª¨ë“  ì¤‘ê°„ í…ì„œë¥¼ ìë™ ì •ë¦¬
					return new Float32Array(_expandDims.dataSync());
				});
				let feed: { input: OnnxTensor | null } = { input: new OnnxTensor(_configTensor.slice(), [1, 3, 112, 112]) };

				await pfldOnnxModel
					//@ts-ignore
					.run(feed, pfldOnnxModel.outputNames)
					.then((fetches: InferenceSession.OnnxValueMapType | null) => {
						if (fetches) {
							const result = Array.from({ length: fetches.output.data.length / 2 }, (_, i) => [
								fetches!.output.data[i * 2],
								fetches!.output.data[i * 2 + 1],
							]);

							// @ts-ignore
							const restoredLandmarks = result.map(([nx, ny]: [number, number]) => {
								const originalX = nx * width + x; // x ë³µì›
								const originalY = ny * height + y; // y ë³µì›
								return [originalX, originalY];
							});
							resultPfld = restoredLandmarks;
						}
						for (const key in fetches) {
							(fetches as any)[key] = null;
						}
						// input/out ONNX Tensor ë¹„ìš°ê¸°
						feed.input = null; // ì°¸ì¡° ì œê±° (input ê°’ ì œê±°)
						fetches = null;
					})
					.catch((err) => {
						console.error(`pfldModel.run() í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤ : ${err}`);
					});
			}
		} catch (error) {
			console.log(`[-] estimatePfld error :: ${error}`);
		}

		return resultPfld;
	},

	/**
	 *
	 * @param imageToTensor
	 * @param pfldArr
	 * @returns
	 */
	async estimateIrisLandmarkModel(
		imageToTensor: Tensor3D,
		pfldArr: number[][],
	): Promise<{ leftIrisArr: number[]; rightIrisArr: number[] }> {
		const resultObj: { leftIrisArr: number[]; rightIrisArr: number[] } = {
			leftIrisArr: [],
			rightIrisArr: [],
		};

		if (pfldArr.length > 0) {
			const irisLandmarkModel = modelManager.getIrisLandmarkOnnxModel;
			const { leftEye, rightEye } = calcHandler.extractEyeRegions(pfldArr, imageToTensor);

			const runInference = async (input: Float32Array): Promise<number[]> => {
				let feed: { input_1: OnnxTensor | null } = {
					input_1: new OnnxTensor(input.slice(), [1, 64, 64, 3]),
				};
				try {
					// @ts-ignore
					const fetches = await irisLandmarkModel.run(feed, irisLandmarkModel.outputNames);
					const result = Array.from(fetches.output_iris.data as Float32Array);

					// cleanup
					for (const key in fetches) {
						(fetches as any)[key] = null;
					}
					feed.input_1 = null;

					return result;
				} catch (err) {
					console.error('irisLandmarkModel.run() error:', err);
					return [];
				}
			};

			if (irisLandmarkModel) {
				try {
					resultObj.leftIrisArr = await runInference(leftEye);
					resultObj.rightIrisArr = await runInference(rightEye);
				} catch (e) {
					console.error(`[-] estimateIrisLandmark error :: ${e}`);
				}
			}
		}

		return resultObj;
	},

	/**
	 * FSA-NET ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì— ëŒ€í•œ ì¸¡ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
	 * @param tensorImage
	 * @param estimateArr
	 * @returns {Promise<number[]>}
	 */
	async fsanetEstimate(tensorImage: Tensor3D, versionRfd320Result): Promise<number[]> {
		const fsanetModel = modelManager.getFSANetSession;
		let resultFaceDtct: number[] = [];

		// í•„ìˆ˜ê°’ ì¡´ì¬ ì²´í¬
		if (versionRfd320Result.length > 0) {
			// [ê¸°ëŠ¥-1] FaceMeshë¥¼ í†µí•´ ì „ë‹¬ë°›ì€ ë°ì´í„°ë¥¼ í†µí•´ ê°’ ì„¸íŒ…
			const [x1, y1, x2, y2, score] = versionRfd320Result[0];

			const box = new Array(x1, y1, x2, y2); // [x1, y1, x2, y2]

			const _topLeft: number[] = new Array(box[0], box[1]);
			const _bottomRight: number[] = new Array(box[2], box[3]);

			// í•´ë‹¹ ì˜ì—­ì•ˆì—ì„œ ì²˜ë¦¬í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©í›„ ì œê±°
			const _configTensor: TensorContainer = tf.tidy(() => {
				const w: number = _bottomRight[0] - _topLeft[0];
				const h: number = _bottomRight[1] - _topLeft[1];
				const _xw1: number = Math.trunc(Math.max(box[0] - 0.4 * w, 0));
				const _yw1: number = Math.trunc(Math.max(box[1] - 0.4 * h, 0));
				const _xw2: number = Math.trunc(Math.min(box[2] + 0.4 * w, tensorImage.shape[1] - 1));
				const _yw2: number = Math.trunc(Math.min(box[3] + 0.4 * h, tensorImage.shape[0] - 1));

				// [ê¸°ëŠ¥-2] posbox í˜•íƒœë¡œ ìë¥¸ í˜•íƒœ (TensorImage)
				const _indexingResult: Tensor<Rank.R3> = tf.slice(tensorImage, [_yw1, _xw1, 0], [_yw2 - _yw1, _xw2 - _xw1, 3]);

				// [ê¸°ëŠ¥-3] ì¸ë±ì‹±í•œ ê°’ì„ ë¦¬ì‚¬ì´ì§• í•˜ëŠ” ì‘ì—…
				const _resizeFace: Tensor3D | Tensor4D = tf.image.resizeBilinear(_indexingResult, [64, 64]);

				// [ê¸°ëŠ¥-4] ë…¸ë©€ë¼ì´ì¦ˆ ìˆ˜í–‰ ì‘ì—…
				const _min = tf.min(_resizeFace);
				const aa = tf.sub(_resizeFace, _min);
				const bb = tf.sub(tf.max(_resizeFace), _min);
				const cc = tf.div(aa, bb);
				const _normalize_imagetensor = tf.mul(cc, 255);

				// [ê¸°ëŠ¥-5] 3ì°¨ì› ë°ì´í„°ë¥¼ ì°¨ì› ë¶„ë¦¬í•˜ì—¬ ì›í•˜ëŠ” ìˆœì„œë¡œ ì¬ì¡°ë¦½
				const _temp = tf.unstack(_normalize_imagetensor, 2);
				const _finalFace = tf.stack([_temp[2], _temp[1], _temp[0]], 2);
				tf.dispose(_temp); // âœ… ì¶”ê°€ ê¶Œì¥

				// [ê¸°ëŠ¥-6] ëª¨ë¸ë§ì„ ìœ„í•œ ë§¨ì•ì— ì°¨ì›ì„ ì¶”ê°€í•˜ëŠ” ì‘ì—…(reshapeë¡œ ì°¨ì›ì¶”ê°€)
				const _expandDims = tf.reshape(_finalFace, [1, 64, 64, 3]);

				// [STEP7] í…ì„œí”Œë¡œìš° ë°ì´í„°ë¥¼ ìë°”ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ì»¨ë²„íŒ…í•©ë‹ˆë‹¤.
				return new Float32Array(_expandDims.dataSync());
			});

			// [STEP8] ì»¨ë²„íŒ…í•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.'
			let feed: { input: OnnxTensor | null } = { input: new OnnxTensor(_configTensor.slice(), [1, 64, 64, 3]) };

			if (fsanetModel) {
				await fsanetModel
					// @ts-ignore
					.run(feed, fsanetModel.outputNames)
					.then((fetches: InferenceSession.OnnxValueMapType | null) => {
						//@ts-ignore
						resultFaceDtct = tf.tidy(() => CalcStudyModule.calcFsanetInfo(fetches));
						// ê²°ê³¼ê°’(fetches) ë‚´ì— ëª¨ë“  ê°’ì„ NULLë¡œ ì°¸ì¡°í•´ì œ
						for (const key in fetches) {
							(fetches as any)[key] = null;
						}
						// input/out ONNX Tensor ë¹„ìš°ê¸°
						feed.input = null; // ì°¸ì¡° ì œê±° (input ê°’ ì œê±°)
						fetches = null; // ì°¸ì¡° ì œê±° (result ê°’ ì œê±°)
					})
					.catch((err) => console.error(`modelCalcHandler.fsanetEstimate() í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤ : ${err}`));
			} else console.error('[+] FSA-NET ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ ');
		}

		return resultFaceDtct;
	},

	/**
	 * FaceMesh ì¸¡ì •í•œ ì‚¬ìš©ìì˜ ê°’ì— ëŒ€í•´ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
	 *
	 * [STEP1] FaceMeshë¥¼ í†µí•´ ì „ë‹¬ë°›ì€ ì¸¡ì • ë°ì´í„° í†µí•´ ì–¼êµ´ ì¸¡ì •ê°’(boundingBox), ì§‘ì¤‘ë„(faceInViewConfidence)ë¥¼ ë°˜í™˜ë°›ìŠµë‹ˆë‹¤.
	 * [STEP2] í•„ìš”í•œ ì˜ì—­(posbox)ì— ëŒ€í•´ì„œë§Œ ë‚¨ê¸°ê³  ìë¦…ë‹ˆë‹¤.
	 * [STEP3] ìë¥¸ ì˜ì—­ì„ ì¼ì •í•œ í¬ê¸°(224, 224)ë¡œ ë¦¬ì‚¬ì´ì¦ˆ í•©ë‹ˆë‹¤.
	 * [STEP4] ë¦¬ì‚¬ì´ì¦ˆëœ ì˜ì—­(ì–¼êµ´)ì— ëŒ€í•´ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
	 * [STEP5] PyTorch ë°ì´í„° í˜•íƒœë¥¼ Tensor ë°ì´í„° í˜•íƒœë¡œ ì»¨ë²„íŒ…í•©ë‹ˆë‹¤.
	 *
	 * @param {tf.Tensor3D} tensorImage : í…ì„œ ì¹´ë©”ë¼ì—ì„œ ì „ë‹¬ ë°›ì€ ì´ë¯¸ì§€
	 * @param {AnnotatedPrediction[]} estimateArr
	 */

	async hsemotionEstimate(tensorImage: Tensor3D, versionRfd320Result: number[][]): Promise<StudyType.ResultHsemotion> {
		const hsemotionModel = modelManager.getHSEmotionSession;

		// [STEP1] ì—°ì‚° ê²°ê³¼ë¥¼ ë°˜í™˜í•  ê°ì²´ë¥¼ ì´ˆê¸° ì„ ì–¸í•©ë‹ˆë‹¤.
		let resultHsemotion: StudyType.ResultHsemotion = {
			arousalArr: [],
			valenceArr: [],
			emotionCode: '',
		};

		try {
			// [CASE1] íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ ë°›ì€ ê°’ì´ ì¡´ì¬í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
			if (versionRfd320Result.length > 0 && hsemotionModel != null) {
				const [x1, y1, x2, y2, score] = versionRfd320Result[0];

				const box = new Array(x1, y1, x2, y2); // [x1, y1, x2, y2]

				const _topLeft: number[] = new Array(box[0], box[1]);
				const _bottomRight: number[] = new Array(box[2], box[3]);

				const _configTensor = tf.tidy(() => {
					const w = _bottomRight[0] - _topLeft[0];
					const h = _bottomRight[1] - _topLeft[1];

					const xw1 = Math.trunc(Math.max(box[0] - 0.4 * w, 0));
					const yw1 = Math.trunc(Math.max(box[1] - 0.4 * h, 0));
					const xw2 = Math.trunc(Math.min(box[2] + 0.4 * w, tensorImage.shape[1] - 1));
					const yw2 = Math.trunc(Math.min(box[3] + 0.4 * h, tensorImage.shape[0] - 1));

					const sliced = tf.slice(tensorImage, [yw1, xw1, 0], [yw2 - yw1, xw2 - xw1, 3]);
					const resized = tf.image.resizeBilinear(sliced, [224, 224]);
					const resizedDiv = tf.div(resized, 255);

					const mean = [0.485, 0.456, 0.406];
					const std = [0.229, 0.224, 0.225];
					const [r, g, b] = tf.split(resizedDiv, 3, 2);

					const rNorm = tf.div(tf.sub(r, mean[0]), std[0]);
					const gNorm = tf.div(tf.sub(g, mean[1]), std[1]);
					const bNorm = tf.div(tf.sub(b, mean[2]), std[2]);

					const normalized = tf.concat([rNorm, gNorm, bNorm], 2);
					const transposed = tf.transpose(normalized, [2, 0, 1]);
					const expanded = tf.expandDims(transposed, 0); // ì´ê±´ ë°–ì—ì„œ ì°¸ì¡°í•˜ë¯€ë¡œ í•´ì œ ì•ˆë¨

					// ëª¨ë“  ì¤‘ê°„ í…ì„œ í•´ì œ
					tf.dispose([sliced, resized, resizedDiv, r, g, b, rNorm, gNorm, bNorm, normalized, transposed]);
					return expanded;
				});
				const tensorData = new Float32Array(_configTensor.dataSync());
				tf.dispose(_configTensor); // ëª…ì‹œì ìœ¼ë¡œ í•´ì œ

				// âœ… configArrayëŠ” Float32Array â†’ í•´ì œ í•„ìš” ì—†ìŒ
				const feed: { input: OnnxTensor | null } = { input: new OnnxTensor(tensorData.slice(), [1, 3, 224, 224]) };

				await hsemotionModel
					//@ts-ignore
					.run(feed, hsemotionModel.outputNames)
					.then((fetches: InferenceSession.OnnxValueMapType | null) => {
						resultHsemotion = tf.tidy(() => CalcStudyModule.calcHsemotionInfo(fetches!));
						// resultHsemotion = { "arousalArr": [0.08552277088165283], "emotionCode": "SUP", "valenceArr": [-0.027108697220683098] }

						// âœ… fetches ë‚´ë¶€ í•´ì œ
						for (const key in fetches) {
							(fetches as any)[key] = null;
						}
						// input/out ONNX Tensor ë¹„ìš°ê¸°
						feed.input = null; // ì°¸ì¡° ì œê±° (input ê°’ ì œê±°)
						fetches = null;
					})
					.catch((err) => console.error(`modelCalcHandler.hsemotionEstimate() í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤ : ${err}`));
			}
		} catch (error) {
			console.log(`[-] hsemotionEstimate error :: ${error}`);
		}

		return resultHsemotion;
	},

	/**
	 * Hpose ì¸¡ì •
	 * @param inputData
	 * @returns
	 */
	async hPoseEstimate(data1: Float32Array): Promise<number> {
		const hposeModel = modelManager.getHposeSession;
		let resultScore = 0;

		let inputTensor1: OnnxTensor | null = null;
		let inputTensor2: OnnxTensor | null = null;
		let tfHelperTensor: Tensor | null = null;

		try {
			if (hposeModel) {
				// [STEP1] tf.onesë¥¼ í™œìš©í•˜ì—¬ Int32Array ìƒì„±
				tfHelperTensor = tf.tidy(() => tf.ones([1, 1]));
				const data2 = new Int32Array(tfHelperTensor.dataSync());

				// [STEP2] ONNX Tensor ìƒì„±
				inputTensor1 = new OnnxTensor(data1.slice(), [1, 10, 8]);
				inputTensor2 = new OnnxTensor(data2.slice(), [1, 1]);

				const feed: { args_0: OnnxTensor | null; args_1: OnnxTensor | null } = {
					args_0: inputTensor1,
					args_1: inputTensor2,
				};

				// [STEP3] ëª¨ë¸ ì‹¤í–‰

				await hposeModel
					//@ts-ignore
					.run(feed, hposeModel.outputNames)
					.then((fetches: InferenceSession.OnnxValueMapType | null) => {
						if (fetches) {
							const result = tf.tidy(() => {
								const output = fetches!.output_1;
								const [, data2] = output.data;
								return Math.floor((data2 as number) * 100);
							});
							resultScore = result;


							// [STEP4] ONNX output ë¹„ìš°ê¸°
							for (const key in fetches) {
								(fetches as any)[key] = null;
							}
							// âœ… outputë„ ì§ì ‘ null ì²˜ë¦¬ (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ê¶Œì¥)
							(fetches as any).output_1 = null;
							fetches = null;
						}

						feed.args_0 = null;
						feed.args_1 = null;
					})
					.catch((err) => {
						console.error(`_hposeModel.run() í•¨ìˆ˜ì—ì„œ ì—ëŸ¬ ë°œìƒ: ${err}`);
					});
			}
		} catch (error) {
			console.log(`[-] hPoseEstimate error :: ${error}`);
		} finally {
			// [STEP5] TensorFlow Tensor ì œê±°
			tfHelperTensor?.dispose();
			tfHelperTensor = null;

			// [STEP6] ONNX Tensor ì°¸ì¡° ì œê±°
			inputTensor1 = null;
			inputTensor2 = null;
		}

		return resultScore;
	},
};

/**
 * ì§€ì—­ ë³€ìˆ˜ë“¤ ê´€ë¦¬
 */
const calcHandler = (() => {
	return {
		/**
		 * NMS(Non-Maximum Suppression) êµ¬í˜„
		 * @param boxScores ë°•ìŠ¤ ì¢Œí‘œ ë° ì ìˆ˜ ë°°ì—´ [[x_min, y_min, x_max, y_max, score], ...]
		 * @param iouThreshold IoU ì„ê³„ê°’
		 * @param topK ìƒìœ„ kê°œ ê²°ê³¼ë§Œ ìœ ì§€ (-1ì´ë©´ ëª¨ë“  ê²°ê³¼ ìœ ì§€)
		 * @param candidateSize ê³ ë ¤í•  í›„ë³´ ê°œìˆ˜
		 * @returns ìœ ì§€ëœ ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸
		 */
		hardNMS: (
			boxScores: number[][],
			iouThreshold: number,
			topK: number = -1,
			candidateSize: number = 200,
		): number[][] => {
			let candidates = boxScores
				.slice()
				.sort((a, b) => b[4] - a[4])
				.slice(0, candidateSize);

			const picked: number[][] = [];

			while (candidates.length > 0) {
				const current = candidates.shift()!;
				picked.push(current);
				if (topK > 0 && picked.length >= topK) break;

				candidates = candidates.filter((box) => {
					const iou = calcHandler.iouOf([current], [box.slice(0, 4)]);
					return iou[0] <= iouThreshold;
				});
			}

			return picked;
		},

		/**
		 * IoU(Intersection over Union) ê³„ì‚° í•¨ìˆ˜
		 * @param boxes0 ê¸°ì¤€ ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸ [[x_min, y_min, x_max, y_max], ...]
		 * @param boxes1 ë¹„êµ ëŒ€ìƒ ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸ [[x_min, y_min, x_max, y_max], ...]
		 * @param eps 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì‘ì€ ê°’
		 * @returns IoU ê°’ ë¦¬ìŠ¤íŠ¸
		 */
		iouOf: (boxes0: number[][], boxes1: number[][], eps: number = 1e-5): number[] => {
			const [x1_0, y1_0, x2_0, y2_0] = boxes0[0];
			const area0 = (x2_0 - x1_0) * (y2_0 - y1_0);

			const results: number[] = [];

			for (let i = 0; i < boxes1.length; i++) {
				const [x1_1, y1_1, x2_1, y2_1] = boxes1[i];

				const x1 = Math.max(x1_0, x1_1);
				const y1 = Math.max(y1_0, y1_1);
				const x2 = Math.min(x2_0, x2_1);
				const y2 = Math.min(y2_0, y2_1);

				const overlapArea = Math.max(x2 - x1, 0) * Math.max(y2 - y1, 0);
				const area1 = (x2_1 - x1_1) * (y2_1 - y1_1);

				results.push(overlapArea / (area0 + area1 - overlapArea + eps));
			}

			return results;
		},
		extractEyeRegions: (
			landmarks: number[][],
			origImage: Tensor3D,
			padding: number = 50,
			targetSize: [number, number] = [64, 64],
		): { leftEye: Float32Array; rightEye: Float32Array } => {
			const leftEyeLandmarks = landmarks.slice(36, 42);
			const rightEyeLandmarks = landmarks.slice(42, 48);

			const [leftX1, leftY1] = leftEyeLandmarks.reduce(
				([minX, minY], [x, y]) => [Math.min(minX, x - padding), Math.min(minY, y - padding)],
				[Infinity, Infinity],
			);
			const [leftX2, leftY2] = leftEyeLandmarks.reduce(
				([maxX, maxY], [x, y]) => [Math.max(maxX, x + padding), Math.max(maxY, y + padding)],
				[-Infinity, -Infinity],
			);

			const [rightX1, rightY1] = rightEyeLandmarks.reduce(
				([minX, minY], [x, y]) => [Math.min(minX, x - padding), Math.min(minY, y - padding)],
				[Infinity, Infinity],
			);
			const [rightX2, rightY2] = rightEyeLandmarks.reduce(
				([maxX, maxY], [x, y]) => [Math.max(maxX, x + padding), Math.max(maxY, y + padding)],
				[-Infinity, -Infinity],
			);

			// ê³µí†µ í•¨ìˆ˜ë¡œ ì¶”ì¶œ
			const extractAndNormalizeEye = (x1: number, y1: number, x2: number, y2: number): Float32Array => {
				return tf.tidy(() => {
					const region = tf.slice(
						origImage,
						[Math.floor(Math.max(y1, 0)), Math.floor(Math.max(x1, 0)), 0],
						[
							Math.floor(Math.min(y2 - y1, origImage.shape[0] - y1)),
							Math.floor(Math.min(x2 - x1, origImage.shape[1] - x1)),
							3,
						],
					);
					const resized = tf.image.resizeBilinear(region, targetSize);
					const normalized = tf.div(resized, 255);
					const reshaped = tf.reshape(normalized, [1, ...targetSize, 3]);
					const result = reshaped.dataSync();
					return new Float32Array(result);
				});
			};

			return {
				leftEye: extractAndNormalizeEye(leftX1, leftY1, leftX2, leftY2),
				rightEye: extractAndNormalizeEye(rightX1, rightY1, rightX2, rightY2),
			};
		},
	};
})();
export default OnnxModules;
